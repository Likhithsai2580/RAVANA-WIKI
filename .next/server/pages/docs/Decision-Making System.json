{"pageProps":{"doc":{"slug":"Decision-Making System","title":"Decision-Making System","content":"<h1>Decision-Making System</h1>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#project-structure\">Project Structure</a></li>\n<li><a href=\"#core-components\">Core Components</a></li>\n<li><a href=\"#architecture-overview\">Architecture Overview</a></li>\n<li><a href=\"#detailed-component-analysis\">Detailed Component Analysis</a></li>\n<li><a href=\"#dependency-analysis\">Dependency Analysis</a></li>\n<li><a href=\"#performance-considerations\">Performance Considerations</a></li>\n<li><a href=\"#troubleshooting-guide\">Troubleshooting Guide</a></li>\n<li><a href=\"#conclusion\">Conclusion</a></li>\n</ol>\n<h2>Introduction</h2>\n<p>This document provides a comprehensive overview of the decision-making system within the RAVANA AGI framework. The system is designed to enable autonomous, goal-driven behavior through a sophisticated integration of planning, hypothesis testing, LLM-based reasoning, and external data sources. It operates on a continuous loop from situation recognition to action execution, incorporating emotional state, memory, and learning to adapt over time. The architecture emphasizes structured planning, confidence-aware decisions, and robust fallback mechanisms to handle uncertainty and avoid stuck states.</p>\n<h2>Project Structure</h2>\n<p>The decision-making system is distributed across several modules and core components within the RAVANA repository. The primary logic resides in the <code>modules/decision_engine</code> directory, which contains the <code>decision_maker.py</code> and <code>planner.py</code> files for core decision and planning logic. The <code>core/llm.py</code> file provides the interface to multiple LLM providers, which is central to the decision-making process. State management is handled by <code>core/state.py</code>, while external data integration is facilitated by modules like <code>event_detection</code> and services like <code>data_service.py</code>. The overall structure is modular, allowing for independent development and testing of components like emotional intelligence, curiosity, and memory.</p>\n<pre><code class=\"language-mermaid\">graph TD\nsubgraph \"Core System\"\nState[core/state.py&#x3C;br/>SharedState]\nLLM[core/llm.py&#x3C;br/>LLM Interface]\nSystem[core/system.py&#x3C;br/>AGISystem]\nend\nsubgraph \"Decision Engine\"\nPlanner[modules/decision_engine/planner.py&#x3C;br/>GoalPlanner]\nDecisionMaker[modules/decision_engine/decision_maker.py&#x3C;br/>Decision Logic]\nend\nsubgraph \"Supporting Modules\"\nEmotionalIntelligence[modules/emotional_intellegence/emotional_intellegence.py&#x3C;br/>Mood &#x26; Behavior]\nEventDetection[modules/event_detection/event_detector.py&#x3C;br/>Event Detection]\nAdaptiveLearning[modules/adaptive_learning/learning_engine.py&#x3C;br/>Learning Engine]\nSituationGenerator[modules/situation_generator/situation_generator.py&#x3C;br/>Situation Creation]\nend\nsubgraph \"Services\"\nDataService[services/data_service.py&#x3C;br/>Data &#x26; Event Handling]\nend\nSystem --> State\nSystem --> LLM\nSystem --> Planner\nSystem --> DecisionMaker\nSystem --> EmotionalIntelligence\nSystem --> AdaptiveLearning\nSystem --> SituationGenerator\nSystem --> DataService\nDataService --> EventDetection\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>core/state.py</a></li>\n<li><a>core/llm.py</a></li>\n<li><a>modules/decision_engine/planner.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>modules/emotional_intellegence/emotional_intellegence.py</a></li>\n<li><a>modules/event_detection/event_detector.py</a></li>\n<li><a>services/data_service.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n<li><a>modules/situation_generator/situation_generator.py</a></li>\n<li><a>core/system.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/state.py</a></li>\n<li><a>core/llm.py</a></li>\n<li><a>modules/decision_engine/planner.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>modules/emotional_intellegence/emotional_intellegence.py</a></li>\n<li><a>modules/event_detection/event_detector.py</a></li>\n<li><a>services/data_service.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n<li><a>modules/situation_generator/situation_generator.py</a></li>\n<li><a>core/system.py</a></li>\n</ul>\n<h2>Core Components</h2>\n<p>The core components of the decision-making system are the <code>GoalPlanner</code>, the <code>DecisionMaker</code>, and the <code>SharedState</code>. The <code>GoalPlanner</code> manages a hierarchy of long-term goals, breaking them down into sub-goals and tasks. The <code>DecisionMaker</code> uses LLM input to analyze situations, evaluate options, and generate actions based on the current state, goals, and hypotheses. The <code>SharedState</code> acts as a central repository for the system's mood, current situation, recent memories, and other contextual data, ensuring all components have a consistent view of the agent's internal and external world. These components work in concert, with the <code>DecisionMaker</code> relying on the <code>GoalPlanner</code> for strategic direction and the <code>SharedState</code> for contextual awareness.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>modules/decision_engine/planner.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>core/state.py</a></li>\n</ul>\n<h2>Architecture Overview</h2>\n<p>The decision-making architecture is a goal-driven loop that integrates multiple cognitive functions. The process begins with the <code>SituationGenerator</code> creating a new situation, which could be based on external events, internal curiosity, or subconscious processing. This situation is then fed into the <code>DecisionMaker</code>, which uses an LLM to generate a decision. The LLM prompt includes the current situation, the agent's mood, relevant memories, available actions, and its long-term goals. The <code>DecisionMaker</code> evaluates the LLM's response, extracts a structured decision, and returns an action for execution. After the action is executed, the system updates its mood, records the outcome for learning, and retrieves new memories, closing the loop. This architecture allows for flexible, context-aware decision-making that can adapt to changing circumstances.</p>\n<pre><code class=\"language-mermaid\">sequenceDiagram\nparticipant SG as SituationGenerator\nparticipant DM as DecisionMaker\nparticipant LLM as LLM\nparticipant AM as ActionManager\nparticipant EI as EmotionalIntelligence\nparticipant AL as AdaptiveLearning\nparticipant SS as SharedState\nSG->>DM : Generate Situation\nactivate DM\nDM->>LLM : Send Prompt with State\nactivate LLM\nLLM-->>DM : Return Raw Response\ndeactivate LLM\nDM->>DM : Extract Decision\nDM-->>AM : Execute Action\nactivate AM\nAM-->>DM : Return Result\ndeactivate AM\nDM->>EI : Process Result\nEI-->>SS : Update Mood\nDM->>AL : Record Outcome\nDM->>SS : Update State\ndeactivate DM\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>modules/situation_generator/situation_generator.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>core/llm.py</a></li>\n<li><a>core/system.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n</ul>\n<h2>Detailed Component Analysis</h2>\n<h3>Goal-Driven Architecture and Planning Mechanisms</h3>\n<p>The system employs a hierarchical goal-driven architecture. The <code>GoalPlanner</code> class manages a collection of goals stored in an in-memory dictionary. Goals are created using the <code>plan_from_context</code> function, which takes a natural language description and converts it into a structured goal object with a unique ID, title, description, and status. The planner supports breaking down high-level goals into sub-goals, creating a tree-like structure. For example, a goal like \"Learn reinforcement learning basics\" can be decomposed into sub-goals such as \"Understand Q-learning\" and \"Implement a DQN agent.\" The <code>get_goals</code> method allows the decision-maker to retrieve all active goals, which are then used to guide the decision-making process. This hierarchical approach enables the system to manage complex, long-term objectives by focusing on one actionable task at a time.</p>\n<pre><code class=\"language-mermaid\">classDiagram\nclass GoalPlanner {\n+get_goal(goal_id : str) Dict[str, Any]\n+get_goals(status : str) List[Dict[str, Any]]\n+add_sub_goal(parent_id : str, description : str) str\n+update_goal_status(goal_id : str, status : str) bool\n}\nclass Goal {\n+id : str\n+title : str\n+description : str\n+timeframe : str\n+status : str\n+sub_goals : List[Dict[str, Any]]\n+tasks : List[Dict[str, Any]]\n+context : str\n}\nGoalPlanner --> Goal : \"manages\"\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>modules/decision_engine/planner.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>modules/decision_engine/planner.py</a></li>\n</ul>\n<h3>Hypothesis Testing and Experimentation</h3>\n<p>The system can generate and test hypotheses as part of its self-improvement cycle. The <code>goal_driven_decision_maker_loop</code> function checks for opportunities to start an experiment, with a 10% probability if no experiment is active. It calls <code>generate_hypothesis</code> from the <code>agent_self_reflection</code> module to create a new hypothesis based on the current state. If a hypothesis is generated, the decision-maker returns a decision with the action <code>initiate_experiment</code>, which triggers the creation of a test situation. The system can also analyze the outcome of a completed experiment by calling <code>analyze_experiment_outcome</code>. The <code>agi_experimentation_engine</code> function in <code>llm.py</code> provides a unified framework for experimentation, which can involve generating and executing Python code, performing online validation with search, and providing a final verdict on the hypothesis. This closed-loop process allows the agent to learn from its own actions and refine its understanding of the world.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>core/llm.py</a></li>\n</ul>\n<h3>Decision Evaluation with LLM Input</h3>\n<p>The <code>decision_maker_loop</code> function in <code>llm.py</code> is responsible for evaluating options using LLM input. It constructs a comprehensive prompt that includes the current situation, emotional state, relevant memories, external knowledge (RAG), and available actions. The prompt instructs the LLM to perform a deep analysis, create a strategic plan, assess its confidence, provide a reasoning chain, and specify the first action. The LLM is expected to respond with a JSON object containing keys like <code>analysis</code>, <code>plan</code>, <code>action</code>, <code>params</code>, <code>confidence</code>, and <code>reasoning</code>. The <code>extract_decision</code> function parses this response, handling cases where the JSON is malformed by returning a default structure with an error message. The confidence score (a float between 0.0 and 1.0) is used by the system to gauge the reliability of the decision. This structured approach ensures that the LLM's output is predictable and can be reliably integrated into the agent's action loop.</p>\n<pre><code class=\"language-mermaid\">flowchart TD\nStart([Start Decision Loop]) --> PrepareContext[\"Prepare Context&#x3C;br/>(Situation, Mood, Memory, RAG)\"]\nPrepareContext --> ConstructPrompt[\"Construct LLM Prompt\"]\nConstructPrompt --> CallLLM[\"Call LLM with safe_call_llm\"]\nCallLLM --> ExtractDecision[\"Extract Decision with extract_decision\"]\nExtractDecision --> ParseJSON{\"Valid JSON?\"}\nParseJSON --> |Yes| UseData[\"Use analysis, plan, action, confidence\"]\nParseJSON --> |No| Fallback[\"Use fallback plan, log error\"]\nUseData --> ReturnDecision[\"Return Decision\"]\nFallback --> ReturnDecision\nReturnDecision --> End([End])\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>core/llm.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/llm.py</a></li>\n</ul>\n<h3>Integration with External Data</h3>\n<p>The system integrates external data through search results and event detection. The <code>call_gemini_with_search</code> function in <code>llm.py</code> enables the LLM to perform Google searches, with results being added to a <code>search_result_manager</code>. This allows the agent to access up-to-date information for its decisions. The <code>event_detection</code> module processes a stream of text (e.g., news articles) to detect significant events. It uses an embedding model to cluster similar documents and a sentiment classifier to filter out irrelevant or negative content. The <code>data_service.py</code> module periodically fetches articles from configured feeds and runs them through the event detector, saving detected events to the database. These events can then influence the <code>SituationGenerator</code>, creating situations that respond to real-world developments. This integration ensures the agent's decisions are informed by current events and external knowledge.</p>\n<pre><code class=\"language-mermaid\">sequenceDiagram\nparticipant DS as DataService\nparticipant ED as EventDetector\nparticipant DB as Database\nDS->>ED : fetch_and_save_articles()\nED->>ED : process_data_for_events(texts)\nED->>ED : filter_content()\nED->>ED : get_embeddings()\nED->>ED : cluster_documents()\nED->>ED : generate_event_alerts()\nED-->>DS : return events\nDS->>DB : save events to database\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>services/data_service.py</a></li>\n<li><a>modules/event_detection/event_detector.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/data_service.py</a></li>\n<li><a>modules/event_detection/event_detector.py</a></li>\n</ul>\n<h3>Workflow from Situation Recognition to Action Selection</h3>\n<p>The complete workflow begins with situation recognition. The <code>AGISystem</code>'s main loop calls <code>_generate_situation</code>, which uses the <code>SituationGenerator</code> to create a new situation based on the current state, curiosity topics, and behavior modifiers. This situation is then passed to the <code>decision_maker_loop</code>, which constructs a prompt and sends it to an LLM. The LLM's response is parsed into a decision object. The <code>ActionManager</code> then executes the specified action, such as writing a file or logging a message. After execution, the system updates the agent's mood based on the action's outcome, records the decision and its result for future learning, and retrieves relevant memories for the next cycle. The <code>current_plan</code> is also managed; if the decision contains a multi-step plan, the remaining steps are stored for future execution. This end-to-end workflow enables the agent to operate autonomously, continuously making decisions and taking actions.</p>\n<pre><code class=\"language-mermaid\">flowchart LR\nA[Situation Recognition] --> B[Decision Making]\nB --> C[Action Selection]\nC --> D[Action Execution]\nD --> E[Mood Update]\nE --> F[Learning &#x26; Memory]\nF --> A\n</code></pre>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/system.py</a></li>\n<li><a>core/system.py</a></li>\n<li><a>core/llm.py</a></li>\n</ul>\n<h3>Handling Uncertainty and Confidence Scores</h3>\n<p>The system handles uncertainty through explicit confidence scores and fallback strategies. The LLM is instructed to include a <code>confidence</code> field in its response, a float between 0.0 and 1.0. This score is used by the system to prioritize high-confidence decisions and to trigger fallback mechanisms when confidence is low. The <code>extract_decision</code> function includes a default confidence of 0.5 if the field is missing. The decision structure also includes a <code>fallback_plan</code> field, which specifies what to do if the primary action fails. The <code>safe_call_llm</code> function implements retry logic with exponential backoff, attempting up to three times before failing. The <code>is_lazy_llm_response</code> function can detect generic or unhelpful responses, allowing the system to retry with a different prompt or provider. This multi-layered approach ensures robust operation even when the LLM is uncertain or produces a poor response.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/llm.py</a></li>\n<li><a>core/llm.py</a></li>\n</ul>\n<h3>Integration with Emotional State and Memory Systems</h3>\n<p>The decision-making process is deeply integrated with the agent's emotional state and memory systems. The <code>SharedState</code> object contains the current <code>mood</code> vector and a list of <code>recent_memories</code>. These are included in the LLM prompt, allowing the agent's decisions to be influenced by its current emotional state and past experiences. After an action is executed, the <code>EmotionalIntelligence</code> module processes the outcome to update the mood vector. For example, a successful action might increase <code>happy</code> and <code>confident</code>, while an error might increase <code>frustrated</code>. The updated mood is then stored in the <code>SharedState</code>. The <code>AdaptiveLearningEngine</code> records each decision and its outcome, building a history that can be analyzed to improve future decisions. This integration creates a cohesive cognitive architecture where emotion, memory, and learning all contribute to more intelligent and adaptive behavior.</p>\n<pre><code class=\"language-mermaid\">classDiagram\nclass SharedState {\n+mood : Dict[str, float]\n+recent_memories : List[Dict[str, Any]]\n+current_situation : Dict[str, Any]\n+get_state_summary() str\n}\nclass EmotionalIntelligence {\n+process_action_natural(output : str)\n+get_mood_vector() Dict[str, float]\n+influence_behavior() Dict[str, Any]\n}\nclass AdaptiveLearningEngine {\n+record_decision_outcome(decision : Dict, outcome : Any, success : bool)\n+analyze_decision_patterns() Dict[str, Any]\n}\nSharedState &#x3C;.. EmotionalIntelligence : \"updates\"\nSharedState &#x3C;.. AdaptiveLearningEngine : \"uses\"\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>core/state.py</a></li>\n<li><a>modules/emotional_intellegence/emotional_intellegence.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/state.py</a></li>\n<li><a>core/system.py</a></li>\n<li><a>modules/emotional_intellegence/emotional_intellegence.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n</ul>\n<h2>Dependency Analysis</h2>\n<p>The decision-making system has a clear dependency hierarchy. The top-level <code>AGISystem</code> class depends on all other components, orchestrating their interactions. The <code>DecisionMaker</code> depends on <code>llm.py</code> for LLM calls, <code>planner.py</code> for goal management, and <code>state.py</code> for contextual data. The <code>SituationGenerator</code> depends on the <code>emotional_intellegence</code> and <code>curiosity_trigger</code> modules to create contextually relevant situations. The <code>data_service.py</code> module depends on <code>event_detector.py</code> for event processing and the database models for persistence. There are no circular dependencies, and the use of well-defined interfaces (e.g., the LLM prompt format) allows for loose coupling between components. This modular design makes the system easy to extend and maintain.</p>\n<pre><code class=\"language-mermaid\">graph TD\nAGISystem --> DecisionMaker\nAGISystem --> SituationGenerator\nAGISystem --> EmotionalIntelligence\nAGISystem --> AdaptiveLearning\nAGISystem --> DataService\nAGISystem --> SharedState\nDecisionMaker --> LLM\nDecisionMaker --> Planner\nDecisionMaker --> SharedState\nSituationGenerator --> EmotionalIntelligence\nSituationGenerator --> CuriosityTrigger\nDataService --> EventDetector\nDataService --> Database\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>core/system.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/system.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n</ul>\n<h2>Performance Considerations</h2>\n<p>The system's performance is primarily constrained by LLM call latency and the complexity of the decision-making loop. The <code>safe_call_llm</code> function includes a 30-second timeout and retry logic, which can add significant delay if the LLM is slow or unresponsive. To mitigate this, the system uses a fast local LLM as a fallback and caches repeated actions in the <code>EnhancedActionManager</code>. The <code>SituationGenerator</code> and <code>EventDetector</code> are computationally intensive due to the use of embedding models and clustering algorithms, but these are run asynchronously and not on every decision loop. The in-memory storage of goals and state ensures fast access, but could become a bottleneck if the number of goals grows very large. Overall, the system is designed for quality of decision over speed, prioritizing thorough analysis and robustness.</p>\n<h2>Troubleshooting Guide</h2>\n<p>Common issues in the decision-making system include decision loops, stuck states, and suboptimal planning. A decision loop can occur if the agent repeatedly generates the same situation and takes the same action without progress. This can be mitigated by ensuring the <code>AdaptiveLearningEngine</code> is properly recording outcomes and adjusting behavior. A stuck state might happen if the LLM consistently returns low-confidence decisions or errors. Checking the LLM provider configuration and fallback mechanisms in <code>llm.py</code> is essential. Suboptimal planning can result from poor goal decomposition or inadequate memory retrieval. Reviewing the <code>GoalPlanner</code> logic and the <code>memory_service</code> configuration can help. For debugging, enable verbose logging and inspect the <code>decision_history</code> and <code>mood_history</code> in the <code>SharedState</code>. If the agent is not generating new hypotheses, verify that the <code>agent_self_reflection</code> module is loaded and functioning.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/llm.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n</ul>\n<h2>Conclusion</h2>\n<p>The RAVANA decision-making system is a sophisticated, goal-driven architecture that leverages LLMs for high-level reasoning while maintaining robust control through structured planning and state management. Its integration of emotion, memory, and learning creates a cohesive cognitive model capable of autonomous, adaptive behavior. The system's modular design and clear interfaces make it extensible and maintainable. By handling uncertainty through confidence scores and fallback strategies, and by continuously learning from its experiences, the agent is well-equipped to operate effectively in complex and dynamic environments. Future improvements could include more sophisticated goal prioritization and deeper integration of the learning engine into the decision loop.</p>\n<p><strong>Referenced Files in This Document</strong></p>\n<ul>\n<li><a>core/llm.py</a></li>\n<li><a>core/state.py</a></li>\n<li><a>modules/decision_engine/decision_maker.py</a></li>\n<li><a>modules/decision_engine/planner.py</a></li>\n<li><a>modules/adaptive_learning/learning_engine.py</a></li>\n<li><a>modules/event_detection/event_detector.py</a></li>\n<li><a>services/data_service.py</a></li>\n<li><a>core/system.py</a></li>\n<li><a>modules/emotional_intellegence/emotional_intellegence.py</a></li>\n<li><a>modules/situation_generator/situation_generator.py</a></li>\n</ul>\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture & Design","title":"Architecture & Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment & Operations","title":"Deployment & Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true}