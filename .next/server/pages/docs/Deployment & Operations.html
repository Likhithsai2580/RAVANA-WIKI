<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta charSet="utf-8"/><title>Deployment &amp; Operations<!-- --> - RAVANA AGI Documentation</title><meta name="description" content="Documentation for Deployment &amp; Operations"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/aa7d986e9c238cc1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/aa7d986e9c238cc1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.0/dist/mermaid.min.js" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-eb143115b8bf2786.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a41459f5c0b49356.js" defer=""></script><script src="/_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="/_next/static/chunks/pages/docs/%5Bslug%5D-37d587d3c8e56222.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_buildManifest.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen flex flex-col"><div class="min-h-screen flex flex-col"><header class="bg-wiki-blue text-white p-4 shadow-md"><div class="container mx-auto flex justify-between items-center"><h1 class="text-2xl font-bold">RAVANA AGI Documentation</h1><nav><ul class="flex space-x-4"><li><a class="hover:underline" href="/">Home</a></li></ul></nav></div></header><div class="flex-grow container mx-auto p-4 flex flex-col md:flex-row gap-6"><div class="w-full md:w-64 flex-shrink-0"><nav class="w-full md:w-64 flex-shrink-0"><div class="bg-white rounded-lg shadow p-4 sticky top-4"><h3 class="font-bold text-lg mb-3">Documentation</h3><ul class="space-y-1"><li class="mb-3"><div class="font-semibold text-gray-700">A</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Action%20System">Action System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/API%20Reference">API Reference</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Architecture%20&amp;%20Design">Architecture &amp; Design</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">C</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Configuration">Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Conversational%20AI%20Communication%20Framework">Conversational AI Communication Framework</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Core%20System">Core System</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">D</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Database%20Schema">Database Schema</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Decision-Making%20System">Decision-Making System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 bg-wiki-blue text-white" href="/docs/Deployment%20&amp;%20Operations">Deployment &amp; Operations</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Development%20Guide">Development Guide</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">E</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Emotional%20Intelligence">Emotional Intelligence</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent">Enhanced Snake Agent</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent%20Architecture">Enhanced Snake Agent Architecture</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">G</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Graceful%20Shutdown">Graceful Shutdown</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">L</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/LLM%20Integration">LLM Integration</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">M</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Memory%20Systems">Memory Systems</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Multi-Modal%20Memory">Multi-Modal Memory</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">P</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Project%20Overview">Project Overview</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">S</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Self-Improvement">Self-Improvement</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Services">Services</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Snake%20Agent%20Configuration">Snake Agent Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules-57f9b30b-b165-48d3-8e89-196940d26190">Specialized Modules</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules">Specialized Modules</a></li></ul></li></ul></div></nav></div><main class="flex-grow"><nav class="mb-4 text-sm"><ol class="list-none p-0 inline-flex"><li class="flex items-center"><a class="text-wiki-blue hover:underline" href="/">Home</a><svg class="fill-current w-3 h-3 mx-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path></svg></li><li class="flex items-center"><span class="text-gray-500">Deployment &amp; Operations</span></li></ol></nav><div class="flex flex-col md:flex-row gap-6"><article class="prose max-w-none bg-white p-6 rounded-lg shadow flex-grow"><h1>Deployment &amp; Operations</h1><div><h1>Deployment &#x26; Operations</h1>
<h2>Update Summary</h2>
<p><strong>Changes Made</strong></p>
<ul>
<li>Added documentation for singleton pattern implementation in ConversationalAI class</li>
<li>Updated Bot Connectivity Verification section with instance tracking details</li>
<li>Added new section for User Platform Preference Tracking</li>
<li>Enhanced section sources to reflect new files and functionality</li>
<li>Added references to DiscordBot and TelegramBot instance tracking mechanisms</li>
<li>Updated startup command documentation to reflect singleton behavior</li>
</ul>
<h2>Table of Contents</h2>
<ol>
<li><a href="#runtime-requirements">Runtime Requirements</a></li>
<li><a href="#dependency-installation">Dependency Installation</a></li>
<li><a href="#configuration-setup">Configuration Setup</a></li>
<li><a href="#startup-commands">Startup Commands</a></li>
<li><a href="#monitoring--health-checks">Monitoring &#x26; Health Checks</a></li>
<li><a href="#logging-configuration">Logging Configuration</a></li>
<li><a href="#backup--recovery-procedures">Backup &#x26; Recovery Procedures</a></li>
<li><a href="#performance-tuning">Performance Tuning</a></li>
<li><a href="#scaling-considerations">Scaling Considerations</a></li>
<li><a href="#failure-recovery">Failure Recovery</a></li>
<li><a href="#security-hardening">Security Hardening</a></li>
<li><a href="#operational-runbooks">Operational Runbooks</a></li>
<li><a href="#bot-connectivity-verification">Bot Connectivity Verification</a></li>
<li><a href="#user-platform-preference-tracking">User Platform Preference Tracking</a></li>
</ol>
<h2>Runtime Requirements</h2>
<p>The RAVANA system is a Python-based artificial general intelligence (AGI) framework designed for autonomous reasoning, memory consolidation, and multi-modal processing. It requires a modern computing environment with sufficient resources to support concurrent AI inference, database operations, and background services.</p>
<h3>CPU &#x26; Memory Requirements</h3>
<ul>
<li><strong>Minimum</strong>: 4-core CPU, 8GB RAM</li>
<li><strong>Recommended</strong>: 8-core CPU, 16GB+ RAM</li>
<li><strong>High-performance</strong>: 16-core CPU, 32GB+ RAM (for large-scale knowledge compression and physics simulations)</li>
</ul>
<p>The system runs multiple concurrent processes including:</p>
<ul>
<li>Main AGI loop</li>
<li>Memory server (separate process)</li>
<li>Background data fetching and event detection</li>
<li>Multi-modal processing (image/audio)</li>
</ul>
<h3>GPU Requirements</h3>
<ul>
<li><strong>Optional but recommended</strong>: NVIDIA GPU with CUDA support (RTX 3060 or higher)</li>
<li>Required for:
<ul>
<li>Local LLM inference (if not using cloud APIs)</li>
<li>FAISS semantic search acceleration</li>
<li>Image/audio processing</li>
</ul>
</li>
<li>Minimum VRAM: 8GB</li>
<li>Supported frameworks: CUDA, cuDNN</li>
</ul>
<h3>Storage Requirements</h3>
<ul>
<li><strong>System files</strong>: ~500MB</li>
<li><strong>Database</strong>: Variable, scales with usage (initial: 100MB)</li>
<li><strong>FAISS index</strong>: ~100MB–1GB depending on knowledge base size</li>
<li><strong>Temporary files</strong>: Up to 2GB in <code>/tmp/agi_multimodal</code></li>
<li><strong>Recommended</strong>: SSD storage for optimal database and index performance</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>core/config.py</a></li>
<li><a>services/knowledge_service.py</a></li>
<li><a>services/memory_service.py</a></li>
</ul>
<h2>Dependency Installation</h2>
<h3>Python Environment Setup</h3>
<pre><code class="language-bash"># Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Upgrade pip
pip install --upgrade pip
</code></pre>
<h3>Core Dependencies (via pyproject.toml)</h3>
<pre><code class="language-bash"># Install all dependencies
pip install -e .

# Or install manually from requirements
pip install \
  sqlmodel \
  "sentence-transformers" \
  faiss-cpu \
  psutil \
  requests \
  python-dotenv \
  numpy \
  "openai" \
  "google-generativeai" \
  "pydantic>=2.0"
</code></pre>
<h3>Optional GPU-Accelerated Dependencies</h3>
<pre><code class="language-bash"># For GPU support (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install faiss-gpu

# Verify GPU availability
python -c "import torch; print(torch.cuda.is_available())"
</code></pre>
<h3>External Service Dependencies</h3>
<ul>
<li><strong>Google Gemini API</strong>: Required for multi-modal processing
<ul>
<li>Set <code>GEMINI_API_KEY</code> in environment</li>
</ul>
</li>
<li><strong>OpenAI API</strong>: Optional fallback for LLM calls
<ul>
<li>Set <code>OPENAI_API_KEY</code> in environment</li>
</ul>
</li>
<li><strong>PostgreSQL (optional)</strong>: For production database
<ul>
<li>Default: SQLite (file-based)</li>
</ul>
</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>pyproject.toml</a></li>
<li><a>services/multi_modal_service.py</a></li>
<li><a>core/llm.py</a></li>
</ul>
<h2>Configuration Setup</h2>
<h3>Environment Variables</h3>
<p>Create <code>.env</code> file in project root:</p>
<pre><code class="language-env"># API Keys
GEMINI_API_KEY=your_gemini_key_here
OPENAI_API_KEY=your_openai_key_here

# Database
DATABASE_URL=sqlite:///./ravana.db
# For PostgreSQL: DATABASE_URL=postgresql://user:pass@localhost/ravana

# Memory Service
MEMORY_SERVER_HOST=localhost
MEMORY_SERVER_PORT=8001
MEMORY_SERVICE_SHUTDOWN_TIMEOUT=30

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/ravana.log

# Data Feeds
FEED_URLS=https://example.com/feed1.xml,https://example.com/feed2.json
</code></pre>
<h3>Database Initialization</h3>
<pre><code class="language-python">from database.engine import create_db_and_tables

create_db_and_tables()
</code></pre>
<p>This creates all required tables based on <code>database/models.py</code>.</p>
<h3>Configuration File (config.json)</h3>
<pre><code class="language-json">{
  "debug_mode": false,
  "auto_save_interval": 300,
  "max_memory_entries": 10000,
  "consolidation_threshold": 100,
  "embedding_model": "all-MiniLM-L6-v2",
  "llm_provider": "gemini"
}
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>core/config.py</a></li>
<li><a>database/models.py</a></li>
<li><a>database/engine.py</a></li>
</ul>
<h2>Startup Commands</h2>
<h3>Autonomous Mode (Main AGI Loop)</h3>
<pre><code class="language-bash">python main.py
</code></pre>
<p>This starts the full AGI system with:</p>
<ul>
<li>Continuous data ingestion</li>
<li>Decision engine</li>
<li>Memory management</li>
<li>Autonomous action execution</li>
</ul>
<h3>Physics Testing Mode</h3>
<pre><code class="language-bash"># Run all physics experiments
python run_physics_tests.py

# Interactive physics CLI
python physics_cli.py
</code></pre>
<p>These modes are used for validating physical reasoning capabilities and running controlled experiments.</p>
<h3>Single Task Mode</h3>
<pre><code class="language-bash"># Execute specific action via CLI
python main.py --action coding --params '{"task": "implement quicksort"}'

# Process multi-modal directory
python main.py --mode multimodal --dir ./input_files
</code></pre>
<h3>Service-Specific Startup</h3>
<pre><code class="language-bash"># Start only memory service
python -m modules.episodic_memory.main

# Start data fetching service
python -c "from services.data_service import DataService; from database.engine import engine; ds = DataService(engine, ['https://rss.example.com']); ds.fetch_and_save_articles()"
</code></pre>
<h3>Conversational AI Module Startup</h3>
<pre><code class="language-bash"># Start conversational AI module in standalone mode
python launch_conversational_ai.py

# Verify bot connectivity before starting
python launch_conversational_ai.py --verify-bots
</code></pre>
<p>The <code>--verify-bots</code> flag checks the connectivity of configured bot platforms (Discord and Telegram) and exits with appropriate status code. The ConversationalAI class implements a singleton pattern, ensuring only one instance can be created and initialized in the system.</p>
<p><strong>Section sources</strong></p>
<ul>
<li><a>main.py</a></li>
<li><a>run_physics_tests.py</a></li>
<li><a>physics_cli.py</a></li>
<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>
<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>
</ul>
<h2>Monitoring &#x26; Health Checks</h2>
<h3>Service Health Endpoints</h3>
<table>
<thead>
<tr>
<th>Service</th>
<th>Health Endpoint</th>
<th>Process Detection</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Server</td>
<td><code>http://localhost:8001/health</code></td>
<td>Checks for <code>memory.py</code> process</td>
</tr>
<tr>
<td>Main System</td>
<td>None (internal)</td>
<td>PID file or process name</td>
</tr>
</tbody>
</table>
<h3>Health Check Implementation</h3>
<pre><code class="language-python"># Example health check for memory service
async def check_memory_service_health():
    import requests
    try:
        response = requests.get("http://localhost:8001/health", timeout=5)
        return response.status_code == 200
    except:
        return False
</code></pre>
<h3>Monitoring Metrics</h3>
<p>The system exposes the following operational metrics:</p>
<ul>
<li>
<p><strong>MemoryService.get_memory_statistics()</strong></p>
<ul>
<li><code>status</code>: operational/error</li>
<li><code>total_memories</code>: count of stored memories</li>
<li><code>last_consolidation</code>: timestamp</li>
<li><code>memory_server_status</code>: running/not running</li>
</ul>
</li>
<li>
<p><strong>Database health</strong></p>
<ul>
<li>Connection pool status</li>
<li>Table row counts</li>
<li>Index integrity</li>
</ul>
</li>
</ul>
<h3>Prometheus Integration (Recommended)</h3>
<p>Add metrics exporter to expose for Prometheus:</p>
<pre><code class="language-python">from prometheus_client import start_http_server, Counter, Gauge

# Example metric
MEMORY_COUNT = Gauge('ravana_memory_count', 'Number of stored memories')
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/memory_service.py</a></li>
<li><a>services/data_service.py</a></li>
</ul>
<h2>Logging Configuration</h2>
<h3>Log Levels</h3>
<ul>
<li><strong>DEBUG</strong>: Detailed debugging information</li>
<li><strong>INFO</strong>: Normal operation events</li>
<li><strong>WARNING</strong>: Potential issues</li>
<li><strong>ERROR</strong>: Recoverable errors</li>
<li><strong>CRITICAL</strong>: System failures</li>
</ul>
<h3>Log Output</h3>
<p>Logs are written to:</p>
<ul>
<li><strong>Console</strong>: Real-time monitoring</li>
<li><strong>File</strong>: <code>logs/ravana.log</code> (rotated daily)</li>
<li><strong>Structured format</strong>: JSON when <code>LOG_JSON=true</code></li>
</ul>
<h3>Log Categories</h3>
<table>
<thead>
<tr>
<th>Logger</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ravana</code></td>
<td>Main system events</td>
</tr>
<tr>
<td><code>MemoryService</code></td>
<td>Memory operations</td>
</tr>
<tr>
<td><code>DataService</code></td>
<td>Data fetching and storage</td>
</tr>
<tr>
<td><code>KnowledgeService</code></td>
<td>Knowledge compression and retrieval</td>
</tr>
<tr>
<td><code>MultiModalService</code></td>
<td>Image/audio processing</td>
</tr>
</tbody>
</table>
<h3>Example Log Entry</h3>
<pre><code class="language-json">{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "INFO",
  "logger": "MemoryService",
  "event": "Memory server detected, attempting graceful shutdown...",
  "context": {"pid": 1234}
}
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/memory_service.py</a></li>
<li><a>services/knowledge_service.py</a></li>
</ul>
<h2>Backup &#x26; Recovery Procedures</h2>
<h3>Database Backup</h3>
<pre><code class="language-bash"># SQLite backup
cp ravana.db ravana.db.backup.$(date +%Y%m%d_%H%M%S)

# With compression
tar -czf ravana_db_$(date +%Y%m%d).tar.gz ravana.db
</code></pre>
<p>For production PostgreSQL:</p>
<pre><code class="language-bash">pg_dump -U ravana_user -h localhost ravana_db > ravana_backup.sql
</code></pre>
<h3>Knowledge Index Backup</h3>
<pre><code class="language-bash"># FAISS index and ID map
cp knowledge_index.faiss knowledge_index.faiss.backup
cp knowledge_id_map.pkl knowledge_id_map.pkl.backup

# Compressed backup
tar -czf knowledge_backup_$(date +%Y%m%d).tar.gz knowledge_index.faiss knowledge_id_map.pkl
</code></pre>
<h3>Automated Backup Script</h3>
<pre><code class="language-bash">#!/bin/bash
BACKUP_DIR="/backups/ravana/$(date +%Y%m%d)"
mkdir -p $BACKUP_DIR

cp ravana.db $BACKUP_DIR/
cp knowledge_index.faiss $BACKUP_DIR/
cp knowledge_id_map.pkl $BACKUP_DIR/

# Keep last 7 days
find /backups/ravana -type d -name "202*" | sort | head -n -7 | xargs rm -rf
</code></pre>
<h3>Recovery Procedure</h3>
<ol>
<li>Stop RAVANA services</li>
<li>Replace database and index files</li>
<li>Restart services</li>
<li>Verify integrity via health checks</li>
</ol>
<p><strong>Section sources</strong></p>
<ul>
<li><a>database/models.py</a></li>
<li><a>services/knowledge_service.py</a></li>
<li><a>services/memory_service.py</a></li>
</ul>
<h2>Performance Tuning</h2>
<h3>Database Optimization</h3>
<ul>
<li><strong>Indexing</strong>: Ensure proper indexes on timestamp fields</li>
<li><strong>Connection pooling</strong>: Use SQLAlchemy pool settings for PostgreSQL</li>
<li><strong>Vacuuming</strong>: Regular <code>VACUUM</code> for SQLite databases</li>
</ul>
<h3>Knowledge Service Tuning</h3>
<ul>
<li><strong>FAISS Index Type</strong>: Use <code>IndexIVFFlat</code> for large datasets (>100K entries)</li>
<li><strong>Embedding Model</strong>: Switch to larger models (<code>all-mpnet-base-v2</code>) for better accuracy</li>
<li><strong>Batch Processing</strong>: Process summaries in batches during compression</li>
</ul>
<h3>Memory Management</h3>
<ul>
<li><strong>Consolidation Frequency</strong>: Adjust based on activity level</li>
<li><strong>Memory Pruning</strong>: Implement TTL-based cleanup for old memories</li>
<li><strong>Vector Store</strong>: Consider switching to dedicated vector database (Pinecone, Weaviate) at scale</li>
</ul>
<h3>Caching Strategy</h3>
<ul>
<li><strong>Query Results</strong>: Cache frequent knowledge queries</li>
<li><strong>LLM Responses</strong>: Cache deterministic LLM calls</li>
<li><strong>Embeddings</strong>: Cache frequently used text embeddings</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/knowledge_service.py</a></li>
<li><a>database/models.py</a></li>
</ul>
<h2>Scaling Considerations</h2>
<h3>Horizontal Scaling</h3>
<p>The system can be scaled by separating services:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Scalability</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main AGI Loop</td>
<td>Single instance (stateful)</td>
</tr>
<tr>
<td>Memory Service</td>
<td>Can run as separate microservice</td>
</tr>
<tr>
<td>Data Service</td>
<td>Can be distributed</td>
</tr>
<tr>
<td>Knowledge Service</td>
<td>Can be containerized</td>
</tr>
<tr>
<td>Multi-Modal Service</td>
<td>Horizontally scalable</td>
</tr>
</tbody>
</table>
<h3>Containerization (Docker)</h3>
<pre><code class="language-dockerfile">FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -e .

CMD ["python", "main.py"]
</code></pre>
<h3>Kubernetes Deployment (Recommended for Production)</h3>
<p>Deploy components as separate pods with:</p>
<ul>
<li>Resource limits/requests</li>
<li>Liveness and readiness probes</li>
<li>Persistent volumes for database and indexes</li>
<li>Secret management for API keys</li>
</ul>
<h3>Load Distribution</h3>
<ul>
<li><strong>Data ingestion</strong>: Distribute feed processing</li>
<li><strong>Multi-modal processing</strong>: Queue-based worker pool</li>
<li><strong>Knowledge compression</strong>: Scheduled batch jobs</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/data_service.py</a></li>
<li><a>services/multi_modal_service.py</a></li>
</ul>
<h2>Failure Recovery</h2>
<h3>Process Crash Recovery</h3>
<ul>
<li><strong>MemoryService</strong>: Automatically detects and shuts down orphaned memory server processes</li>
<li><strong>Main System</strong>: Use process manager (systemd, PM2) for auto-restart</li>
<li><strong>Data Service</strong>: Idempotent operations prevent duplication</li>
</ul>
<h3>Data Corruption Handling</h3>
<ul>
<li><strong>Database</strong>: Use transactions and regular integrity checks</li>
<li><strong>FAISS Index</strong>: Recreate from database if corrupted</li>
<li><strong>Backups</strong>: Restore from last known good state</li>
</ul>
<h3>Graceful Shutdown</h3>
<pre><code class="language-python"># Uses Shutdownable interface
await memory_service.prepare_shutdown()
await memory_service.shutdown(timeout=30)
</code></pre>
<p>Ensures:</p>
<ul>
<li>Memory server shutdown</li>
<li>Database transaction completion</li>
<li>Log flushing</li>
<li>State persistence</li>
</ul>
<h3>Disaster Recovery Plan</h3>
<ol>
<li><strong>Immediate</strong>: Switch to backup instance</li>
<li><strong>Short-term</strong>: Restore from latest backup</li>
<li><strong>Long-term</strong>: Rebuild knowledge index from logs</li>
</ol>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/memory_service.py</a></li>
<li><a>core/shutdown_coordinator.py</a></li>
</ul>
<h2>Security Hardening</h2>
<h3>API Key Protection</h3>
<ul>
<li>Store keys in environment variables or secret manager</li>
<li>Never commit to version control</li>
<li>Use short-lived keys when possible</li>
</ul>
<h3>Input Validation</h3>
<ul>
<li>Validate all external inputs</li>
<li>Sanitize file paths in multi-modal service</li>
<li>Limit upload sizes</li>
</ul>
<h3>File System Security</h3>
<ul>
<li>Restrict permissions on configuration files</li>
<li>Secure temporary directory (<code>/tmp/agi_multimodal</code>)</li>
<li>Regular cleanup of temp files via <code>cleanup_temp_files()</code></li>
</ul>
<h3>Network Security</h3>
<ul>
<li>Bind services to localhost when possible</li>
<li>Use firewall rules to restrict access</li>
<li>Enable TLS for external endpoints</li>
</ul>
<h3>Audit Logging</h3>
<ul>
<li>Log all security-relevant events</li>
<li>Monitor for suspicious activity</li>
<li>Regular log reviews</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/multi_modal_service.py</a></li>
<li><a>core/llm.py</a></li>
</ul>
<h2>Operational Runbooks</h2>
<h3>Normal Restart Procedure</h3>
<ol>
<li>Send SIGTERM to main process</li>
<li>Wait for graceful shutdown (30s timeout)</li>
<li>Verify all services terminated</li>
<li>Start process again</li>
</ol>
<pre><code class="language-bash">pkill -f main.py
sleep 5
python main.py
</code></pre>
<h3>Emergency Stop</h3>
<pre><code class="language-bash">pkill -9 -f memory.py
pkill -9 -f main.py
</code></pre>
<p>Use only if process is unresponsive.</p>
<h3>Update Procedure</h3>
<ol>
<li>Pull latest code</li>
<li>Backup database and indexes</li>
<li>Install new dependencies</li>
<li>Test in staging environment</li>
<li>Deploy with rolling restart</li>
</ol>
<h3>Incident Response</h3>
<table>
<thead>
<tr>
<th>Incident</th>
<th>Response</th>
</tr>
</thead>
<tbody>
<tr>
<td>High CPU usage</td>
<td>Check for runaway processes, restart memory service</td>
</tr>
<tr>
<td>Database corruption</td>
<td>Restore from backup, rebuild indexes</td>
</tr>
<tr>
<td>API key exhaustion</td>
<td>Rotate keys, check for leaks</td>
</tr>
<tr>
<td>Memory leak</td>
<td>Restart service, analyze logs</td>
</tr>
<tr>
<td>Failed health check</td>
<td>Investigate logs, restore from backup if needed</td>
</tr>
</tbody>
</table>
<h3>Routine Maintenance</h3>
<ul>
<li><strong>Daily</strong>: Check logs, verify backups</li>
<li><strong>Weekly</strong>: Database optimization, cleanup temp files</li>
<li><strong>Monthly</strong>: Security audit, dependency updates</li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>services/memory_service.py</a></li>
<li><a>services/data_service.py</a></li>
<li><a>main.py</a></li>
</ul>
<h2>Bot Connectivity Verification</h2>
<h3>Purpose</h3>
<p>The bot connectivity verification feature allows administrators to test the connectivity of configured bot platforms (Discord and Telegram) before starting the full conversational AI system. This helps identify configuration issues such as invalid tokens or network connectivity problems.</p>
<h3>Verification Command</h3>
<pre><code class="language-bash">python launch_conversational_ai.py --verify-bots
</code></pre>
<p>This command:</p>
<ul>
<li>Loads the conversational AI configuration</li>
<li>Tests connectivity for each enabled bot platform</li>
<li>Returns exit code 0 if all enabled bots are connected successfully</li>
<li>Returns exit code 1 if any enabled bot fails to connect</li>
<li>Outputs detailed results to stdout</li>
</ul>
<h3>Expected Output</h3>
<pre><code>=== Bot Verification Results ===
Discord: CONNECTED
Telegram: FAILED
  Message: Telegram bot did not connect in time
</code></pre>
<h3>Configuration Requirements</h3>
<p>Bot connectivity verification relies on the configuration in <code>modules/conversational_ai/config.json</code>:</p>
<pre><code class="language-json">{
  "discord_token": "your_discord_bot_token",
  "telegram_token": "your_telegram_bot_token",
  "platforms": {
    "discord": {
      "enabled": true,
      "command_prefix": "!"
    },
    "telegram": {
      "enabled": true,
      "command_prefix": "/"
    }
  }
}
</code></pre>
<h3>Integration with Deployment Workflows</h3>
<p>The verification command can be integrated into deployment pipelines:</p>
<pre><code class="language-bash"># In CI/CD pipeline
python launch_conversational_ai.py --verify-bots
if [ $? -ne 0 ]; then
    echo "Bot verification failed, aborting deployment"
    exit 1
fi
# Proceed with deployment
</code></pre>
<h3>Troubleshooting</h3>
<p>Common issues and solutions:</p>
<ul>
<li><strong>Token not found</strong>: Verify that tokens are correctly specified in <code>config.json</code></li>
<li><strong>Network connectivity</strong>: Ensure the server has outbound internet access</li>
<li><strong>Firewall restrictions</strong>: Check if firewall rules block connections to Discord/Telegram APIs</li>
<li><strong>Invalid tokens</strong>: Regenerate bot tokens from the respective developer portals</li>
</ul>
<h3>Instance Tracking Implementation</h3>
<p>The DiscordBot and TelegramBot classes implement class-level tracking to prevent multiple instances from running concurrently. Each bot class has <code>_instance_started</code> and <code>_active_instance</code> class variables that track the global state of bot instances. When a bot is started, it checks these class variables to ensure no other instance is already running. This prevents conflicts and resource contention in production environments.</p>
<p><strong>Section sources</strong></p>
<ul>
<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>
<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>
<li><a>modules/conversational_ai/config.json</a> - <em>Configuration for conversational AI module</em></li>
<li><a>modules/conversational_ai/bots/discord_bot.py</a> - <em>Added instance tracking in recent commit</em></li>
<li><a>modules/conversational_ai/bots/telegram_bot.py</a> - <em>Added instance tracking in recent commit</em></li>
</ul>
<h2>User Platform Preference Tracking</h2>
<h3>Purpose</h3>
<p>The system tracks user platform preferences to enable intelligent message routing and personalized interactions. When a user interacts with the conversational AI through a specific platform (Discord or Telegram), this preference is recorded and used for future communications.</p>
<h3>Implementation Details</h3>
<p>The user platform preference tracking is implemented through the following components:</p>
<ol>
<li>
<p><strong>UserPlatformProfile</strong>: A dataclass that stores user platform information including:</p>
<ul>
<li><code>user_id</code>: Unique identifier of the user</li>
<li><code>last_platform</code>: The last platform used (discord/telegram)</li>
<li><code>platform_user_id</code>: Platform-specific user identifier</li>
<li><code>preferences</code>: User preferences dictionary</li>
<li><code>last_interaction</code>: Timestamp of last interaction</li>
</ul>
</li>
<li>
<p><strong>UserProfileManager</strong>: Manages the storage and retrieval of user profiles, persisting them across sessions.</p>
</li>
<li>
<p><strong>_track_user_platform method</strong>: Automatically called whenever a user sends a message, updating their platform preference.</p>
</li>
</ol>
<h3>Automatic Tracking</h3>
<p>The system automatically tracks platform usage through the <code>process_user_message</code> method in the ConversationalAI class:</p>
<pre><code class="language-python">def _track_user_platform(self, user_id: str, platform: str):
    """Track the user's platform preference."""
    try:
        profile = UserPlatformProfile(
            user_id=user_id,
            last_platform=platform,
            platform_user_id=user_id,
            preferences={},
            last_interaction=datetime.now()
        )
        self.user_profile_manager.set_user_platform_profile(user_id, profile)
    except Exception as e:
        logger.error(f"Error tracking user platform for user {user_id}: {e}")
</code></pre>
<h3>Intelligent Message Routing</h3>
<p>When sending messages to users, the system uses their stored platform preference:</p>
<pre><code class="language-python">async def send_message_to_user(self, user_id: str, message: str, platform: str = None):
    """Send a message to a user through the appropriate platform."""
    try:
        if not platform:
            # Try to get the user's last used platform from their profile
            profile = self.user_profile_manager.get_user_platform_profile(user_id)
            if profile:
                platform = profile.last_platform
        # Send message through the determined platform
        # ...
    except Exception as e:
        logger.error(f"Error sending message to user {user_id}: {e}")
</code></pre>
<h3>Configuration</h3>
<p>No additional configuration is required for user platform preference tracking. The feature is enabled by default and works automatically with the conversational AI module.</p>
<h3>Use Cases</h3>
<ol>
<li><strong>Follow-up messages</strong>: The system can send follow-up messages through the user's preferred platform</li>
<li><strong>Task notifications</strong>: When a task is completed, notifications are sent through the user's last-used platform</li>
<li><strong>Cross-platform continuity</strong>: Users can switch between platforms while maintaining their conversation context</li>
</ol>
<p><strong>Section sources</strong></p>
<ul>
<li><a>modules/conversational_ai/main.py</a> - <em>User platform tracking implementation</em></li>
<li><a>modules/conversational_ai/communication/data_models.py</a> - <em>UserPlatformProfile data model</em></li>
<li><a>modules/conversational_ai/main.py</a> - <em>Intelligent message routing</em></li>
</ul>
<p><strong>Referenced Files in This Document</strong></p>
<ul>
<li><a>main.py</a> - <em>Updated in recent commit</em></li>
<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>
<li><a>core/config.py</a></li>
<li><a>core/system.py</a></li>
<li><a>database/models.py</a></li>
<li><a>services/data_service.py</a></li>
<li><a>services/knowledge_service.py</a></li>
<li><a>services/memory_service.py</a></li>
<li><a>services/multi_modal_service.py</a></li>
<li><a>physics_cli.py</a></li>
<li><a>run_physics_tests.py</a></li>
<li><a>pyproject.toml</a></li>
<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>
<li><a>modules/conversational_ai/config.json</a> - <em>Configuration for conversational AI module</em></li>
<li><a>modules/conversational_ai/bots/discord_bot.py</a> - <em>Added instance tracking in recent commit</em></li>
<li><a>modules/conversational_ai/bots/telegram_bot.py</a> - <em>Added instance tracking in recent commit</em></li>
<li><a>modules/conversational_ai/communication/data_models.py</a> - <em>User platform tracking models</em></li>
</ul>
</div></article><div class="w-full md:w-64 flex-shrink-0"></div></div></main></div><footer class="bg-wiki-dark text-white p-4"><div class="container mx-auto text-center"><p>© <!-- -->2025<!-- --> RAVANA AGI System Documentation</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"doc":{"slug":"Deployment \u0026 Operations","title":"Deployment \u0026 Operations","content":"\u003ch1\u003eDeployment \u0026#x26; Operations\u003c/h1\u003e\n\u003ch2\u003eUpdate Summary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eChanges Made\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdded documentation for singleton pattern implementation in ConversationalAI class\u003c/li\u003e\n\u003cli\u003eUpdated Bot Connectivity Verification section with instance tracking details\u003c/li\u003e\n\u003cli\u003eAdded new section for User Platform Preference Tracking\u003c/li\u003e\n\u003cli\u003eEnhanced section sources to reflect new files and functionality\u003c/li\u003e\n\u003cli\u003eAdded references to DiscordBot and TelegramBot instance tracking mechanisms\u003c/li\u003e\n\u003cli\u003eUpdated startup command documentation to reflect singleton behavior\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#runtime-requirements\"\u003eRuntime Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dependency-installation\"\u003eDependency Installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#configuration-setup\"\u003eConfiguration Setup\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#startup-commands\"\u003eStartup Commands\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#monitoring--health-checks\"\u003eMonitoring \u0026#x26; Health Checks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#logging-configuration\"\u003eLogging Configuration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#backup--recovery-procedures\"\u003eBackup \u0026#x26; Recovery Procedures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#performance-tuning\"\u003ePerformance Tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#scaling-considerations\"\u003eScaling Considerations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#failure-recovery\"\u003eFailure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#security-hardening\"\u003eSecurity Hardening\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#operational-runbooks\"\u003eOperational Runbooks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#bot-connectivity-verification\"\u003eBot Connectivity Verification\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#user-platform-preference-tracking\"\u003eUser Platform Preference Tracking\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eRuntime Requirements\u003c/h2\u003e\n\u003cp\u003eThe RAVANA system is a Python-based artificial general intelligence (AGI) framework designed for autonomous reasoning, memory consolidation, and multi-modal processing. It requires a modern computing environment with sufficient resources to support concurrent AI inference, database operations, and background services.\u003c/p\u003e\n\u003ch3\u003eCPU \u0026#x26; Memory Requirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMinimum\u003c/strong\u003e: 4-core CPU, 8GB RAM\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecommended\u003c/strong\u003e: 8-core CPU, 16GB+ RAM\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHigh-performance\u003c/strong\u003e: 16-core CPU, 32GB+ RAM (for large-scale knowledge compression and physics simulations)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe system runs multiple concurrent processes including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMain AGI loop\u003c/li\u003e\n\u003cli\u003eMemory server (separate process)\u003c/li\u003e\n\u003cli\u003eBackground data fetching and event detection\u003c/li\u003e\n\u003cli\u003eMulti-modal processing (image/audio)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eGPU Requirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOptional but recommended\u003c/strong\u003e: NVIDIA GPU with CUDA support (RTX 3060 or higher)\u003c/li\u003e\n\u003cli\u003eRequired for:\n\u003cul\u003e\n\u003cli\u003eLocal LLM inference (if not using cloud APIs)\u003c/li\u003e\n\u003cli\u003eFAISS semantic search acceleration\u003c/li\u003e\n\u003cli\u003eImage/audio processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMinimum VRAM: 8GB\u003c/li\u003e\n\u003cli\u003eSupported frameworks: CUDA, cuDNN\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStorage Requirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSystem files\u003c/strong\u003e: ~500MB\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDatabase\u003c/strong\u003e: Variable, scales with usage (initial: 100MB)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFAISS index\u003c/strong\u003e: ~100MB–1GB depending on knowledge base size\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTemporary files\u003c/strong\u003e: Up to 2GB in \u003ccode\u003e/tmp/agi_multimodal\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecommended\u003c/strong\u003e: SSD storage for optimal database and index performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ecore/config.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/knowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDependency Installation\u003c/h2\u003e\n\u003ch3\u003ePython Environment Setup\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# venv\\Scripts\\activate  # Windows\n\n# Upgrade pip\npip install --upgrade pip\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eCore Dependencies (via pyproject.toml)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install all dependencies\npip install -e .\n\n# Or install manually from requirements\npip install \\\n  sqlmodel \\\n  \"sentence-transformers\" \\\n  faiss-cpu \\\n  psutil \\\n  requests \\\n  python-dotenv \\\n  numpy \\\n  \"openai\" \\\n  \"google-generativeai\" \\\n  \"pydantic\u003e=2.0\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eOptional GPU-Accelerated Dependencies\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# For GPU support (CUDA 11.8)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip install faiss-gpu\n\n# Verify GPU availability\npython -c \"import torch; print(torch.cuda.is_available())\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eExternal Service Dependencies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Gemini API\u003c/strong\u003e: Required for multi-modal processing\n\u003cul\u003e\n\u003cli\u003eSet \u003ccode\u003eGEMINI_API_KEY\u003c/code\u003e in environment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpenAI API\u003c/strong\u003e: Optional fallback for LLM calls\n\u003cul\u003e\n\u003cli\u003eSet \u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e in environment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePostgreSQL (optional)\u003c/strong\u003e: For production database\n\u003cul\u003e\n\u003cli\u003eDefault: SQLite (file-based)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003epyproject.toml\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/multi_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecore/llm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eConfiguration Setup\u003c/h2\u003e\n\u003ch3\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp\u003eCreate \u003ccode\u003e.env\u003c/code\u003e file in project root:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-env\"\u003e# API Keys\nGEMINI_API_KEY=your_gemini_key_here\nOPENAI_API_KEY=your_openai_key_here\n\n# Database\nDATABASE_URL=sqlite:///./ravana.db\n# For PostgreSQL: DATABASE_URL=postgresql://user:pass@localhost/ravana\n\n# Memory Service\nMEMORY_SERVER_HOST=localhost\nMEMORY_SERVER_PORT=8001\nMEMORY_SERVICE_SHUTDOWN_TIMEOUT=30\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=logs/ravana.log\n\n# Data Feeds\nFEED_URLS=https://example.com/feed1.xml,https://example.com/feed2.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eDatabase Initialization\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom database.engine import create_db_and_tables\n\ncreate_db_and_tables()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis creates all required tables based on \u003ccode\u003edatabase/models.py\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003eConfiguration File (config.json)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"debug_mode\": false,\n  \"auto_save_interval\": 300,\n  \"max_memory_entries\": 10000,\n  \"consolidation_threshold\": 100,\n  \"embedding_model\": \"all-MiniLM-L6-v2\",\n  \"llm_provider\": \"gemini\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ecore/config.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003edatabase/models.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003edatabase/engine.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStartup Commands\u003c/h2\u003e\n\u003ch3\u003eAutonomous Mode (Main AGI Loop)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epython main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis starts the full AGI system with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContinuous data ingestion\u003c/li\u003e\n\u003cli\u003eDecision engine\u003c/li\u003e\n\u003cli\u003eMemory management\u003c/li\u003e\n\u003cli\u003eAutonomous action execution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ePhysics Testing Mode\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Run all physics experiments\npython run_physics_tests.py\n\n# Interactive physics CLI\npython physics_cli.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese modes are used for validating physical reasoning capabilities and running controlled experiments.\u003c/p\u003e\n\u003ch3\u003eSingle Task Mode\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Execute specific action via CLI\npython main.py --action coding --params '{\"task\": \"implement quicksort\"}'\n\n# Process multi-modal directory\npython main.py --mode multimodal --dir ./input_files\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eService-Specific Startup\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start only memory service\npython -m modules.episodic_memory.main\n\n# Start data fetching service\npython -c \"from services.data_service import DataService; from database.engine import engine; ds = DataService(engine, ['https://rss.example.com']); ds.fetch_and_save_articles()\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eConversational AI Module Startup\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start conversational AI module in standalone mode\npython launch_conversational_ai.py\n\n# Verify bot connectivity before starting\npython launch_conversational_ai.py --verify-bots\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003e--verify-bots\u003c/code\u003e flag checks the connectivity of configured bot platforms (Discord and Telegram) and exits with appropriate status code. The ConversationalAI class implements a singleton pattern, ensuring only one instance can be created and initialized in the system.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emain.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003erun_physics_tests.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ephysics_cli.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003elaunch_conversational_ai.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/main.py\u003c/a\u003e - \u003cem\u003eModified in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMonitoring \u0026#x26; Health Checks\u003c/h2\u003e\n\u003ch3\u003eService Health Endpoints\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eService\u003c/th\u003e\n\u003cth\u003eHealth Endpoint\u003c/th\u003e\n\u003cth\u003eProcess Detection\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMemory Server\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehttp://localhost:8001/health\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eChecks for \u003ccode\u003ememory.py\u003c/code\u003e process\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMain System\u003c/td\u003e\n\u003ctd\u003eNone (internal)\u003c/td\u003e\n\u003ctd\u003ePID file or process name\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eHealth Check Implementation\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Example health check for memory service\nasync def check_memory_service_health():\n    import requests\n    try:\n        response = requests.get(\"http://localhost:8001/health\", timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eMonitoring Metrics\u003c/h3\u003e\n\u003cp\u003eThe system exposes the following operational metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMemoryService.get_memory_statistics()\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003estatus\u003c/code\u003e: operational/error\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etotal_memories\u003c/code\u003e: count of stored memories\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_consolidation\u003c/code\u003e: timestamp\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ememory_server_status\u003c/code\u003e: running/not running\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDatabase health\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConnection pool status\u003c/li\u003e\n\u003cli\u003eTable row counts\u003c/li\u003e\n\u003cli\u003eIndex integrity\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ePrometheus Integration (Recommended)\u003c/h3\u003e\n\u003cp\u003eAdd metrics exporter to expose for Prometheus:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom prometheus_client import start_http_server, Counter, Gauge\n\n# Example metric\nMEMORY_COUNT = Gauge('ravana_memory_count', 'Number of stored memories')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/data_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLogging Configuration\u003c/h2\u003e\n\u003ch3\u003eLog Levels\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDEBUG\u003c/strong\u003e: Detailed debugging information\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eINFO\u003c/strong\u003e: Normal operation events\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWARNING\u003c/strong\u003e: Potential issues\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eERROR\u003c/strong\u003e: Recoverable errors\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCRITICAL\u003c/strong\u003e: System failures\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eLog Output\u003c/h3\u003e\n\u003cp\u003eLogs are written to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConsole\u003c/strong\u003e: Real-time monitoring\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFile\u003c/strong\u003e: \u003ccode\u003elogs/ravana.log\u003c/code\u003e (rotated daily)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStructured format\u003c/strong\u003e: JSON when \u003ccode\u003eLOG_JSON=true\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eLog Categories\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLogger\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eravana\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eMain system events\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMemoryService\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eMemory operations\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eDataService\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eData fetching and storage\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eKnowledgeService\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eKnowledge compression and retrieval\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMultiModalService\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eImage/audio processing\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eExample Log Entry\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"logger\": \"MemoryService\",\n  \"event\": \"Memory server detected, attempting graceful shutdown...\",\n  \"context\": {\"pid\": 1234}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/knowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBackup \u0026#x26; Recovery Procedures\u003c/h2\u003e\n\u003ch3\u003eDatabase Backup\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# SQLite backup\ncp ravana.db ravana.db.backup.$(date +%Y%m%d_%H%M%S)\n\n# With compression\ntar -czf ravana_db_$(date +%Y%m%d).tar.gz ravana.db\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor production PostgreSQL:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epg_dump -U ravana_user -h localhost ravana_db \u003e ravana_backup.sql\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eKnowledge Index Backup\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# FAISS index and ID map\ncp knowledge_index.faiss knowledge_index.faiss.backup\ncp knowledge_id_map.pkl knowledge_id_map.pkl.backup\n\n# Compressed backup\ntar -czf knowledge_backup_$(date +%Y%m%d).tar.gz knowledge_index.faiss knowledge_id_map.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eAutomated Backup Script\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\nBACKUP_DIR=\"/backups/ravana/$(date +%Y%m%d)\"\nmkdir -p $BACKUP_DIR\n\ncp ravana.db $BACKUP_DIR/\ncp knowledge_index.faiss $BACKUP_DIR/\ncp knowledge_id_map.pkl $BACKUP_DIR/\n\n# Keep last 7 days\nfind /backups/ravana -type d -name \"202*\" | sort | head -n -7 | xargs rm -rf\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eRecovery Procedure\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStop RAVANA services\u003c/li\u003e\n\u003cli\u003eReplace database and index files\u003c/li\u003e\n\u003cli\u003eRestart services\u003c/li\u003e\n\u003cli\u003eVerify integrity via health checks\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003edatabase/models.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/knowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePerformance Tuning\u003c/h2\u003e\n\u003ch3\u003eDatabase Optimization\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIndexing\u003c/strong\u003e: Ensure proper indexes on timestamp fields\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConnection pooling\u003c/strong\u003e: Use SQLAlchemy pool settings for PostgreSQL\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVacuuming\u003c/strong\u003e: Regular \u003ccode\u003eVACUUM\u003c/code\u003e for SQLite databases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eKnowledge Service Tuning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFAISS Index Type\u003c/strong\u003e: Use \u003ccode\u003eIndexIVFFlat\u003c/code\u003e for large datasets (\u003e100K entries)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEmbedding Model\u003c/strong\u003e: Switch to larger models (\u003ccode\u003eall-mpnet-base-v2\u003c/code\u003e) for better accuracy\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBatch Processing\u003c/strong\u003e: Process summaries in batches during compression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eMemory Management\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConsolidation Frequency\u003c/strong\u003e: Adjust based on activity level\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory Pruning\u003c/strong\u003e: Implement TTL-based cleanup for old memories\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVector Store\u003c/strong\u003e: Consider switching to dedicated vector database (Pinecone, Weaviate) at scale\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCaching Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eQuery Results\u003c/strong\u003e: Cache frequent knowledge queries\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLLM Responses\u003c/strong\u003e: Cache deterministic LLM calls\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEmbeddings\u003c/strong\u003e: Cache frequently used text embeddings\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/knowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003edatabase/models.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eScaling Considerations\u003c/h2\u003e\n\u003ch3\u003eHorizontal Scaling\u003c/h3\u003e\n\u003cp\u003eThe system can be scaled by separating services:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eComponent\u003c/th\u003e\n\u003cth\u003eScalability\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMain AGI Loop\u003c/td\u003e\n\u003ctd\u003eSingle instance (stateful)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMemory Service\u003c/td\u003e\n\u003ctd\u003eCan run as separate microservice\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eData Service\u003c/td\u003e\n\u003ctd\u003eCan be distributed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eKnowledge Service\u003c/td\u003e\n\u003ctd\u003eCan be containerized\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMulti-Modal Service\u003c/td\u003e\n\u003ctd\u003eHorizontally scalable\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eContainerization (Docker)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-dockerfile\"\u003eFROM python:3.11-slim\n\nWORKDIR /app\nCOPY . .\nRUN pip install -e .\n\nCMD [\"python\", \"main.py\"]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eKubernetes Deployment (Recommended for Production)\u003c/h3\u003e\n\u003cp\u003eDeploy components as separate pods with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eResource limits/requests\u003c/li\u003e\n\u003cli\u003eLiveness and readiness probes\u003c/li\u003e\n\u003cli\u003ePersistent volumes for database and indexes\u003c/li\u003e\n\u003cli\u003eSecret management for API keys\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eLoad Distribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eData ingestion\u003c/strong\u003e: Distribute feed processing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-modal processing\u003c/strong\u003e: Queue-based worker pool\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKnowledge compression\u003c/strong\u003e: Scheduled batch jobs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/data_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/multi_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFailure Recovery\u003c/h2\u003e\n\u003ch3\u003eProcess Crash Recovery\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMemoryService\u003c/strong\u003e: Automatically detects and shuts down orphaned memory server processes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMain System\u003c/strong\u003e: Use process manager (systemd, PM2) for auto-restart\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Service\u003c/strong\u003e: Idempotent operations prevent duplication\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eData Corruption Handling\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDatabase\u003c/strong\u003e: Use transactions and regular integrity checks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFAISS Index\u003c/strong\u003e: Recreate from database if corrupted\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackups\u003c/strong\u003e: Restore from last known good state\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eGraceful Shutdown\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Uses Shutdownable interface\nawait memory_service.prepare_shutdown()\nawait memory_service.shutdown(timeout=30)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsures:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMemory server shutdown\u003c/li\u003e\n\u003cli\u003eDatabase transaction completion\u003c/li\u003e\n\u003cli\u003eLog flushing\u003c/li\u003e\n\u003cli\u003eState persistence\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDisaster Recovery Plan\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eImmediate\u003c/strong\u003e: Switch to backup instance\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShort-term\u003c/strong\u003e: Restore from latest backup\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLong-term\u003c/strong\u003e: Rebuild knowledge index from logs\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecore/shutdown_coordinator.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSecurity Hardening\u003c/h2\u003e\n\u003ch3\u003eAPI Key Protection\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eStore keys in environment variables or secret manager\u003c/li\u003e\n\u003cli\u003eNever commit to version control\u003c/li\u003e\n\u003cli\u003eUse short-lived keys when possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eInput Validation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eValidate all external inputs\u003c/li\u003e\n\u003cli\u003eSanitize file paths in multi-modal service\u003c/li\u003e\n\u003cli\u003eLimit upload sizes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eFile System Security\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRestrict permissions on configuration files\u003c/li\u003e\n\u003cli\u003eSecure temporary directory (\u003ccode\u003e/tmp/agi_multimodal\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eRegular cleanup of temp files via \u003ccode\u003ecleanup_temp_files()\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eNetwork Security\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBind services to localhost when possible\u003c/li\u003e\n\u003cli\u003eUse firewall rules to restrict access\u003c/li\u003e\n\u003cli\u003eEnable TLS for external endpoints\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAudit Logging\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLog all security-relevant events\u003c/li\u003e\n\u003cli\u003eMonitor for suspicious activity\u003c/li\u003e\n\u003cli\u003eRegular log reviews\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/multi_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecore/llm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eOperational Runbooks\u003c/h2\u003e\n\u003ch3\u003eNormal Restart Procedure\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eSend SIGTERM to main process\u003c/li\u003e\n\u003cli\u003eWait for graceful shutdown (30s timeout)\u003c/li\u003e\n\u003cli\u003eVerify all services terminated\u003c/li\u003e\n\u003cli\u003eStart process again\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epkill -f main.py\nsleep 5\npython main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eEmergency Stop\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epkill -9 -f memory.py\npkill -9 -f main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse only if process is unresponsive.\u003c/p\u003e\n\u003ch3\u003eUpdate Procedure\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003ePull latest code\u003c/li\u003e\n\u003cli\u003eBackup database and indexes\u003c/li\u003e\n\u003cli\u003eInstall new dependencies\u003c/li\u003e\n\u003cli\u003eTest in staging environment\u003c/li\u003e\n\u003cli\u003eDeploy with rolling restart\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eIncident Response\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eIncident\u003c/th\u003e\n\u003cth\u003eResponse\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eHigh CPU usage\u003c/td\u003e\n\u003ctd\u003eCheck for runaway processes, restart memory service\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDatabase corruption\u003c/td\u003e\n\u003ctd\u003eRestore from backup, rebuild indexes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAPI key exhaustion\u003c/td\u003e\n\u003ctd\u003eRotate keys, check for leaks\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMemory leak\u003c/td\u003e\n\u003ctd\u003eRestart service, analyze logs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFailed health check\u003c/td\u003e\n\u003ctd\u003eInvestigate logs, restore from backup if needed\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eRoutine Maintenance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDaily\u003c/strong\u003e: Check logs, verify backups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeekly\u003c/strong\u003e: Database optimization, cleanup temp files\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonthly\u003c/strong\u003e: Security audit, dependency updates\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/data_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emain.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBot Connectivity Verification\u003c/h2\u003e\n\u003ch3\u003ePurpose\u003c/h3\u003e\n\u003cp\u003eThe bot connectivity verification feature allows administrators to test the connectivity of configured bot platforms (Discord and Telegram) before starting the full conversational AI system. This helps identify configuration issues such as invalid tokens or network connectivity problems.\u003c/p\u003e\n\u003ch3\u003eVerification Command\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003epython launch_conversational_ai.py --verify-bots\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLoads the conversational AI configuration\u003c/li\u003e\n\u003cli\u003eTests connectivity for each enabled bot platform\u003c/li\u003e\n\u003cli\u003eReturns exit code 0 if all enabled bots are connected successfully\u003c/li\u003e\n\u003cli\u003eReturns exit code 1 if any enabled bot fails to connect\u003c/li\u003e\n\u003cli\u003eOutputs detailed results to stdout\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eExpected Output\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e=== Bot Verification Results ===\nDiscord: CONNECTED\nTelegram: FAILED\n  Message: Telegram bot did not connect in time\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eConfiguration Requirements\u003c/h3\u003e\n\u003cp\u003eBot connectivity verification relies on the configuration in \u003ccode\u003emodules/conversational_ai/config.json\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"discord_token\": \"your_discord_bot_token\",\n  \"telegram_token\": \"your_telegram_bot_token\",\n  \"platforms\": {\n    \"discord\": {\n      \"enabled\": true,\n      \"command_prefix\": \"!\"\n    },\n    \"telegram\": {\n      \"enabled\": true,\n      \"command_prefix\": \"/\"\n    }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eIntegration with Deployment Workflows\u003c/h3\u003e\n\u003cp\u003eThe verification command can be integrated into deployment pipelines:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# In CI/CD pipeline\npython launch_conversational_ai.py --verify-bots\nif [ $? -ne 0 ]; then\n    echo \"Bot verification failed, aborting deployment\"\n    exit 1\nfi\n# Proceed with deployment\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eTroubleshooting\u003c/h3\u003e\n\u003cp\u003eCommon issues and solutions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eToken not found\u003c/strong\u003e: Verify that tokens are correctly specified in \u003ccode\u003econfig.json\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNetwork connectivity\u003c/strong\u003e: Ensure the server has outbound internet access\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFirewall restrictions\u003c/strong\u003e: Check if firewall rules block connections to Discord/Telegram APIs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInvalid tokens\u003c/strong\u003e: Regenerate bot tokens from the respective developer portals\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eInstance Tracking Implementation\u003c/h3\u003e\n\u003cp\u003eThe DiscordBot and TelegramBot classes implement class-level tracking to prevent multiple instances from running concurrently. Each bot class has \u003ccode\u003e_instance_started\u003c/code\u003e and \u003ccode\u003e_active_instance\u003c/code\u003e class variables that track the global state of bot instances. When a bot is started, it checks these class variables to ensure no other instance is already running. This prevents conflicts and resource contention in production environments.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003elaunch_conversational_ai.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/main.py\u003c/a\u003e - \u003cem\u003eModified in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/config.json\u003c/a\u003e - \u003cem\u003eConfiguration for conversational AI module\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/bots/discord_bot.py\u003c/a\u003e - \u003cem\u003eAdded instance tracking in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/bots/telegram_bot.py\u003c/a\u003e - \u003cem\u003eAdded instance tracking in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eUser Platform Preference Tracking\u003c/h2\u003e\n\u003ch3\u003ePurpose\u003c/h3\u003e\n\u003cp\u003eThe system tracks user platform preferences to enable intelligent message routing and personalized interactions. When a user interacts with the conversational AI through a specific platform (Discord or Telegram), this preference is recorded and used for future communications.\u003c/p\u003e\n\u003ch3\u003eImplementation Details\u003c/h3\u003e\n\u003cp\u003eThe user platform preference tracking is implemented through the following components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUserPlatformProfile\u003c/strong\u003e: A dataclass that stores user platform information including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003euser_id\u003c/code\u003e: Unique identifier of the user\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_platform\u003c/code\u003e: The last platform used (discord/telegram)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eplatform_user_id\u003c/code\u003e: Platform-specific user identifier\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epreferences\u003c/code\u003e: User preferences dictionary\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_interaction\u003c/code\u003e: Timestamp of last interaction\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUserProfileManager\u003c/strong\u003e: Manages the storage and retrieval of user profiles, persisting them across sessions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e_track_user_platform method\u003c/strong\u003e: Automatically called whenever a user sends a message, updating their platform preference.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eAutomatic Tracking\u003c/h3\u003e\n\u003cp\u003eThe system automatically tracks platform usage through the \u003ccode\u003eprocess_user_message\u003c/code\u003e method in the ConversationalAI class:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _track_user_platform(self, user_id: str, platform: str):\n    \"\"\"Track the user's platform preference.\"\"\"\n    try:\n        profile = UserPlatformProfile(\n            user_id=user_id,\n            last_platform=platform,\n            platform_user_id=user_id,\n            preferences={},\n            last_interaction=datetime.now()\n        )\n        self.user_profile_manager.set_user_platform_profile(user_id, profile)\n    except Exception as e:\n        logger.error(f\"Error tracking user platform for user {user_id}: {e}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eIntelligent Message Routing\u003c/h3\u003e\n\u003cp\u003eWhen sending messages to users, the system uses their stored platform preference:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def send_message_to_user(self, user_id: str, message: str, platform: str = None):\n    \"\"\"Send a message to a user through the appropriate platform.\"\"\"\n    try:\n        if not platform:\n            # Try to get the user's last used platform from their profile\n            profile = self.user_profile_manager.get_user_platform_profile(user_id)\n            if profile:\n                platform = profile.last_platform\n        # Send message through the determined platform\n        # ...\n    except Exception as e:\n        logger.error(f\"Error sending message to user {user_id}: {e}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eConfiguration\u003c/h3\u003e\n\u003cp\u003eNo additional configuration is required for user platform preference tracking. The feature is enabled by default and works automatically with the conversational AI module.\u003c/p\u003e\n\u003ch3\u003eUse Cases\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFollow-up messages\u003c/strong\u003e: The system can send follow-up messages through the user's preferred platform\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTask notifications\u003c/strong\u003e: When a task is completed, notifications are sent through the user's last-used platform\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-platform continuity\u003c/strong\u003e: Users can switch between platforms while maintaining their conversation context\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/main.py\u003c/a\u003e - \u003cem\u003eUser platform tracking implementation\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/communication/data_models.py\u003c/a\u003e - \u003cem\u003eUserPlatformProfile data model\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/main.py\u003c/a\u003e - \u003cem\u003eIntelligent message routing\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eReferenced Files in This Document\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emain.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003elaunch_conversational_ai.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecore/config.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecore/system.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003edatabase/models.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/data_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/knowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/memory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eservices/multi_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ephysics_cli.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003erun_physics_tests.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epyproject.toml\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/main.py\u003c/a\u003e - \u003cem\u003eModified in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/config.json\u003c/a\u003e - \u003cem\u003eConfiguration for conversational AI module\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/bots/discord_bot.py\u003c/a\u003e - \u003cem\u003eAdded instance tracking in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/bots/telegram_bot.py\u003c/a\u003e - \u003cem\u003eAdded instance tracking in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodules/conversational_ai/communication/data_models.py\u003c/a\u003e - \u003cem\u003eUser platform tracking models\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture \u0026 Design","title":"Architecture \u0026 Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment \u0026 Operations","title":"Deployment \u0026 Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true},"page":"/docs/[slug]","query":{"slug":"Deployment \u0026 Operations"},"buildId":"QHWQNiRZOuW15nbk5-ngt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>