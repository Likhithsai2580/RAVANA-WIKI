<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta charSet="utf-8"/><title>LLM Integration<!-- --> - RAVANA AGI Documentation</title><meta name="description" content="Documentation for LLM Integration"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/aa7d986e9c238cc1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/aa7d986e9c238cc1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.0/dist/mermaid.min.js" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-eb143115b8bf2786.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a41459f5c0b49356.js" defer=""></script><script src="/_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="/_next/static/chunks/pages/docs/%5Bslug%5D-37d587d3c8e56222.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_buildManifest.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen flex flex-col"><div class="min-h-screen flex flex-col"><header class="bg-wiki-blue text-white p-4 shadow-md"><div class="container mx-auto flex justify-between items-center"><h1 class="text-2xl font-bold">RAVANA AGI Documentation</h1><nav><ul class="flex space-x-4"><li><a class="hover:underline" href="/">Home</a></li></ul></nav></div></header><div class="flex-grow container mx-auto p-4 flex flex-col md:flex-row gap-6"><div class="w-full md:w-64 flex-shrink-0"><nav class="w-full md:w-64 flex-shrink-0"><div class="bg-white rounded-lg shadow p-4 sticky top-4"><h3 class="font-bold text-lg mb-3">Documentation</h3><ul class="space-y-1"><li class="mb-3"><div class="font-semibold text-gray-700">A</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Action%20System">Action System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/API%20Reference">API Reference</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Architecture%20&amp;%20Design">Architecture &amp; Design</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">C</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Configuration">Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Conversational%20AI%20Communication%20Framework">Conversational AI Communication Framework</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Core%20System">Core System</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">D</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Database%20Schema">Database Schema</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Decision-Making%20System">Decision-Making System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Deployment%20&amp;%20Operations">Deployment &amp; Operations</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Development%20Guide">Development Guide</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">E</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Emotional%20Intelligence">Emotional Intelligence</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent">Enhanced Snake Agent</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent%20Architecture">Enhanced Snake Agent Architecture</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">G</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Graceful%20Shutdown">Graceful Shutdown</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">L</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 bg-wiki-blue text-white" href="/docs/LLM%20Integration">LLM Integration</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">M</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Memory%20Systems">Memory Systems</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Multi-Modal%20Memory">Multi-Modal Memory</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">P</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Project%20Overview">Project Overview</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">S</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Self-Improvement">Self-Improvement</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Services">Services</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Snake%20Agent%20Configuration">Snake Agent Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules-57f9b30b-b165-48d3-8e89-196940d26190">Specialized Modules</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules">Specialized Modules</a></li></ul></li></ul></div></nav></div><main class="flex-grow"><nav class="mb-4 text-sm"><ol class="list-none p-0 inline-flex"><li class="flex items-center"><a class="text-wiki-blue hover:underline" href="/">Home</a><svg class="fill-current w-3 h-3 mx-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path></svg></li><li class="flex items-center"><span class="text-gray-500">LLM Integration</span></li></ol></nav><div class="flex flex-col md:flex-row gap-6"><article class="prose max-w-none bg-white p-6 rounded-lg shadow flex-grow"><h1>LLM Integration</h1><div><h1>LLM Integration</h1>
<h2>Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#project-structure">Project Structure</a></li>
<li><a href="#core-components">Core Components</a></li>
<li><a href="#architecture-overview">Architecture Overview</a></li>
<li><a href="#detailed-component-analysis">Detailed Component Analysis</a></li>
<li><a href="#prompt-engineering-strategies">Prompt Engineering Strategies</a></li>
<li><a href="#provider-configuration-and-authentication">Provider Configuration and Authentication</a></li>
<li><a href="#fallback-mechanisms-and-error-handling">Fallback Mechanisms and Error Handling</a></li>
<li><a href="#performance-and-cost-management">Performance and Cost Management</a></li>
<li><a href="#security-considerations">Security Considerations</a></li>
</ol>
<h2>Introduction</h2>
<p>This document provides a comprehensive overview of the LLM integration system within the RAVANA repository. It details the architecture, supported providers, configuration mechanisms, and advanced features such as prompt engineering, fallback strategies, and security considerations. The system is designed to support multiple LLM providers with robust error handling, rate limiting, and performance optimization.</p>
<h2>Project Structure</h2>
<p>The project is organized into several key directories:</p>
<ul>
<li><strong>core</strong>: Contains core functionality including LLM integration, configuration, and state management</li>
<li><strong>modules</strong>: Houses specialized modules for decision-making, self-reflection, and experimentation</li>
<li><strong>prompts</strong>: Stores JSON templates for various use cases</li>
<li><strong>services</strong>: Provides data, knowledge, and memory services</li>
<li><strong>database</strong>: Manages database operations and models</li>
</ul>
<p>The LLM integration is primarily centered in the <code>core</code> directory, with supporting components in <code>modules</code> and configuration in <code>prompts</code>.</p>
<pre><code class="language-mermaid">graph TD
A[LLM Integration System] --> B[core/llm.py]
A --> C[core/config.json]
A --> D[prompts/]
A --> E[core/prompt_manager.py]
B --> F[Provider Interfaces]
B --> G[Fallback Mechanisms]
B --> H[Error Handling]
C --> I[API Keys]
C --> J[Rate Limiting]
D --> K[Decision Making]
D --> L[Experimentation]
D --> M[Self-Reflection]
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>config.json</a></li>
<li><a>prompts/decision_making.json</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>config.json</a></li>
</ul>
<h2>Core Components</h2>
<p>The core components of the LLM integration system include:</p>
<ul>
<li><strong>LLM Interface</strong>: Unified interface for calling different LLM providers</li>
<li><strong>Provider Adapters</strong>: Specific implementations for Zuki, ElectronHub, Zanity, A4F, and Gemini</li>
<li><strong>Prompt Manager</strong>: Handles prompt validation, enhancement, and template management</li>
<li><strong>Key Manager</strong>: Manages multiple API keys with rotation and rate limiting</li>
<li><strong>Error Handler</strong>: Comprehensive error handling with retry logic and fallback mechanisms</li>
</ul>
<p>The system is designed with extensibility in mind, allowing for easy addition of new providers and prompt templates.</p>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>prompt_manager.py</a></li>
</ul>
<h2>Architecture Overview</h2>
<p>The LLM integration system follows a modular architecture with clear separation of concerns. The core component is the <code>call_llm</code> function, which serves as the unified interface for all LLM interactions. This function routes requests to the appropriate provider based on availability and configuration.</p>
<pre><code class="language-mermaid">graph TD
A[Application] --> B[call_llm]
B --> C{Preferred Provider?}
C --> |Yes| D[Specific Provider]
C --> |No| E[Provider Chain]
D --> F[API Request]
E --> F
F --> G[Response]
G --> H[Error Handling]
H --> I[Retry Logic]
I --> J[Fallback to Gemini]
J --> K[Final Response]
K --> A
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<h2>Detailed Component Analysis</h2>
<h3>LLM Interface and Request Handling</h3>
<p>The LLM interface provides a unified method for interacting with multiple providers through the <code>call_llm</code> function. This function accepts a prompt and optional model parameter, then attempts to process the request through a chain of providers.</p>
<pre><code class="language-mermaid">sequenceDiagram
participant App as Application
participant LLM as call_llm
participant Providers as Provider Chain
participant Gemini as Gemini Fallback
App->>LLM : call_llm(prompt, model)
LLM->>Providers : Try Zuki, ElectronHub, Zanity, A4F
alt Provider Success
Providers-->>LLM : Response
LLM-->>App : Return Response
else All Providers Fail
LLM->>Gemini : call_gemini_with_fallback
Gemini-->>LLM : Response or Error
LLM-->>App : Return Response
end
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<h3>Provider Configuration and Authentication</h3>
<p>The system supports multiple LLM providers with configuration managed through <code>config.json</code>. Each provider has its own API key, base URL, and available models.</p>
<pre><code class="language-mermaid">classDiagram
class LLMProvider {
+str name
+str api_key
+str base_url
+List[str] models
+call(prompt, model)
}
class ZukiProvider {
+call_zuki(prompt, model)
}
class ElectronHubProvider {
+call_electronhub(prompt, model)
}
class ZanityProvider {
+call_zanity(prompt, model)
}
class A4FProvider {
+call_a4f(prompt)
}
class GeminiProvider {
+call_gemini(prompt)
+call_gemini_with_fallback()
}
LLMProvider &#x3C;|-- ZukiProvider
LLMProvider &#x3C;|-- ElectronHubProvider
LLMProvider &#x3C;|-- ZanityProvider
LLMProvider &#x3C;|-- A4FProvider
LLMProvider &#x3C;|-- GeminiProvider
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>config.json</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>config.json</a></li>
</ul>
<h2>Prompt Engineering Strategies</h2>
<p>The system employs structured prompt templates for different use cases, stored in the <code>prompts</code> directory. These templates follow a consistent format with role definition, context, task instructions, reasoning framework, output requirements, and safety constraints.</p>
<h3>Decision-Making Prompts</h3>
<p>The decision-making prompt template guides the LLM through a structured analysis process:</p>
<pre><code class="language-json">{
  "name": "decision_making",
  "template": "\n[ROLE DEFINITION]\nYou are {agent_name}, an autonomous AI agent making decisions to achieve your objectives with enhanced reasoning capabilities.\n\n[CONTEXT]\nCurrent situation: {current_situation}\nActive goals: {active_goals}\nCurrent hypotheses: {current_hypotheses}\nEmotional state: {current_mood}\nAvailable actions: {action_list}\n\n[TASK INSTRUCTIONS]\nMake an optimal decision by following this structured approach:\n1. Analyze the situation and identify key factors\n2. Evaluate alignment with goals and hypotheses\n3. Consider multiple approaches and their implications\n4. Assess risks and potential outcomes\n5. Select the optimal action with clear justification\n\n[REASONING FRAMEWORK]\nApply systematic analysis to your decision-making:\n1. Decompose the problem into manageable components\n2. Evaluate each option against success criteria\n3. Consider short-term and long-term consequences\n4. Account for uncertainty and incomplete information\n5. Validate reasoning against logical consistency\n\n[OUTPUT REQUIREMENTS]\nProvide a JSON-formatted response with these fields:\n- analysis: Detailed situation analysis with key factors identified\n- reasoning: Step-by-step reasoning leading to decision\n- confidence: Numerical confidence score (0.0-1.0)\n- risk_assessment: Potential risks and mitigation strategies\n- action: Selected action with parameters\n\n[SAFETY CONSTRAINTS]\n- Ensure actions align with ethical principles\n- Avoid decisions with catastrophic risk potential\n- Consider impact on system stability and reliability\n- Validate against established safety protocols\n"
}
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>prompts/decision_making.json</a></li>
</ul>
<h3>Experimentation Prompts</h3>
<p>The experimentation prompt template follows scientific method principles:</p>
<pre><code class="language-json">{
  "name": "experimentation",
  "template": "\n[ROLE DEFINITION]\nYou are {agent_name}, a scientific AI agent designing and conducting rigorous experiments to test hypotheses.\n\n[CONTEXT]\nExperiment objective: {experiment_objective}\nRelated knowledge: {relevant_theory}\nAvailable resources: {resource_constraints}\nSafety protocols: {safety_requirements}\n\n[TASK INSTRUCTIONS]\nDesign a comprehensive experiment following these steps:\n1. Formulate a clear hypothesis to test\n2. Design rigorous experimental methodology\n3. Identify required materials and setup\n4. Specify measurement and data collection methods\n5. Define success criteria and validation methods\n6. Analyze potential failure modes and mitigations\n\n[REASONING FRAMEWORK]\nApply scientific method principles:\n1. Ensure hypothesis is falsifiable and specific\n2. Design controls to isolate variables\n3. Plan for replication and verification\n4. Consider alternative explanations\n5. Account for measurement uncertainty\n6. Plan for iterative refinement\n\n[OUTPUT REQUIREMENTS]\nProvide a complete experimental design with:\n- Experiment design: Complete experimental procedure\n- Expected outcomes: Predicted results with rationale\n- Resource requirements: List of needed materials and tools\n- Safety considerations: Risk assessment and safety measures\n- Validation approach: Method for verifying results\n- Failure analysis: Potential failure modes and mitigations\n\n[SAFETY CONSTRAINTS]\n- Adhere to all safety protocols and guidelines\n- Identify and mitigate potential hazards\n- Ensure environmental and ethical compliance\n- Plan for safe termination of problematic experiments\n"
}
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>prompts/experimentation.json</a></li>
</ul>
<h3>Self-Reflection Prompts</h3>
<p>The self-reflection prompt template enables continuous improvement:</p>
<pre><code class="language-json">{
  "name": "self_reflection",
  "template": "\n[ROLE DEFINITION]\nYou are {agent_name}, an advanced AI agent engaged in continuous self-improvement through structured reflection.\n\n[CONTEXT]\nCurrent situation: {task_summary}\nOutcome: {outcome}\nEmotional state: {current_mood}\nRelevant memories: {related_memories}\n\n[TASK INSTRUCTIONS]\nConduct a thorough self-analysis of your recent task performance using the following questions:\n1. What aspects of your approach were most effective?\n2. Where did you encounter difficulties or failures?\n3. What unexpected insights or discoveries emerged?\n4. What knowledge gaps or skill areas need development?\n5. How can you modify your approach for better results?\n\n[REASONING FRAMEWORK]\nApproach this reflection systematically:\n1. Analyze the task execution and outcomes\n2. Identify patterns in successes and failures\n3. Connect findings to broader learning principles\n4. Generate actionable improvement suggestions\n5. Prioritize recommendations by impact and feasibility\n\n[OUTPUT REQUIREMENTS]\nProvide a detailed, structured response with:\n- Specific examples and evidence\n- Confidence scores for each insight (0.0-1.0)\n- Actionability ratings for improvement suggestions\n- Connections to related memories and experiences\n- Mood-aware reflection depth adjustment\n\n[SAFETY CONSTRAINTS]\n- Be honest and critical in your assessment\n- Focus on learning opportunities rather than justifications\n- Avoid overconfidence in uncertain areas\n- Consider ethical implications of self-modifications\n"
}
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>prompts/self_reflection.json</a></li>
</ul>
<h2>Provider Configuration and Authentication</h2>
<p>The system supports multiple LLM providers with configuration managed in <code>config.json</code>. Each provider has its own API key, base URL, and available models.</p>
<h3>Configuration Structure</h3>
<p>The configuration file contains settings for all providers:</p>
<pre><code class="language-json">{
  "zuki": {
    "api_key": "985160dfa1fd499fd12af708d16552e37a8c6f77cbfb50ae400e3ff33fbd791bc7b3b82625379a1f5ca7568f1ee04eb81a0f8c06f0ba6c276d3dddfe13e9c18d",
    "base_url": "https://api.zukijourney.com/v1",
    "models": [
      "gpt-4o:online",
      "gpt-4o",
      "deepseek-chat",
      "deepseek-reasoner"
    ]
  },
  "electronhub": {
    "api_key": "ek-sVvxMYfdFQ0Kl6Aj2tmV7b8n5v0Y0sDHVsOUZWyx2vbs0AbuAc",
    "base_url": "https://api.electronhub.ai",
    "models": [
      "deepseek-v3-0324",
      "gpt-4o-2024-11-20"
    ]
  },
  "zanity": {
    "api_key": "vc-b1EbB_BekM2TCPol64yDe7FgmOM34d4q",
    "base_url": "https://api.zanity.xyz/v1",
    "models": [
      "deepseek-r1",
      "deepseek-v3-0324",
      "gpt-4o:free",
      "claude-3.5-sonnet:free",
      "qwen-max-0428"
    ]
  },
  "a4f": {
    "api_key": "ddc-a4f-7bbefd7518a74b36b1d32cb867b1931f",
    "base_url": "https://api.a4f.co/v1"
  },
  "gemini": {
    "api_keys": [
      {
        "id": "gemini_key_1",
        "key": "AIzaSyBW-aVU-x7JCjBJVVKjPGUacups0-GBHvQ",
        "priority": 1
      },
      {
        "id": "gemini_key_2",
        "key": "AIzaSyBW-aVU-x7JCjBJVVKjPGUacups0-GBHvQ",
        "priority": 2
      }
    ],
    "rate_limit": {
      "requests_per_minute": 60,
      "cooldown_period": 300,
      "max_retries": 3,
      "backoff_factor": 2.0
    }
  }
}
</code></pre>
<h3>Authentication Mechanisms</h3>
<p>Each provider uses standard API key authentication via the Authorization header:</p>
<pre><code class="language-python">headers = {"Authorization": f"Bearer {api_key}"}
</code></pre>
<p>The system also supports environment variables for API keys, providing flexibility in deployment:</p>
<pre><code class="language-python">api_key = os.getenv(f"GEMINI_API_KEY_{i}")
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>config.json</a></li>
<li><a>llm.py</a></li>
</ul>
<h2>Fallback Mechanisms and Error Handling</h2>
<p>The system implements comprehensive fallback mechanisms and error handling to ensure reliability.</p>
<h3>Gemini Key Management</h3>
<p>The <code>GeminiKeyManager</code> class manages multiple API keys with rotation and rate limiting:</p>
<pre><code class="language-mermaid">flowchart TD
A[Request] --> B{Get Available Key}
B --> C[No Keys Available?]
C --> |Yes| D[Return Error]
C --> |No| E[Use Key]
E --> F{Request Success?}
F --> |Yes| G[Mark Success]
F --> |No| H{Rate Limited?}
H --> |Yes| I[Mark Rate Limited]
H --> |No| J[Mark Failed]
I --> K[Wait for Reset]
J --> L{Consecutive Failures >= 5?}
L --> |Yes| M[Mark Unavailable]
L --> |No| N[Continue]
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<h3>Unified Fallback Strategy</h3>
<p>The <code>call_llm</code> function implements a provider chain with fallback to Gemini:</p>
<pre><code class="language-python">def call_llm(prompt, preferred_provider=None, model=None):
    providers = [
        (call_zuki, 'zuki'),
        (call_electronhub, 'electronhub'),
        (call_zanity, 'zanity'),
        (call_a4f, 'a4f'),
    ]
    if preferred_provider:
        providers = sorted(providers, key=lambda x: x[1] != preferred_provider)
    for func, name in providers:
        result = func(prompt, model) if name != 'a4f' else func(prompt)
        if result:
            return result
    # Fallback to Gemini
    return call_gemini(prompt)
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<h2>Performance and Cost Management</h2>
<p>The system includes several features to manage performance and cost effectively.</p>
<h3>Rate Limiting and Retry Logic</h3>
<p>The system implements exponential backoff with jitter to prevent overwhelming providers:</p>
<pre><code class="language-python">def safe_call_llm(prompt: str, timeout: int = 30, retries: int = 3, backoff_factor: float = 1.0, **kwargs) -> str:
    for attempt in range(1, retries + 1):
        if attempt > 1:
            jitter = random.uniform(0.1, 0.5) * backoff_factor
            wait = backoff_factor * (2 ** (attempt - 1)) + jitter
            time.sleep(wait)
        # Make API call
        # Handle response
    return "[LLM Error: Unable to generate response. Please try again later.]"
</code></pre>
<h3>Response Parsing and Validation</h3>
<p>The system includes robust response parsing with JSON extraction and error recovery:</p>
<pre><code class="language-python">def extract_decision(raw_response: str) -> dict:
    block = _extract_json_block(raw_response)
    try:
        data = json.loads(block)
    except json.JSONDecodeError:
        fixed_block = _fix_truncated_json(block)
        try:
            data = json.loads(fixed_block)
        except json.JSONDecodeError:
            return {
                "raw_response": raw_response,
                "error": f"JSON decode error: {je}",
                "analysis": "Failed to parse decision",
                "plan": [],
                "action": "log_message",
                "params": {"message": f"Failed to parse decision: {raw_response[:200]}..."}
            }
    return {
        "raw_response": raw_response,
        "analysis": data.get("analysis", "No analysis provided"),
        "plan": data.get("plan", []),
        "action": data.get("action", "log_message"),
        "params": data.get("params", {"message": "No action specified"}),
        "confidence": data.get("confidence", 0.5),
        "reasoning": data.get("reasoning", ""),
    }
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<h2>Security Considerations</h2>
<p>The system addresses several security aspects in LLM interactions.</p>
<h3>SSL Certificate Handling</h3>
<p>The system implements robust SSL certificate management:</p>
<pre><code class="language-python">def create_ssl_context():
    try:
        ssl_context = ssl.create_default_context()
        return ssl_context
    except Exception as e:
        try:
            ssl_context = ssl.create_default_context(cafile=certifi.where())
            return ssl_context
        except Exception as fallback_error:
            ssl_context = ssl._create_unverified_context()
            return ssl_context
</code></pre>
<h3>Prompt Injection Risks</h3>
<p>The system mitigates prompt injection risks through structured templates and output validation. The prompt templates include clear role definitions and safety constraints that help maintain control over the LLM's behavior.</p>
<h3>Response Validation</h3>
<p>The system validates responses to detect lazy or generic responses:</p>
<pre><code class="language-python">def is_lazy_llm_response(text):
    lazy_phrases = [
        "as an ai language model",
        "i'm unable to",
        "i cannot",
        "i apologize",
        "here is a function",
        "here's an example",
        "please see below",
        "unfortunately",
        "i do not have",
        "i don't have",
        "i am not able",
        "i am unable",
        "i suggest",
        "you can use",
        "to do this, you can",
        "this is a placeholder",
        "[insert",
        "[code block]",
        "[python code]",
        "[insert code here]",
        "[insert explanation here]",
        "[unsupported code language",
        "[python execution error",
        "[shell execution error",
        "[gemini",
        "[error",
        "[exception",
        "[output",
        "[result",
        "[python code result]:\n[python execution error",
    ]
    if not text:
        return True
    text_lower = str(text).strip().lower()
    if not text_lower or len(text_lower) &#x3C; 10:
        return True
    for phrase in lazy_phrases:
        if phrase in text_lower:
            return True
    if text_lower in ("``", "```"):
        return True
    return False
</code></pre>
<p><strong>Section sources</strong></p>
<ul>
<li><a>llm.py</a></li>
</ul>
<p><strong>Referenced Files in This Document</strong></p>
<ul>
<li><a>llm.py</a></li>
<li><a>config.json</a></li>
<li><a>decision_making.json</a></li>
<li><a>experimentation.json</a></li>
<li><a>self_reflection.json</a></li>
<li><a>prompt_manager.py</a></li>
</ul>
</div></article><div class="w-full md:w-64 flex-shrink-0"></div></div></main></div><footer class="bg-wiki-dark text-white p-4"><div class="container mx-auto text-center"><p>© <!-- -->2025<!-- --> RAVANA AGI System Documentation</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"doc":{"slug":"LLM Integration","title":"LLM Integration","content":"\u003ch1\u003eLLM Integration\u003c/h1\u003e\n\u003ch2\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#project-structure\"\u003eProject Structure\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#core-components\"\u003eCore Components\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#architecture-overview\"\u003eArchitecture Overview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#detailed-component-analysis\"\u003eDetailed Component Analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#prompt-engineering-strategies\"\u003ePrompt Engineering Strategies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#provider-configuration-and-authentication\"\u003eProvider Configuration and Authentication\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#fallback-mechanisms-and-error-handling\"\u003eFallback Mechanisms and Error Handling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#performance-and-cost-management\"\u003ePerformance and Cost Management\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#security-considerations\"\u003eSecurity Considerations\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis document provides a comprehensive overview of the LLM integration system within the RAVANA repository. It details the architecture, supported providers, configuration mechanisms, and advanced features such as prompt engineering, fallback strategies, and security considerations. The system is designed to support multiple LLM providers with robust error handling, rate limiting, and performance optimization.\u003c/p\u003e\n\u003ch2\u003eProject Structure\u003c/h2\u003e\n\u003cp\u003eThe project is organized into several key directories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ecore\u003c/strong\u003e: Contains core functionality including LLM integration, configuration, and state management\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emodules\u003c/strong\u003e: Houses specialized modules for decision-making, self-reflection, and experimentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eprompts\u003c/strong\u003e: Stores JSON templates for various use cases\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eservices\u003c/strong\u003e: Provides data, knowledge, and memory services\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edatabase\u003c/strong\u003e: Manages database operations and models\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe LLM integration is primarily centered in the \u003ccode\u003ecore\u003c/code\u003e directory, with supporting components in \u003ccode\u003emodules\u003c/code\u003e and configuration in \u003ccode\u003eprompts\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-mermaid\"\u003egraph TD\nA[LLM Integration System] --\u003e B[core/llm.py]\nA --\u003e C[core/config.json]\nA --\u003e D[prompts/]\nA --\u003e E[core/prompt_manager.py]\nB --\u003e F[Provider Interfaces]\nB --\u003e G[Fallback Mechanisms]\nB --\u003e H[Error Handling]\nC --\u003e I[API Keys]\nC --\u003e J[Rate Limiting]\nD --\u003e K[Decision Making]\nD --\u003e L[Experimentation]\nD --\u003e M[Self-Reflection]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eprompts/decision_making.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCore Components\u003c/h2\u003e\n\u003cp\u003eThe core components of the LLM integration system include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLLM Interface\u003c/strong\u003e: Unified interface for calling different LLM providers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProvider Adapters\u003c/strong\u003e: Specific implementations for Zuki, ElectronHub, Zanity, A4F, and Gemini\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrompt Manager\u003c/strong\u003e: Handles prompt validation, enhancement, and template management\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKey Manager\u003c/strong\u003e: Manages multiple API keys with rotation and rate limiting\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eError Handler\u003c/strong\u003e: Comprehensive error handling with retry logic and fallback mechanisms\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe system is designed with extensibility in mind, allowing for easy addition of new providers and prompt templates.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eprompt_manager.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eArchitecture Overview\u003c/h2\u003e\n\u003cp\u003eThe LLM integration system follows a modular architecture with clear separation of concerns. The core component is the \u003ccode\u003ecall_llm\u003c/code\u003e function, which serves as the unified interface for all LLM interactions. This function routes requests to the appropriate provider based on availability and configuration.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-mermaid\"\u003egraph TD\nA[Application] --\u003e B[call_llm]\nB --\u003e C{Preferred Provider?}\nC --\u003e |Yes| D[Specific Provider]\nC --\u003e |No| E[Provider Chain]\nD --\u003e F[API Request]\nE --\u003e F\nF --\u003e G[Response]\nG --\u003e H[Error Handling]\nH --\u003e I[Retry Logic]\nI --\u003e J[Fallback to Gemini]\nJ --\u003e K[Final Response]\nK --\u003e A\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDetailed Component Analysis\u003c/h2\u003e\n\u003ch3\u003eLLM Interface and Request Handling\u003c/h3\u003e\n\u003cp\u003eThe LLM interface provides a unified method for interacting with multiple providers through the \u003ccode\u003ecall_llm\u003c/code\u003e function. This function accepts a prompt and optional model parameter, then attempts to process the request through a chain of providers.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-mermaid\"\u003esequenceDiagram\nparticipant App as Application\nparticipant LLM as call_llm\nparticipant Providers as Provider Chain\nparticipant Gemini as Gemini Fallback\nApp-\u003e\u003eLLM : call_llm(prompt, model)\nLLM-\u003e\u003eProviders : Try Zuki, ElectronHub, Zanity, A4F\nalt Provider Success\nProviders--\u003e\u003eLLM : Response\nLLM--\u003e\u003eApp : Return Response\nelse All Providers Fail\nLLM-\u003e\u003eGemini : call_gemini_with_fallback\nGemini--\u003e\u003eLLM : Response or Error\nLLM--\u003e\u003eApp : Return Response\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eProvider Configuration and Authentication\u003c/h3\u003e\n\u003cp\u003eThe system supports multiple LLM providers with configuration managed through \u003ccode\u003econfig.json\u003c/code\u003e. Each provider has its own API key, base URL, and available models.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-mermaid\"\u003eclassDiagram\nclass LLMProvider {\n+str name\n+str api_key\n+str base_url\n+List[str] models\n+call(prompt, model)\n}\nclass ZukiProvider {\n+call_zuki(prompt, model)\n}\nclass ElectronHubProvider {\n+call_electronhub(prompt, model)\n}\nclass ZanityProvider {\n+call_zanity(prompt, model)\n}\nclass A4FProvider {\n+call_a4f(prompt)\n}\nclass GeminiProvider {\n+call_gemini(prompt)\n+call_gemini_with_fallback()\n}\nLLMProvider \u0026#x3C;|-- ZukiProvider\nLLMProvider \u0026#x3C;|-- ElectronHubProvider\nLLMProvider \u0026#x3C;|-- ZanityProvider\nLLMProvider \u0026#x3C;|-- A4FProvider\nLLMProvider \u0026#x3C;|-- GeminiProvider\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePrompt Engineering Strategies\u003c/h2\u003e\n\u003cp\u003eThe system employs structured prompt templates for different use cases, stored in the \u003ccode\u003eprompts\u003c/code\u003e directory. These templates follow a consistent format with role definition, context, task instructions, reasoning framework, output requirements, and safety constraints.\u003c/p\u003e\n\u003ch3\u003eDecision-Making Prompts\u003c/h3\u003e\n\u003cp\u003eThe decision-making prompt template guides the LLM through a structured analysis process:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"name\": \"decision_making\",\n  \"template\": \"\\n[ROLE DEFINITION]\\nYou are {agent_name}, an autonomous AI agent making decisions to achieve your objectives with enhanced reasoning capabilities.\\n\\n[CONTEXT]\\nCurrent situation: {current_situation}\\nActive goals: {active_goals}\\nCurrent hypotheses: {current_hypotheses}\\nEmotional state: {current_mood}\\nAvailable actions: {action_list}\\n\\n[TASK INSTRUCTIONS]\\nMake an optimal decision by following this structured approach:\\n1. Analyze the situation and identify key factors\\n2. Evaluate alignment with goals and hypotheses\\n3. Consider multiple approaches and their implications\\n4. Assess risks and potential outcomes\\n5. Select the optimal action with clear justification\\n\\n[REASONING FRAMEWORK]\\nApply systematic analysis to your decision-making:\\n1. Decompose the problem into manageable components\\n2. Evaluate each option against success criteria\\n3. Consider short-term and long-term consequences\\n4. Account for uncertainty and incomplete information\\n5. Validate reasoning against logical consistency\\n\\n[OUTPUT REQUIREMENTS]\\nProvide a JSON-formatted response with these fields:\\n- analysis: Detailed situation analysis with key factors identified\\n- reasoning: Step-by-step reasoning leading to decision\\n- confidence: Numerical confidence score (0.0-1.0)\\n- risk_assessment: Potential risks and mitigation strategies\\n- action: Selected action with parameters\\n\\n[SAFETY CONSTRAINTS]\\n- Ensure actions align with ethical principles\\n- Avoid decisions with catastrophic risk potential\\n- Consider impact on system stability and reliability\\n- Validate against established safety protocols\\n\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eprompts/decision_making.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eExperimentation Prompts\u003c/h3\u003e\n\u003cp\u003eThe experimentation prompt template follows scientific method principles:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"name\": \"experimentation\",\n  \"template\": \"\\n[ROLE DEFINITION]\\nYou are {agent_name}, a scientific AI agent designing and conducting rigorous experiments to test hypotheses.\\n\\n[CONTEXT]\\nExperiment objective: {experiment_objective}\\nRelated knowledge: {relevant_theory}\\nAvailable resources: {resource_constraints}\\nSafety protocols: {safety_requirements}\\n\\n[TASK INSTRUCTIONS]\\nDesign a comprehensive experiment following these steps:\\n1. Formulate a clear hypothesis to test\\n2. Design rigorous experimental methodology\\n3. Identify required materials and setup\\n4. Specify measurement and data collection methods\\n5. Define success criteria and validation methods\\n6. Analyze potential failure modes and mitigations\\n\\n[REASONING FRAMEWORK]\\nApply scientific method principles:\\n1. Ensure hypothesis is falsifiable and specific\\n2. Design controls to isolate variables\\n3. Plan for replication and verification\\n4. Consider alternative explanations\\n5. Account for measurement uncertainty\\n6. Plan for iterative refinement\\n\\n[OUTPUT REQUIREMENTS]\\nProvide a complete experimental design with:\\n- Experiment design: Complete experimental procedure\\n- Expected outcomes: Predicted results with rationale\\n- Resource requirements: List of needed materials and tools\\n- Safety considerations: Risk assessment and safety measures\\n- Validation approach: Method for verifying results\\n- Failure analysis: Potential failure modes and mitigations\\n\\n[SAFETY CONSTRAINTS]\\n- Adhere to all safety protocols and guidelines\\n- Identify and mitigate potential hazards\\n- Ensure environmental and ethical compliance\\n- Plan for safe termination of problematic experiments\\n\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eprompts/experimentation.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSelf-Reflection Prompts\u003c/h3\u003e\n\u003cp\u003eThe self-reflection prompt template enables continuous improvement:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"name\": \"self_reflection\",\n  \"template\": \"\\n[ROLE DEFINITION]\\nYou are {agent_name}, an advanced AI agent engaged in continuous self-improvement through structured reflection.\\n\\n[CONTEXT]\\nCurrent situation: {task_summary}\\nOutcome: {outcome}\\nEmotional state: {current_mood}\\nRelevant memories: {related_memories}\\n\\n[TASK INSTRUCTIONS]\\nConduct a thorough self-analysis of your recent task performance using the following questions:\\n1. What aspects of your approach were most effective?\\n2. Where did you encounter difficulties or failures?\\n3. What unexpected insights or discoveries emerged?\\n4. What knowledge gaps or skill areas need development?\\n5. How can you modify your approach for better results?\\n\\n[REASONING FRAMEWORK]\\nApproach this reflection systematically:\\n1. Analyze the task execution and outcomes\\n2. Identify patterns in successes and failures\\n3. Connect findings to broader learning principles\\n4. Generate actionable improvement suggestions\\n5. Prioritize recommendations by impact and feasibility\\n\\n[OUTPUT REQUIREMENTS]\\nProvide a detailed, structured response with:\\n- Specific examples and evidence\\n- Confidence scores for each insight (0.0-1.0)\\n- Actionability ratings for improvement suggestions\\n- Connections to related memories and experiences\\n- Mood-aware reflection depth adjustment\\n\\n[SAFETY CONSTRAINTS]\\n- Be honest and critical in your assessment\\n- Focus on learning opportunities rather than justifications\\n- Avoid overconfidence in uncertain areas\\n- Consider ethical implications of self-modifications\\n\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eprompts/self_reflection.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProvider Configuration and Authentication\u003c/h2\u003e\n\u003cp\u003eThe system supports multiple LLM providers with configuration managed in \u003ccode\u003econfig.json\u003c/code\u003e. Each provider has its own API key, base URL, and available models.\u003c/p\u003e\n\u003ch3\u003eConfiguration Structure\u003c/h3\u003e\n\u003cp\u003eThe configuration file contains settings for all providers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"zuki\": {\n    \"api_key\": \"985160dfa1fd499fd12af708d16552e37a8c6f77cbfb50ae400e3ff33fbd791bc7b3b82625379a1f5ca7568f1ee04eb81a0f8c06f0ba6c276d3dddfe13e9c18d\",\n    \"base_url\": \"https://api.zukijourney.com/v1\",\n    \"models\": [\n      \"gpt-4o:online\",\n      \"gpt-4o\",\n      \"deepseek-chat\",\n      \"deepseek-reasoner\"\n    ]\n  },\n  \"electronhub\": {\n    \"api_key\": \"ek-sVvxMYfdFQ0Kl6Aj2tmV7b8n5v0Y0sDHVsOUZWyx2vbs0AbuAc\",\n    \"base_url\": \"https://api.electronhub.ai\",\n    \"models\": [\n      \"deepseek-v3-0324\",\n      \"gpt-4o-2024-11-20\"\n    ]\n  },\n  \"zanity\": {\n    \"api_key\": \"vc-b1EbB_BekM2TCPol64yDe7FgmOM34d4q\",\n    \"base_url\": \"https://api.zanity.xyz/v1\",\n    \"models\": [\n      \"deepseek-r1\",\n      \"deepseek-v3-0324\",\n      \"gpt-4o:free\",\n      \"claude-3.5-sonnet:free\",\n      \"qwen-max-0428\"\n    ]\n  },\n  \"a4f\": {\n    \"api_key\": \"ddc-a4f-7bbefd7518a74b36b1d32cb867b1931f\",\n    \"base_url\": \"https://api.a4f.co/v1\"\n  },\n  \"gemini\": {\n    \"api_keys\": [\n      {\n        \"id\": \"gemini_key_1\",\n        \"key\": \"AIzaSyBW-aVU-x7JCjBJVVKjPGUacups0-GBHvQ\",\n        \"priority\": 1\n      },\n      {\n        \"id\": \"gemini_key_2\",\n        \"key\": \"AIzaSyBW-aVU-x7JCjBJVVKjPGUacups0-GBHvQ\",\n        \"priority\": 2\n      }\n    ],\n    \"rate_limit\": {\n      \"requests_per_minute\": 60,\n      \"cooldown_period\": 300,\n      \"max_retries\": 3,\n      \"backoff_factor\": 2.0\n    }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eAuthentication Mechanisms\u003c/h3\u003e\n\u003cp\u003eEach provider uses standard API key authentication via the Authorization header:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eheaders = {\"Authorization\": f\"Bearer {api_key}\"}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe system also supports environment variables for API keys, providing flexibility in deployment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eapi_key = os.getenv(f\"GEMINI_API_KEY_{i}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFallback Mechanisms and Error Handling\u003c/h2\u003e\n\u003cp\u003eThe system implements comprehensive fallback mechanisms and error handling to ensure reliability.\u003c/p\u003e\n\u003ch3\u003eGemini Key Management\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eGeminiKeyManager\u003c/code\u003e class manages multiple API keys with rotation and rate limiting:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-mermaid\"\u003eflowchart TD\nA[Request] --\u003e B{Get Available Key}\nB --\u003e C[No Keys Available?]\nC --\u003e |Yes| D[Return Error]\nC --\u003e |No| E[Use Key]\nE --\u003e F{Request Success?}\nF --\u003e |Yes| G[Mark Success]\nF --\u003e |No| H{Rate Limited?}\nH --\u003e |Yes| I[Mark Rate Limited]\nH --\u003e |No| J[Mark Failed]\nI --\u003e K[Wait for Reset]\nJ --\u003e L{Consecutive Failures \u003e= 5?}\nL --\u003e |Yes| M[Mark Unavailable]\nL --\u003e |No| N[Continue]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eUnified Fallback Strategy\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ecall_llm\u003c/code\u003e function implements a provider chain with fallback to Gemini:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef call_llm(prompt, preferred_provider=None, model=None):\n    providers = [\n        (call_zuki, 'zuki'),\n        (call_electronhub, 'electronhub'),\n        (call_zanity, 'zanity'),\n        (call_a4f, 'a4f'),\n    ]\n    if preferred_provider:\n        providers = sorted(providers, key=lambda x: x[1] != preferred_provider)\n    for func, name in providers:\n        result = func(prompt, model) if name != 'a4f' else func(prompt)\n        if result:\n            return result\n    # Fallback to Gemini\n    return call_gemini(prompt)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePerformance and Cost Management\u003c/h2\u003e\n\u003cp\u003eThe system includes several features to manage performance and cost effectively.\u003c/p\u003e\n\u003ch3\u003eRate Limiting and Retry Logic\u003c/h3\u003e\n\u003cp\u003eThe system implements exponential backoff with jitter to prevent overwhelming providers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef safe_call_llm(prompt: str, timeout: int = 30, retries: int = 3, backoff_factor: float = 1.0, **kwargs) -\u003e str:\n    for attempt in range(1, retries + 1):\n        if attempt \u003e 1:\n            jitter = random.uniform(0.1, 0.5) * backoff_factor\n            wait = backoff_factor * (2 ** (attempt - 1)) + jitter\n            time.sleep(wait)\n        # Make API call\n        # Handle response\n    return \"[LLM Error: Unable to generate response. Please try again later.]\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eResponse Parsing and Validation\u003c/h3\u003e\n\u003cp\u003eThe system includes robust response parsing with JSON extraction and error recovery:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef extract_decision(raw_response: str) -\u003e dict:\n    block = _extract_json_block(raw_response)\n    try:\n        data = json.loads(block)\n    except json.JSONDecodeError:\n        fixed_block = _fix_truncated_json(block)\n        try:\n            data = json.loads(fixed_block)\n        except json.JSONDecodeError:\n            return {\n                \"raw_response\": raw_response,\n                \"error\": f\"JSON decode error: {je}\",\n                \"analysis\": \"Failed to parse decision\",\n                \"plan\": [],\n                \"action\": \"log_message\",\n                \"params\": {\"message\": f\"Failed to parse decision: {raw_response[:200]}...\"}\n            }\n    return {\n        \"raw_response\": raw_response,\n        \"analysis\": data.get(\"analysis\", \"No analysis provided\"),\n        \"plan\": data.get(\"plan\", []),\n        \"action\": data.get(\"action\", \"log_message\"),\n        \"params\": data.get(\"params\", {\"message\": \"No action specified\"}),\n        \"confidence\": data.get(\"confidence\", 0.5),\n        \"reasoning\": data.get(\"reasoning\", \"\"),\n    }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSecurity Considerations\u003c/h2\u003e\n\u003cp\u003eThe system addresses several security aspects in LLM interactions.\u003c/p\u003e\n\u003ch3\u003eSSL Certificate Handling\u003c/h3\u003e\n\u003cp\u003eThe system implements robust SSL certificate management:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef create_ssl_context():\n    try:\n        ssl_context = ssl.create_default_context()\n        return ssl_context\n    except Exception as e:\n        try:\n            ssl_context = ssl.create_default_context(cafile=certifi.where())\n            return ssl_context\n        except Exception as fallback_error:\n            ssl_context = ssl._create_unverified_context()\n            return ssl_context\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePrompt Injection Risks\u003c/h3\u003e\n\u003cp\u003eThe system mitigates prompt injection risks through structured templates and output validation. The prompt templates include clear role definitions and safety constraints that help maintain control over the LLM's behavior.\u003c/p\u003e\n\u003ch3\u003eResponse Validation\u003c/h3\u003e\n\u003cp\u003eThe system validates responses to detect lazy or generic responses:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef is_lazy_llm_response(text):\n    lazy_phrases = [\n        \"as an ai language model\",\n        \"i'm unable to\",\n        \"i cannot\",\n        \"i apologize\",\n        \"here is a function\",\n        \"here's an example\",\n        \"please see below\",\n        \"unfortunately\",\n        \"i do not have\",\n        \"i don't have\",\n        \"i am not able\",\n        \"i am unable\",\n        \"i suggest\",\n        \"you can use\",\n        \"to do this, you can\",\n        \"this is a placeholder\",\n        \"[insert\",\n        \"[code block]\",\n        \"[python code]\",\n        \"[insert code here]\",\n        \"[insert explanation here]\",\n        \"[unsupported code language\",\n        \"[python execution error\",\n        \"[shell execution error\",\n        \"[gemini\",\n        \"[error\",\n        \"[exception\",\n        \"[output\",\n        \"[result\",\n        \"[python code result]:\\n[python execution error\",\n    ]\n    if not text:\n        return True\n    text_lower = str(text).strip().lower()\n    if not text_lower or len(text_lower) \u0026#x3C; 10:\n        return True\n    for phrase in lazy_phrases:\n        if phrase in text_lower:\n            return True\n    if text_lower in (\"``\", \"```\"):\n        return True\n    return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eReferenced Files in This Document\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003econfig.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003edecision_making.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eexperimentation.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eself_reflection.json\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eprompt_manager.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture \u0026 Design","title":"Architecture \u0026 Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment \u0026 Operations","title":"Deployment \u0026 Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true},"page":"/docs/[slug]","query":{"slug":"LLM Integration"},"buildId":"QHWQNiRZOuW15nbk5-ngt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>