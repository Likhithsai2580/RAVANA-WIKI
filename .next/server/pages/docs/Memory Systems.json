{"pageProps":{"doc":{"slug":"Memory Systems","title":"Memory Systems","content":"<h1>Memory Systems</h1>\n<h2>Update Summary</h2>\n<p><strong>Changes Made</strong></p>\n<ul>\n<li>Updated documentation to reflect corrected many-to-many relationship declarations in VLTM data models</li>\n<li>Added new section on Very Long-Term Memory (VLTM) system and its integration with existing memory systems</li>\n<li>Enhanced architectural diagrams to include VLTM components and integration patterns</li>\n<li>Added detailed information about memory bridges, flow direction, and synchronization mechanisms</li>\n<li>Updated code examples to reflect the use of junction tables for many-to-many relationships</li>\n<li>Added information about memory classification, conversion, and importance threshold evaluation</li>\n<li>Integrated LLM reliability improvements from core/llm.py into memory system documentation</li>\n<li>Added details about enhanced LLM error handling, JSON parsing, and response validation in memory operations</li>\n</ul>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#memory-architecture-overview\">Memory Architecture Overview</a></li>\n<li><a href=\"#episodic-memory-system\">Episodic Memory System</a></li>\n<li><a href=\"#multi-modal-embedding-service\">Multi-Modal Embedding Service</a></li>\n<li><a href=\"#advanced-search-engine\">Advanced Search Engine</a></li>\n<li><a href=\"#multi-modal-memory-orchestration\">Multi-Modal Memory Orchestration</a></li>\n<li><a href=\"#semantic-memory-and-knowledge-compression\">Semantic Memory and Knowledge Compression</a></li>\n<li><a href=\"#memoryservice-interface-and-crud-operations\">MemoryService Interface and CRUD Operations</a></li>\n<li><a href=\"#memory-retrieval-patterns\">Memory Retrieval Patterns</a></li>\n<li><a href=\"#consolidation-triggers-and-retention-policies\">Consolidation Triggers and Retention Policies</a></li>\n<li><a href=\"#performance-considerations\">Performance Considerations</a></li>\n<li><a href=\"#debugging-memory-issues\">Debugging Memory Issues</a></li>\n<li><a href=\"#very-long-term-memory-system\">Very Long-Term Memory System</a></li>\n<li><a href=\"#conclusion\">Conclusion</a></li>\n</ol>\n<h2>Introduction</h2>\n<p>The RAVANA system implements a dual-memory architecture combining episodic and semantic memory systems to enable long-term learning and contextual awareness. This document details the design, implementation, and operational characteristics of these memory systems, focusing on their storage mechanisms, retrieval patterns, and integration points. The system leverages PostgreSQL with pgvector for similarity-based retrieval and employs LLM-driven knowledge compression to transform raw experiences into structured semantic summaries. Recent enhancements have introduced multi-modal memory processing with support for text, audio, and image content, enabling cross-modal search and unified embedding generation. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that integrates with existing memory systems through configurable memory bridges, enabling strategic knowledge consolidation and cross-system synchronization. The memory system has been enhanced with improved LLM reliability features including detailed logging, enhanced error handling, and robust JSON parsing to ensure consistent memory operations.</p>\n<h2>Memory Architecture Overview</h2>\n<p>``mermaid\ngraph TB\nsubgraph \"Memory Systems\"\nEM[Episodic Memory]\nSM[Semantic Memory]\nVLTM[Very Long-Term Memory]\nend\nsubgraph \"Processing\"\nEX[Memory Extraction]\nCO[Consolidation]\nKC[Knowledge Compression]\nES[Embedding Service]\nSE[Search Engine]\nMI[Memory Integration]\nend\nsubgraph \"Storage\"\nPG[PostgreSQL with pgvector]\nSQ[SQLModel Database]\nFS[File System]\nend\nsubgraph \"Access\"\nMS[MemoryService]\nMMS[MultiModalService]\nKS[KnowledgeService]\nVIM[VLTM Integration Manager]\nend\nUserInput --> EX\nEX --> EM\nEM --> CO\nCO --> PG\nKC --> SM\nSM --> SQ\nMS --> EM\nMMS --> EM\nKS --> SM\nEM --> |Vector Search| SE\nSM --> |Semantic Search| KS\nPG --> EM\nSQ --> SM\nFS --> SM\nES --> PG\nSE --> PG\nMMS --> ES\nMMS --> SE\nEM --> MI\nSM --> MI\nMI --> VLTM\nVLTM --> KS\nVIM --> MI\nstyle EM fill:#f9f,stroke:#333\nstyle SM fill:#bbf,stroke:#333\nstyle VLTM fill:#9f9,stroke:#333</p>\n<pre><code>\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [knowledge_service.py](file://services/knowledge_service.py#L0-L255)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)\n\n## Episodic Memory System\n\nThe episodic memory system captures and stores specific events and interactions as discrete memory records. Each memory is stored with rich metadata and indexed using vector embeddings for similarity-based retrieval. Recent updates have replaced ChromaDB with PostgreSQL enhanced with pgvector extension, enabling robust multi-modal storage and advanced querying capabilities.\n\n### Storage Mechanism with PostgreSQL and SentenceTransformers\n\nEpisodic memories are stored in PostgreSQL with pgvector extension, providing a production-grade database solution for vector similarity search. The system uses SentenceTransformers to generate embeddings for memory texts, enabling semantic search capabilities.\n\n``mermaid\nclassDiagram\nclass MemoryRecord {\n+uuid id\n+ContentType content_type\n+string content_text\n+string file_path\n+List[float] text_embedding\n+List[float] image_embedding\n+List[float] audio_embedding\n+List[float] unified_embedding\n+datetime created_at\n+datetime last_accessed\n+int access_count\n+MemoryType memory_type\n+float emotional_valence\n+List[str] tags\n}\nclass PostgreSQLStore {\n+save_memory_record(record)\n+get_memory_record(id)\n+vector_search(embedding)\n+get_memory_statistics()\n}\nclass MultiModalMemoryService {\n+process_text_memory(text)\n+process_audio_memory(path)\n+process_image_memory(path)\n+search_memories(request)\n}\nPostgreSQLStore --> MemoryRecord : \"stores\"\nMultiModalMemoryService --> PostgreSQLStore : \"uses\"\nMultiModalMemoryService --> MemoryRecord : \"manages\"\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>models.py</a></li>\n<li><a>postgresql_store.py</a></li>\n<li><a>multi_modal_service.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>memory.py</a></li>\n<li><a>postgresql_store.py</a></li>\n<li><a>models.py</a></li>\n</ul>\n<h4>Embedding Generation and Storage</h4>\n<p>The system uses the <code>all-MiniLM-L6-v2</code> SentenceTransformer model to generate 384-dimensional text embeddings. For multi-modal content, specialized embedding generation is implemented:</p>\n<pre><code class=\"language-python\">class EmbeddingService:\n    def __init__(self, text_model_name: str = \"all-MiniLM-L6-v2\"):\n        self.text_model_name = text_model_name\n        self.text_embedding_dim = 384\n        self.image_embedding_dim = 512\n        self.audio_embedding_dim = 512\n        self.unified_embedding_dim = 1024\n</code></pre>\n<p>Memories are stored with comprehensive metadata including creation timestamp, access statistics, content type, and confidence scores:</p>\n<pre><code class=\"language-python\">memory_record = MemoryRecord(\n    content_type=content_type,\n    content_text=content_text,\n    file_path=file_path,\n    memory_type=memory_type,\n    tags=tags or [],\n    emotional_valence=emotional_valence,\n    confidence_score=confidence_score,\n    created_at=datetime.utcnow()\n)\n</code></pre>\n<h4>Memory Extraction Process</h4>\n<p>The system extracts memories from conversations using an LLM-powered extraction process. The <code>extract_memories_from_conversation</code> method analyzes user-AI interactions and identifies key information to store:</p>\n<pre><code class=\"language-python\">async def extract_memories_from_conversation(self, request: ConversationRequest) -> MemoriesList:\n    prompt = f\"\"\"\n    You are a memory extraction module for an AGI. Your task is to analyze a conversation \n    and identify key pieces of information to be stored in the AGI's long-term memory.\n\n    Focus on extracting:\n    - Key facts and information\n    - User preferences and characteristics\n    - Important goals, plans, or intentions\n    - Notable events or experiences\n    - Emotional context if relevant\n\n    Guidelines:\n    - Each memory should be a single, self-contained statement\n    - Keep memories concise (under 30 words)\n    - Prefer information that is likely to be relevant long-term\n    - Do not store transitory conversational details\n    - Output as a JSON object with a \"memories\" array\n\n    Conversation:\n    User: {request.user_input}\n    AI: {request.ai_output}\n    \"\"\"\n</code></pre>\n<p>The extraction focuses on key facts, user preferences, major goals, and core beliefs while filtering out transitory conversational details.</p>\n<h2>Multi-Modal Embedding Service</h2>\n<p>The EmbeddingService provides multi-modal embedding generation for text, audio, and image content, enabling cross-modal retrieval and unified embedding creation.</p>\n<h3>Multi-Modal Embedding Generation</h3>\n<p>The embedding service supports multiple content types with specialized processing:</p>\n<p>``mermaid\nflowchart TD\nA[Input Content] --> B{Content Type}\nB --> C[Text]\nB --> D[Audio]\nB --> E[Image]\nC --> F[Generate Text Embeddingusing SentenceTransformer]\nD --> G[Process Audio with WhisperExtract Features]\nE --> H[Extract Image FeaturesColor, Dimensions, Histogram]\nF --> I[Store text_embedding]\nG --> J[Store audio_embedding]\nH --> K[Store image_embedding]\nI --> L[Generate Unified Embedding]\nJ --> L\nK --> L\nL --> M[Store unified_embedding]</p>\n<pre><code>\n**Diagram sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n- [models.py](file://modules/episodic_memory/models.py#L0-L250)\n\n**Section sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n\n#### Text Embedding Implementation\nText embeddings are generated using SentenceTransformers with caching for performance:\n\n```python\nasync def generate_text_embedding(self, text: str) -> List[float]:\n    # Check cache first\n    cached = self.cache.get(text, self.text_model_name)\n    if cached is not None:\n        return cached\n    \n    self._load_text_model()\n    \n    try:\n        loop = asyncio.get_event_loop()\n        embedding = await loop.run_in_executor(\n            None,\n            lambda: self.text_model.encode(text, convert_to_tensor=False, normalize_embeddings=True)\n        )\n        \n        embedding_list = embedding.tolist()\n        self.cache.put(text, self.text_model_name, embedding_list)\n        \n        return embedding_list\n        \n    except Exception as e:\n        logger.error(f\"Text embedding generation failed: {e}\")\n        return [0.0] * self.text_embedding_dim\n</code></pre>\n<h4>Audio Embedding Implementation</h4>\n<p>Audio embeddings are generated from Whisper transcription and audio features:</p>\n<pre><code class=\"language-python\">async def generate_audio_embedding(self, audio_features: Dict[str, Any]) -> List[float]:\n    features = []\n    \n    # Extract numerical features from audio analysis\n    if \"mfcc\" in audio_features:\n        mfcc_data = audio_features[\"mfcc\"]\n        if \"mean\" in mfcc_data:\n            features.extend(mfcc_data[\"mean\"])\n        if \"std\" in mfcc_data:\n            features.extend(mfcc_data[\"std\"])\n    \n    if \"spectral_centroid\" in audio_features:\n        sc = audio_features[\"spectral_centroid\"]\n        features.extend([sc.get(\"mean\", 0.0), sc.get(\"std\", 0.0)])\n    \n    # Pad or truncate to desired dimension\n    if len(features) &#x3C; self.audio_embedding_dim:\n        features.extend([0.0] * (self.audio_embedding_dim - len(features)))\n    else:\n        features = features[:self.audio_embedding_dim]\n    \n    return features\n</code></pre>\n<h4>Image Embedding Implementation</h4>\n<p>Image embeddings are generated from visual features (placeholder for CLIP in production):</p>\n<pre><code class=\"language-python\">async def generate_image_embedding(self, image_path: str) -> List[float]:\n    try:\n        image = Image.open(image_path).convert('RGB')\n        \n        # Extract basic statistics\n        img_array = np.array(image)\n        features = []\n        \n        # Color statistics\n        for channel in range(3):  # RGB\n            channel_data = img_array[:, :, channel].flatten()\n            features.extend([\n                float(np.mean(channel_data)),\n                float(np.std(channel_data)),\n                float(np.median(channel_data)),\n                float(np.percentile(channel_data, 25)),\n                float(np.percentile(channel_data, 75))\n            ])\n        \n        # Image dimensions\n        features.extend([\n            float(image.width),\n            float(image.height),\n            float(image.width * image.height)  # Area\n        ])\n        \n        # Histogram features\n        hist, _ = np.histogram(img_array.flatten(), bins=32, range=(0, 256))\n        hist_normalized = hist / np.sum(hist)\n        features.extend(hist_normalized.tolist())\n        \n        # Pad or truncate to desired dimension\n        if len(features) &#x3C; self.image_embedding_dim:\n            features.extend([0.0] * (self.image_embedding_dim - len(features)))\n        else:\n            features = features[:self.image_embedding_dim]\n        \n        return features\n        \n    except Exception as e:\n        logger.error(f\"Image embedding generation failed for {image_path}: {e}\")\n        return [0.0] * self.image_embedding_dim\n</code></pre>\n<h4>Unified Embedding Generation</h4>\n<p>The system generates unified embeddings by combining modalities with weighted fusion:</p>\n<pre><code class=\"language-python\">async def generate_unified_embedding(self, memory_record: MemoryRecord) -> List[float]:\n    unified = []\n    \n    # Combine embeddings with weights\n    text_weight = 0.4\n    image_weight = 0.3\n    audio_weight = 0.3\n    \n    # Text embedding (weighted)\n    if memory_record.text_embedding:\n        text_emb = np.array(memory_record.text_embedding) * text_weight\n        unified.extend(text_emb.tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * text_weight))\n    \n    # Image embedding (weighted)\n    if memory_record.image_embedding:\n        image_emb = np.array(memory_record.image_embedding) * image_weight\n        unified.extend(image_emb[:int(self.unified_embedding_dim * image_weight)].tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * image_weight))\n    \n    # Audio embedding (weighted)\n    if memory_record.audio_embedding:\n        audio_emb = np.array(memory_record.audio_embedding) * audio_weight\n        unified.extend(audio_emb[:int(self.unified_embedding_dim * audio_weight)].tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * audio_weight))\n    \n    # Normalize the unified embedding\n    unified_array = np.array(unified)\n    norm = np.linalg.norm(unified_array)\n    if norm > 0:\n        unified_array = unified_array / norm\n    \n    return unified_array.tolist()\n</code></pre>\n<h2>Advanced Search Engine</h2>\n<p>The AdvancedSearchEngine provides sophisticated search capabilities including cross-modal search, similarity search, and hybrid search modes.</p>\n<h3>Cross-Modal Search Implementation</h3>\n<p>The search engine supports cross-modal queries where different content types can be used to search across modalities:</p>\n<pre><code class=\"language-python\">async def cross_modal_search(self, request: CrossModalSearchRequest) -> List[SearchResult]:\n    # Generate query embedding based on type\n    if request.query_type == ContentType.TEXT:\n        query_embedding = await self.embeddings.generate_text_embedding(request.query_content)\n    elif request.query_type == ContentType.AUDIO and self.whisper:\n        audio_result = await self.whisper.process_audio(request.query_content)\n        query_embedding = await self.embeddings.generate_text_embedding(\n            audio_result.get(\"transcript\", \"\")\n        )\n    elif request.query_type == ContentType.IMAGE:\n        query_embedding = await self.embeddings.generate_image_embedding(request.query_content)\n    else:\n        raise ValueError(f\"Unsupported query type: {request.query_type}\")\n    \n    # Search using unified embeddings\n    results = await self.postgres.vector_search(\n        embedding=query_embedding,\n        embedding_type=\"unified\",\n        limit=request.limit,\n        similarity_threshold=request.similarity_threshold,\n        content_types=request.target_types\n    )\n    \n    # Convert to SearchResult objects\n    search_results = []\n    for i, (memory_record, similarity) in enumerate(results):\n        search_results.append(SearchResult(\n            memory_record=memory_record,\n            similarity_score=similarity,\n            rank=i + 1,\n            search_metadata={\n                \"search_type\": \"cross_modal_specialized\",\n                \"query_type\": request.query_type.value,\n                \"target_types\": [ct.value for ct in request.target_types]\n            }\n        ))\n    \n    return search_results\n</code></pre>\n<h3>Similarity Search Implementation</h3>\n<p>Find memories similar to a given memory record:</p>\n<pre><code class=\"language-python\">async def find_similar_memories(self, \n                              memory_record: MemoryRecord, \n                              limit: int = 10,\n                              similarity_threshold: float = 0.7) -> List[SearchResult]:\n    # Use the best available embedding\n    if memory_record.unified_embedding:\n        embedding = memory_record.unified_embedding\n        embedding_type = \"unified\"\n    elif memory_record.text_embedding:\n        embedding = memory_record.text_embedding\n        embedding_type = \"text\"\n    elif memory_record.image_embedding:\n        embedding = memory_record.image_embedding\n        embedding_type = \"image\"\n    elif memory_record.audio_embedding:\n        embedding = memory_record.audio_embedding\n        embedding_type = \"audio\"\n    else:\n        logger.warning(\"No embeddings available for similarity search\")\n        return []\n    \n    # Search for similar memories\n    results = await self.postgres.vector_search(\n        embedding=embedding,\n        embedding_type=embedding_type,\n        limit=limit + 1,  # +1 to exclude the original\n        similarity_threshold=similarity_threshold\n    )\n    \n    # Convert to SearchResult objects and exclude the original\n    search_results = []\n    for i, (similar_record, similarity) in enumerate(results):\n        if similar_record.id != memory_record.id:  # Exclude the original\n            search_results.append(SearchResult(\n                memory_record=similar_record,\n                similarity_score=similarity,\n                rank=len(search_results) + 1,\n                search_metadata={\n                    \"search_type\": \"similarity\",\n                    \"reference_id\": str(memory_record.id),\n                    \"embedding_type\": embedding_type\n                }\n            ))\n    \n    return search_results[:limit]\n</code></pre>\n<h3>Hybrid Search Configuration</h3>\n<p>Configure weights for hybrid search combining vector and text search:</p>\n<pre><code class=\"language-python\">def configure_search_weights(self, vector_weight: float, text_weight: float):\n    \"\"\"\n    Configure the weights for hybrid search.\n    \n    Args:\n        vector_weight: Weight for vector similarity (0-1)\n        text_weight: Weight for text search (0-1)\n    \"\"\"\n    total_weight = vector_weight + text_weight\n    if total_weight > 0:\n        self.vector_weight = vector_weight / total_weight\n        self.text_weight = text_weight / total_weight\n        logger.info(f\"Updated search weights: vector={self.vector_weight:.2f}, text={self.text_weight:.2f}\")\n    else:\n        logger.warning(\"Invalid weights provided, keeping current configuration\")\n</code></pre>\n<h2>Multi-Modal Memory Orchestration</h2>\n<p>The MultiModalMemoryService orchestrates all components of the memory system, providing a unified interface for multi-modal operations.</p>\n<h3>Service Architecture</h3>\n<p>``mermaid\nclassDiagram\nclass MultiModalMemoryService {\n+initialize()\n+close()\n+process_text_memory(text)\n+process_audio_memory(path)\n+process_image_memory(path)\n+extract_memories_from_conversation(request)\n+search_memories(request)\n+find_similar_memories(id)\n+batch_process_files(request)\n+health_check()\n}\nclass PostgreSQLStore {\n+save_memory_record()\n+get_memory_record()\n+vector_search()\n}\nclass EmbeddingService {\n+generate_text_embedding()\n+generate_audio_embedding()\n+generate_image_embedding()\n+generate_unified_embedding()\n}\nclass WhisperAudioProcessor {\n+process_audio()\n}\nclass AdvancedSearchEngine {\n+search()\n+cross_modal_search()\n+find_similar_memories()\n}\nMultiModalMemoryService --> PostgreSQLStore : \"uses\"\nMultiModalMemoryService --> EmbeddingService : \"uses\"\nMultiModalMemoryService --> WhisperAudioProcessor : \"uses\"\nMultiModalMemoryService --> AdvancedSearchEngine : \"uses\"</p>\n<pre><code>\n**Diagram sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L0-L508)\n\n**Section sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n\n#### Text Memory Processing\nProcess and store text-based memories:\n\n```python\nasync def process_text_memory(self, \n                            text: str,\n                            memory_type: MemoryType = MemoryType.EPISODIC,\n                            tags: Optional[List[str]] = None,\n                            emotional_valence: Optional[float] = None) -> MemoryRecord:\n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.TEXT,\n        content_text=text,\n        memory_type=memory_type,\n        tags=tags or [],\n        emotional_valence=emotional_valence,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed text memory: {saved_record.id}\")\n    return saved_record\n</code></pre>\n<h4>Audio Memory Processing</h4>\n<p>Process and store audio memories with Whisper transcription:</p>\n<pre><code class=\"language-python\">async def process_audio_memory(self, \n                             audio_path: str,\n                             context: Optional[str] = None,\n                             memory_type: MemoryType = MemoryType.EPISODIC,\n                             tags: Optional[List[str]] = None) -> MemoryRecord:\n    # Process audio with Whisper\n    audio_result = await self.whisper_processor.process_audio(audio_path, context)\n    \n    # Create audio metadata\n    audio_metadata = self.whisper_processor.create_audio_metadata(audio_result)\n    \n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.AUDIO,\n        content_text=audio_result.get(\"transcript\"),\n        file_path=audio_path,\n        memory_type=memory_type,\n        tags=tags or [],\n        confidence_score=audio_result.get(\"confidence\", 0.8),\n        audio_metadata=audio_metadata,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed audio memory: {saved_record.id}\")\n    return saved_record\n</code></pre>\n<h4>Image Memory Processing</h4>\n<p>Process and store image memories:</p>\n<pre><code class=\"language-python\">async def process_image_memory(self, \n                             image_path: str,\n                             description: Optional[str] = None,\n                             memory_type: MemoryType = MemoryType.EPISODIC,\n                             tags: Optional[List[str]] = None) -> MemoryRecord:\n    # Create basic image metadata\n    from PIL import Image\n    with Image.open(image_path) as img:\n        width, height = img.size\n    \n    image_metadata = ImageMetadata(\n        width=width,\n        height=height,\n        scene_description=description\n    )\n    \n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.IMAGE,\n        content_text=description,\n        file_path=image_path,\n        memory_type=memory_type,\n        tags=tags or [],\n        image_metadata=image_metadata,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed image memory: {saved_record.id}\")\n    return saved_record\n</code></pre>\n<h4>Batch Processing</h4>\n<p>Process multiple files in batch with parallel processing support:</p>\n<pre><code class=\"language-python\">async def batch_process_files(self, request: BatchProcessRequest) -> BatchProcessResult:\n    # Process files\n    if request.parallel_processing:\n        # Process in parallel with limited concurrency\n        semaphore = asyncio.Semaphore(request.max_workers)\n        \n        async def process_with_semaphore(task):\n            async with semaphore:\n                return await task\n        \n        parallel_tasks = [process_with_semaphore(task) for task in tasks]\n        results = await asyncio.gather(*parallel_tasks, return_exceptions=True)\n    else:\n        # Process sequentially\n        for task in tasks:\n            result = await task\n            results.append(result)\n    \n    # Process results\n    processing_results = []\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            processing_results.append(ProcessingResult(\n                memory_record=None,\n                processing_time_ms=0,\n                success=False,\n                error_message=str(result)\n            ))\n            failed_count += 1\n        else:\n            processing_results.append(result)\n            if result.success:\n                successful_count += 1\n            else:\n                failed_count += 1\n    \n    return BatchProcessResult(\n        results=processing_results,\n        total_processed=len(request.file_paths),\n        successful_count=successful_count,\n        failed_count=failed_count,\n        total_time_ms=int(total_time)\n    )\n</code></pre>\n<h2>Semantic Memory and Knowledge Compression</h2>\n<p>The semantic memory system transforms episodic experiences into structured knowledge through a compression pipeline that identifies patterns, generalizes information, and creates concise summaries.</p>\n<h3>Knowledge Compression Pipeline</h3>\n<p>The knowledge compression pipeline converts raw experiences into semantic summaries using LLM-driven analysis:</p>\n<p>``mermaid\nflowchart TD\nA[Raw Experiences] --> B{KnowledgeCompression}\nB --> C[Pattern Recognition]\nC --> D[Merge Related Facts]\nD --> E[Deduplicate Redundant Info]\nE --> F[Generalize Specifics]\nF --> G[Create Summary]\nG --> H[Store in Semantic Memory]</p>\n<pre><code>\n**Diagram sources**\n- [main.py](file://modules/knowledge_compression/main.py#L0-L42)\n- [compression_prompts.py](file://modules/knowledge_compression/compression_prompts.py#L0-L5)\n\n**Section sources**\n- [main.py](file://modules/knowledge_compression/main.py#L0-L42)\n- [compressed_memory.py](file://modules/knowledge_compression/compressed_memory.py#L0-L17)\n\n#### Compression Implementation\nThe compression process is implemented in the `compress_knowledge` function, which uses an LLM to summarize accumulated logs:\n\n```python\ndef compress_knowledge(logs):\n    prompt = COMPRESSION_PROMPT.format(logs=json.dumps(logs, indent=2))\n    summary = call_llm(prompt)\n    entry = {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"summary\": summary\n    }\n    save_summary(entry)\n    return entry\n</code></pre>\n<p>The compression prompt instructs the LLM to produce structured summaries of new facts learned, key outcomes, and next goals:</p>\n<pre><code class=\"language-python\">COMPRESSION_PROMPT = (\n    \"You are an AI tasked with summarizing accumulated knowledge and logs. \"\n    \"Given the following logs, produce a concise summary report of new facts learned, key outcomes, and next goals.\\n\"\n    \"Logs: {logs}\\n\"\n    \"Respond in a clear, structured format.\"\n)\n</code></pre>\n<h4>Storage Mechanism</h4>\n<p>Compressed knowledge is stored as JSON files on the filesystem, with each summary entry containing a timestamp and the LLM-generated summary:</p>\n<pre><code class=\"language-python\">def save_summary(entry):\n    data = load_summaries()\n    data.append(entry)\n    with open(COMPRESSED_FILE, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2)\n</code></pre>\n<p>The system also integrates with SQLModel for database-backed semantic memory storage, where summaries are stored in a relational database with metadata:</p>\n<pre><code class=\"language-python\">class Summary(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    timestamp: str\n    summary_text: str\n    source: str\n    category: str\n    content_hash: str = Field(unique=True)\n</code></pre>\n<h2>MemoryService Interface and CRUD Operations</h2>\n<p>The <code>MemoryService</code> provides a unified interface for memory management operations, abstracting the underlying storage mechanisms.</p>\n<p>``mermaid\nclassDiagram\nclass MemoryService {\n+get_relevant_memories(query_text)\n+save_memories(memories)\n+extract_memories(user_input, ai_output)\n+consolidate_memories()\n}\nclass MemoryAPI {\n+get_relevant_memories_api(request)\n+save_memories_api(request)\n+extract_memories_api(request)\n+consolidate_memories_api(request)\n}\nMemoryService --> MemoryAPI : \"delegates\"</p>\n<pre><code>\n**Diagram sources**\n- [memory_service.py](file://services/memory_service.py#L0-L20)\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n\n**Section sources**\n- [memory_service.py](file://services/memory_service.py#L0-L20)\n\n### CRUD Operations\n\nThe MemoryService implements the following CRUD operations:\n\n#### Create: save_memories\nStores new memories in the episodic memory system:\n\n```python\nasync def save_memories(self, memories):\n    await asyncio.to_thread(save_memories, memories)\n</code></pre>\n<p>The operation runs in a separate thread to avoid blocking the event loop.</p>\n<h4>Read: get_relevant_memories</h4>\n<p>Retrieves memories relevant to a query using vector similarity search:</p>\n<pre><code class=\"language-python\">async def get_relevant_memories(self, query_text: str):\n    return await get_relevant_memories_api({\"query_text\": query_text})\n</code></pre>\n<h4>Update: extract_memories</h4>\n<p>Extracts and updates memories from new interactions:</p>\n<pre><code class=\"language-python\">async def extract_memories(self, user_input: str, ai_output: str):\n    return await extract_memories_api({\"user_input\": user_input, \"ai_output\": ai_output})\n</code></pre>\n<h4>Delete: consolidate_memories</h4>\n<p>Removes redundant memories during consolidation:</p>\n<pre><code class=\"language-python\">async def consolidate_memories(self):\n    from modules.episodic_memory.memory import ConsolidateRequest\n    return await consolidate_memories_api(ConsolidateRequest())\n</code></pre>\n<h2>Memory Retrieval Patterns</h2>\n<p>The system implements similarity-based retrieval patterns for efficient memory access.</p>\n<h3>Vector Search Implementation</h3>\n<p>Memory retrieval uses PostgreSQL's pgvector extension for vector search capabilities:</p>\n<pre><code class=\"language-python\">async def vector_search(self, \n                       embedding: List[float],\n                       embedding_type: str = \"text\",\n                       limit: int = 10,\n                       similarity_threshold: float = 0.7,\n                       content_types: Optional[List[ContentType]] = None) -> List[Tuple[MemoryRecord, float]]:\n    # Build query based on embedding type\n    embedding_column = f\"{embedding_type}_embedding\"\n    \n    where_conditions = [f\"{embedding_column} IS NOT NULL\"]\n    params = [embedding]\n    param_count = 1\n    \n    if content_types:\n        param_count += 1\n        where_conditions.append(f\"content_type = ANY(${param_count})\")\n        params.append([ct.value for ct in content_types])\n    \n    param_count += 1\n    where_conditions.append(f\"1 - ({embedding_column} &#x3C;=> ${param_count}) >= ${param_count + 1}\")\n    params.extend([embedding, similarity_threshold])\n    \n    query = f\"\"\"\n        SELECT *, 1 - ({embedding_column} &#x3C;=> $1) as similarity\n        FROM memory_records \n        WHERE {' AND '.join(where_conditions)}\n        ORDER BY {embedding_column} &#x3C;=> $1\n        LIMIT ${param_count + 2}\n    \"\"\"\n    params.append(limit)\n    \n    rows = await conn.fetch(query, *params)\n</code></pre>\n<h3>Retrieval Parameters</h3>\n<p>The retrieval process is configurable through the following parameters:</p>\n<ul>\n<li><strong>top_n</strong>: Maximum number of memories to return (default: 5)</li>\n<li><strong>similarity_threshold</strong>: Minimum similarity score for inclusion (default: 0.7)</li>\n</ul>\n<p>The similarity threshold acts as a filter to ensure only highly relevant memories are retrieved, preventing information overload.</p>\n<h3>Access Pattern Tracking</h3>\n<p>The system tracks memory access patterns by updating metadata on retrieval:</p>\n<pre><code class=\"language-python\"># Update access metadata for retrieved memories\nif ids_to_update:\n    chroma_collection.update(ids=ids_to_update, metadatas=metadatas_to_update)\n</code></pre>\n<p>Each retrieved memory has its <code>last_accessed</code> timestamp and <code>access_count</code> updated, enabling usage-based retention policies.</p>\n<h2>Consolidation Triggers and Retention Policies</h2>\n<p>The system implements automated memory consolidation to prevent memory bloat and improve efficiency.</p>\n<h3>Consolidation Process</h3>\n<p>The consolidation process uses an LLM to merge, deduplicate, and generalize memories:</p>\n<pre><code class=\"language-python\">@app.post(\"/consolidate_memories/\", response_model=StatusResponse, tags=[\"Memories\"])\nasync def consolidate_memories_api(request: ConsolidateRequest):\n    memories_data = chroma_collection.get(\n        limit=request.max_memories_to_process,\n        include=[\"metadatas\"]\n    )\n    \n    prompt = PROMPT_FOR_CONSOLIDATION + \"\\n\" + json.dumps(memories_to_process, indent=2)\n    llm_response_str = await asyncio.to_thread(call_llm, prompt)\n    consolidation_plan = parse_llm_json_response(llm_response_str)\n    \n    # Save new consolidated memories\n    if consolidation_plan[\"consolidated\"]:\n        save_memories(consolidation_plan[\"consolidated\"], memory_type='long-term-consolidated')\n    \n    # Delete old memories\n    if consolidation_plan[\"to_delete\"]:\n        chroma_collection.delete(ids=unique_to_delete_ids)\n</code></pre>\n<p>The consolidation prompt provides specific instructions for merging related memories, removing duplicates, and generalizing specific facts.</p>\n<h3>Trigger Mechanism</h3>\n<p>Consolidation is triggered programmatically by the system:</p>\n<pre><code class=\"language-python\">async def consolidate_memories(self):\n    return await consolidate_memories_api(ConsolidateRequest())\n</code></pre>\n<p>In the core system, consolidation is called at strategic points in the execution flow:</p>\n<pre><code class=\"language-python\">consolidation_result = await self.memory_service.consolidate_memories()\n</code></pre>\n<h3>Retention Policies</h3>\n<p>The system implements retention through:</p>\n<ul>\n<li><strong>Usage-based prioritization</strong>: Frequently accessed memories are retained</li>\n<li><strong>Redundancy elimination</strong>: Duplicate or overlapping memories are removed</li>\n<li><strong>Temporal relevance</strong>: Older, less accessed memories are prioritized for consolidation</li>\n</ul>\n<p>The system currently fetches a random batch of memories for consolidation due to ChromaDB's lack of metadata-based sorting, but this could be enhanced with custom indexing.</p>\n<h2>Performance Considerations</h2>\n<p>The memory system incorporates several performance optimizations and scalability considerations.</p>\n<h3>Vector Search Optimization</h3>\n<p>``mermaid\nflowchart TD\nA[Query Text] --> B[Generate Embedding]\nB --> C[Vector Search in PostgreSQL]\nC --> D[Filter by Similarity]\nD --> E[Update Access Metadata]\nE --> F[Return Results]</p>\n<pre><code>\n**Diagram sources**\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n\n**Section sources**\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n\n#### Indexing Strategies\n- PostgreSQL with pgvector automatically indexes embeddings for fast similarity search\n- GIN indexes are used for tag-based filtering\n- The system could benefit from implementing HNSW or other approximate nearest neighbor algorithms for larger datasets\n\n#### Performance Metrics\n- Query latency: Optimized through in-memory vector indexing\n- Memory footprint: Controlled through periodic consolidation\n- Throughput: Async operations prevent blocking the main event loop\n\n### Semantic Search with FAISS\n\nFor semantic memory, the system uses FAISS for efficient vector search:\n\n```python\n# Initialize FAISS index for semantic search\nself.faiss_index = faiss.IndexFlatL2(self.embedding_dim)\n</code></pre>\n<p>The FAISS index is persisted to disk and automatically loaded on startup:</p>\n<pre><code class=\"language-python\">if os.path.exists(self.index_file) and os.path.exists(self.id_map_file):\n    self.faiss_index = faiss.read_index(self.index_file)\n    with open(self.id_map_file, \"rb\") as f:\n        self.id_map = pickle.load(f)\n</code></pre>\n<h3>Memory Bloat Prevention</h3>\n<p>The system prevents memory bloat through:</p>\n<ul>\n<li><strong>Consolidation</strong>: Regular merging of related memories</li>\n<li><strong>Deduplication</strong>: Removal of redundant information</li>\n<li><strong>Access tracking</strong>: Usage-based retention prioritization</li>\n<li><strong>Batch processing</strong>: Limiting the number of memories processed at once</li>\n</ul>\n<h2>Debugging Memory Issues</h2>\n<p>The system provides several mechanisms for debugging memory-related issues.</p>\n<h3>Health Monitoring</h3>\n<p>The memory service includes a health check endpoint:</p>\n<pre><code class=\"language-python\">async def health_check(self) -> Dict[str, Any]:\n    \"\"\"Perform comprehensive health check.\"\"\"\n    try:\n        # Check database connection\n        db_stats = await self.postgres_store.get_memory_statistics()\n        db_connected = bool(db_stats)\n        \n        # Check embedding service\n        test_embedding = await self.embedding_service.generate_text_embedding(\"test\")\n        embedding_ready = len(test_embedding) > 0\n        \n        uptime = (datetime.utcnow() - self.start_time).total_seconds()\n        \n        return {\n            \"status\": \"healthy\" if db_connected and embedding_ready else \"degraded\",\n            \"database_connected\": db_connected,\n            \"embedding_service_ready\": embedding_ready,\n            \"memory_count\": db_stats.get(\"total_memories\", 0),\n            \"uptime_seconds\": int(uptime),\n            \"initialized\": self.initialized\n        }\n        \n    except Exception as e:\n        logger.error(f\"Health check failed: {e}\")\n        return {\n            \"status\": \"unhealthy\",\n            \"error\": str(e),\n            \"initialized\": self.initialized\n        }\n</code></pre>\n<p>This endpoint verifies database connectivity and reports the current memory count.</p>\n<h3>Diagnostic Endpoints</h3>\n<p>Additional diagnostic capabilities include:</p>\n<ul>\n<li><strong>list_memories_api</strong>: Retrieves all stored memories for inspection</li>\n<li><strong>Logging</strong>: Comprehensive logging of memory operations</li>\n<li><strong>Status responses</strong>: Detailed operation results with metadata</li>\n</ul>\n<h3>Common Issues and Solutions</h3>\n<h4>Retrieval Inaccuracies</h4>\n<ul>\n<li><strong>Cause</strong>: Low similarity threshold or poor embedding quality</li>\n<li><strong>Solution</strong>: Adjust similarity_threshold parameter or retrain embeddings</li>\n</ul>\n<h4>Memory Leaks</h4>\n<ul>\n<li><strong>Cause</strong>: Failed consolidation or improper memory deletion</li>\n<li><strong>Solution</strong>: Verify consolidation process and check deletion logs</li>\n</ul>\n<h4>Performance Degradation</h4>\n<ul>\n<li><strong>Cause</strong>: Large memory database without proper indexing</li>\n<li><strong>Solution</strong>: Implement approximate nearest neighbor search or database partitioning</li>\n</ul>\n<h2>Very Long-Term Memory System</h2>\n<p>The Very Long-Term Memory (VLTM) system provides strategic knowledge management and cross-system memory integration. It uses SQLModel with junction tables to properly implement many-to-many relationships between memory patterns, consolidations, and strategic knowledge.</p>\n<h3>Data Model Relationships</h3>\n<p>The VLTM data models use junction tables to correctly implement many-to-many relationships:</p>\n<p>``mermaid\nclassDiagram\nclass VeryLongTermMemory {\n+memory_id: str\n+memory_type: MemoryType\n+created_at: datetime\n+last_accessed: datetime\n+access_count: int\n+importance_score: float\n+compressed_content: str\n+metadata_info: str\n}\nclass MemoryPattern {\n+pattern_id: str\n+pattern_type: PatternType\n+pattern_description: str\n+confidence_score: float\n+pattern_data: str\n+discovered_at: datetime\n}\nclass MemoryConsolidation {\n+consolidation_id: str\n+consolidation_date: datetime\n+consolidation_type: ConsolidationType\n+memories_processed: int\n+patterns_extracted: int\n+compression_ratio: float\n+success: bool\n}\nclass StrategicKnowledge {\n+knowledge_id: str\n+knoledge_domain: str\n+knoledge_summary: str\n+confidence_level: float\n+last_updated: datetime\n}\nclass ConsolidationPattern {\n+consolidation_id: str\n+pattern_id: str\n+extraction_confidence: float\n}\nclass PatternStrategicKnowledge {\n+pattern_id: str\n+knoledge_id: str\n+contribution_weight: float\n}</p>\n<p>VeryLongTermMemory \"1\" -- \"0..<em>\" MemoryPattern : contains\nMemoryPattern \"0..</em>\" -- \"0..<em>\" MemoryConsolidation : extracted in\nMemoryPattern \"0..</em>\" -- \"0..<em>\" StrategicKnowledge : contributes to\nMemoryConsolidation \"1\" -- \"0..</em>\" ConsolidationPattern : has\nConsolidationPattern \"0..<em>\" -- \"0..</em>\" MemoryPattern : links\nStrategicKnowledge \"1\" -- \"0..<em>\" PatternStrategicKnowledge : has\nPatternStrategicKnowledge \"0..</em>\" -- \"0..*\" MemoryPattern : links</p>\n<pre><code>\n**Diagram sources**\n- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*\n\n**Section sources**\n- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*\n\n#### Junction Table Implementation\nThe system uses junction tables to properly implement many-to-many relationships:\n\n```python\nclass ConsolidationPattern(SQLModel, table=True):\n    \"\"\"Junction table linking memory consolidations and patterns\"\"\"\n    __tablename__ = \"consolidation_patterns\"\n    \n    consolidation_id: str = Field(foreign_key=\"memory_consolidations.consolidation_id\", primary_key=True)\n    pattern_id: str = Field(foreign_key=\"memory_patterns.pattern_id\", primary_key=True)\n    extraction_confidence: float = Field(default=1.0)\n\n\nclass PatternStrategicKnowledge(SQLModel, table=True):\n    \"\"\"Junction table linking memory patterns and strategic knowledge\"\"\"\n    __tablename__ = \"pattern_strategic_knowledge\"\n    \n    pattern_id: str = Field(foreign_key=\"memory_patterns.pattern_id\", primary_key=True)\n    knowledge_id: str = Field(foreign_key=\"strategic_knowledge.knowledge_id\", primary_key=True)\n    contribution_weight: float = Field(default=1.0)\n</code></pre>\n<p>The relationships are properly defined using the <code>link_model</code> parameter:</p>\n<pre><code class=\"language-python\">class MemoryPattern(SQLModel, table=True):\n    # Fixed the relationship to use the junction table\n    consolidations: List[\"MemoryConsolidation\"] = Relationship(\n        back_populates=\"extracted_patterns\",\n        link_model=ConsolidationPattern  # Using the junction table\n    )\n    strategic_knowledge: List[\"StrategicKnowledge\"] = Relationship(\n        back_populates=\"patterns\",\n        link_model=PatternStrategicKnowledge  # Using the junction table\n    )\n</code></pre>\n<h3>Memory Integration Manager</h3>\n<p>The MemoryIntegrationManager coordinates memory flow between existing memory systems and the VLTM system.</p>\n<h4>Integration Architecture</h4>\n<p>``mermaid\nclassDiagram\nclass MemoryIntegrationManager {\n+initialize()\n+shutdown()\n+trigger_manual_sync()\n+get_integration_status()\n}\nclass MemoryBridge {\n+source_system: str\n+target_system: str\n+flow_direction: MemoryFlowDirection\n+memory_types: List[MemoryType]\n+sync_interval_minutes: int\n+batch_size: int\n+enabled: bool\n}\nclass IntegrationStats {\n+memories_synchronized: int\n+patterns_extracted: int\n+knoledge_consolidated: int\n+failed_operations: int\n+processing_time_seconds: float\n+last_sync_timestamp: datetime\n}\nclass MemoryFlowDirection {\n+TO_VLTM: \"to_vltm\"\n+FROM_VLTM: \"from_vltm\"\n+BIDIRECTIONAL: \"bidirectional\"\n}\nclass IntegrationMode {\n+REAL_TIME: \"real_time\"\n+BATCH: \"batch\"\n+HYBRID: \"hybrid\"\n+SELECTIVE: \"selective\"\n}</p>\n<p>MemoryIntegrationManager --> MemoryBridge : \"manages\"\nMemoryIntegrationManager --> IntegrationStats : \"tracks\"\nMemoryIntegrationManager --> MemoryFlowDirection : \"uses\"\nMemoryIntegrationManager --> IntegrationMode : \"uses\"</p>\n<pre><code>\n**Diagram sources**\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*\n\n**Section sources**\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*\n\n#### Memory Bridge Configuration\nThe system uses configurable memory bridges to control memory flow:\n\n```python\n@dataclass\nclass MemoryBridge:\n    \"\"\"Configuration for memory system bridge\"\"\"\n    source_system: str\n    target_system: str\n    flow_direction: MemoryFlowDirection\n    memory_types: List[MemoryType]\n    sync_interval_minutes: int = 60\n    batch_size: int = 100\n    enabled: bool = True\n</code></pre>\n<p>Default bridges are set up during initialization:</p>\n<pre><code class=\"language-python\">async def _setup_default_bridges(self):\n    \"\"\"Setup default memory bridges between systems\"\"\"\n    \n    # Bridge: Episodic Memory → VLTM\n    episodic_to_vltm = MemoryBridge(\n        source_system=\"episodic_memory\",\n        target_system=\"vltm\",\n        flow_direction=MemoryFlowDirection.TO_VLTM,\n        memory_types=[\n            MemoryType.SUCCESSFUL_IMPROVEMENT,\n            MemoryType.FAILED_EXPERIMENT,\n            MemoryType.CRITICAL_FAILURE,\n            MemoryType.ARCHITECTURAL_INSIGHT\n        ],\n        sync_interval_minutes=30,\n        batch_size=50\n    )\n    \n    # Bridge: Knowledge System → VLTM\n    knowledge_to_vltm = MemoryBridge(\n        source_system=\"knowledge_service\",\n        target_system=\"vltm\",\n        flow_direction=MemoryFlowDirection.TO_VLTM,\n        memory_types=[\n            MemoryType.STRATEGIC_KNOWLEDGE,\n            MemoryType.META_LEARNING_RULE,\n            MemoryType.CODE_PATTERN\n        ],\n        sync_interval_minutes=60,\n        batch_size=25\n    )\n    \n    # Bridge: VLTM → Knowledge System (strategic insights)\n    vltm_to_knowledge = MemoryBridge(\n        source_system=\"vltm\",\n        target_system=\"knowledge_service\",\n        flow_direction=MemoryFlowDirection.FROM_VLTM,\n        memory_types=[MemoryType.STRATEGIC_KNOWLEDGE],\n        sync_interval_minutes=120,\n        batch_size=10\n    )\n    \n    self.memory_bridges = [episodic_to_vltm, knowledge_to_vltm, vltm_to_knowledge]\n</code></pre>\n<h4>Memory Synchronization</h4>\n<p>The integration manager continuously synchronizes memories across bridges:</p>\n<pre><code class=\"language-python\">async def _sync_bridge_continuously(self, bridge: MemoryBridge):\n    \"\"\"Continuously sync a memory bridge\"\"\"\n    \n    while self.is_running:\n        try:\n            await self._sync_memory_bridge(bridge)\n            \n            # Wait for the next sync interval\n            await asyncio.sleep(bridge.sync_interval_minutes * 60)\n            \n        except asyncio.CancelledError:\n            logger.info(f\"Sync task cancelled for bridge: {bridge.source_system} → {bridge.target_system}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in bridge sync: {e}\")\n            # Wait before retrying\n            await asyncio.sleep(60)\n</code></pre>\n<h4>Memory Classification and Conversion</h4>\n<p>The system classifies episodic memories into VLTM memory types:</p>\n<pre><code class=\"language-python\">def _classify_episodic_memory(self, memory_data: Dict[str, Any]) -> MemoryType:\n    \"\"\"Classify episodic memory into VLTM memory type\"\"\"\n    \n    content = memory_data.get(\"content\", \"\").lower()\n    tags = memory_data.get(\"tags\", [])\n    \n    # Classification logic\n    if any(word in content for word in [\"optimized\", \"improved\", \"enhanced\"]):\n        return MemoryType.SUCCESSFUL_IMPROVEMENT\n    elif any(word in content for word in [\"error\", \"failed\", \"crash\"]):\n        if any(word in content for word in [\"critical\", \"severe\"]):\n            return MemoryType.CRITICAL_FAILURE\n        else:\n            return MemoryType.FAILED_EXPERIMENT\n    elif any(word in content for word in [\"architecture\", \"design\", \"pattern\"]):\n        return MemoryType.ARCHITECTURAL_INSIGHT\n    elif \"optimization\" in tags:\n        return MemoryType.SUCCESSFUL_IMPROVEMENT\n    else:\n        return MemoryType.CODE_PATTERN\n</code></pre>\n<p>Memories are converted between systems with appropriate metadata:</p>\n<pre><code class=\"language-python\">async def _convert_episodic_to_vltm(self, memory_data: Dict[str, Any], memory_type: MemoryType) -> Optional[Dict[str, Any]]:\n    \"\"\"Convert episodic memory to VLTM format\"\"\"\n    \n    try:\n        content = {\n            \"original_content\": memory_data.get(\"content\"),\n            \"source_system\": \"episodic_memory\",\n            \"integration_info\": {\n                \"synced_at\": datetime.utcnow().isoformat(),\n                \"original_id\": memory_data.get(\"id\"),\n                \"confidence\": memory_data.get(\"confidence\", 0.5)\n            }\n        }\n        \n        metadata = {\n            \"episodic_sync\": True,\n            \"original_timestamp\": memory_data.get(\"timestamp\").isoformat() if memory_data.get(\"timestamp\") else None,\n            \"tags\": memory_data.get(\"tags\", [])\n        }\n        \n        return {\n            \"content\": content,\n            \"memory_type\": memory_type,\n            \"metadata\": metadata\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error converting episodic memory: {e}\")\n        return None\n</code></pre>\n<h2>Conclusion</h2>\n<p>The RAVANA memory system implements a sophisticated dual-architecture approach combining episodic and semantic memory systems. The episodic memory captures specific experiences using PostgreSQL with pgvector and SentenceTransformers for vector-based storage and retrieval, while the semantic memory system uses LLM-driven knowledge compression to create structured summaries. The system has been enhanced with multi-modal capabilities, supporting text, audio, and image content with cross-modal search and unified embedding generation. The MultiModalMemoryService provides a comprehensive interface for memory operations, and the system incorporates automated consolidation to prevent memory bloat. Performance is optimized through vector indexing and async operations, with comprehensive logging and diagnostic capabilities for debugging. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that properly implements many-to-many relationships using junction tables and provides strategic knowledge management through the MemoryIntegrationManager. This architecture enables the system to maintain long-term context, learn from experiences, and provide increasingly personalized responses over time.</p>\n<p><strong>Referenced Files in This Document</strong></p>\n<ul>\n<li><a>memory.py</a> - <em>Updated in recent commit</em></li>\n<li><a>client.py</a> - <em>Updated in recent commit</em></li>\n<li><a>embedding_service.py</a> - <em>Added in recent commit</em></li>\n<li><a>search_engine.py</a> - <em>Added in recent commit</em></li>\n<li><a>multi_modal_service.py</a> - <em>Added in recent commit</em></li>\n<li><a>models.py</a> - <em>Added in recent commit</em></li>\n<li><a>postgresql_store.py</a> - <em>Added in recent commit</em></li>\n<li><a>memory_service.py</a></li>\n<li><a>compressed_memory.py</a></li>\n<li><a>main.py</a></li>\n<li><a>compression_prompts.py</a></li>\n<li><a>test_memory.py</a></li>\n<li><a>knowledge_service.py</a></li>\n<li><a>vltm_data_models.py</a> - <em>Updated in recent commit</em></li>\n<li><a>vltm_memory_integration_manager.py</a> - <em>Updated in recent commit</em></li>\n<li><a>llm.py</a> - <em>Updated in recent commit</em></li>\n</ul>\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture & Design","title":"Architecture & Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment & Operations","title":"Deployment & Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true}