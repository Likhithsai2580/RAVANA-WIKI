{"pageProps":{"doc":{"slug":"Deployment & Operations","title":"Deployment & Operations","content":"<h1>Deployment &#x26; Operations</h1>\n<h2>Update Summary</h2>\n<p><strong>Changes Made</strong></p>\n<ul>\n<li>Added documentation for singleton pattern implementation in ConversationalAI class</li>\n<li>Updated Bot Connectivity Verification section with instance tracking details</li>\n<li>Added new section for User Platform Preference Tracking</li>\n<li>Enhanced section sources to reflect new files and functionality</li>\n<li>Added references to DiscordBot and TelegramBot instance tracking mechanisms</li>\n<li>Updated startup command documentation to reflect singleton behavior</li>\n</ul>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#runtime-requirements\">Runtime Requirements</a></li>\n<li><a href=\"#dependency-installation\">Dependency Installation</a></li>\n<li><a href=\"#configuration-setup\">Configuration Setup</a></li>\n<li><a href=\"#startup-commands\">Startup Commands</a></li>\n<li><a href=\"#monitoring--health-checks\">Monitoring &#x26; Health Checks</a></li>\n<li><a href=\"#logging-configuration\">Logging Configuration</a></li>\n<li><a href=\"#backup--recovery-procedures\">Backup &#x26; Recovery Procedures</a></li>\n<li><a href=\"#performance-tuning\">Performance Tuning</a></li>\n<li><a href=\"#scaling-considerations\">Scaling Considerations</a></li>\n<li><a href=\"#failure-recovery\">Failure Recovery</a></li>\n<li><a href=\"#security-hardening\">Security Hardening</a></li>\n<li><a href=\"#operational-runbooks\">Operational Runbooks</a></li>\n<li><a href=\"#bot-connectivity-verification\">Bot Connectivity Verification</a></li>\n<li><a href=\"#user-platform-preference-tracking\">User Platform Preference Tracking</a></li>\n</ol>\n<h2>Runtime Requirements</h2>\n<p>The RAVANA system is a Python-based artificial general intelligence (AGI) framework designed for autonomous reasoning, memory consolidation, and multi-modal processing. It requires a modern computing environment with sufficient resources to support concurrent AI inference, database operations, and background services.</p>\n<h3>CPU &#x26; Memory Requirements</h3>\n<ul>\n<li><strong>Minimum</strong>: 4-core CPU, 8GB RAM</li>\n<li><strong>Recommended</strong>: 8-core CPU, 16GB+ RAM</li>\n<li><strong>High-performance</strong>: 16-core CPU, 32GB+ RAM (for large-scale knowledge compression and physics simulations)</li>\n</ul>\n<p>The system runs multiple concurrent processes including:</p>\n<ul>\n<li>Main AGI loop</li>\n<li>Memory server (separate process)</li>\n<li>Background data fetching and event detection</li>\n<li>Multi-modal processing (image/audio)</li>\n</ul>\n<h3>GPU Requirements</h3>\n<ul>\n<li><strong>Optional but recommended</strong>: NVIDIA GPU with CUDA support (RTX 3060 or higher)</li>\n<li>Required for:\n<ul>\n<li>Local LLM inference (if not using cloud APIs)</li>\n<li>FAISS semantic search acceleration</li>\n<li>Image/audio processing</li>\n</ul>\n</li>\n<li>Minimum VRAM: 8GB</li>\n<li>Supported frameworks: CUDA, cuDNN</li>\n</ul>\n<h3>Storage Requirements</h3>\n<ul>\n<li><strong>System files</strong>: ~500MB</li>\n<li><strong>Database</strong>: Variable, scales with usage (initial: 100MB)</li>\n<li><strong>FAISS index</strong>: ~100MBâ€“1GB depending on knowledge base size</li>\n<li><strong>Temporary files</strong>: Up to 2GB in <code>/tmp/agi_multimodal</code></li>\n<li><strong>Recommended</strong>: SSD storage for optimal database and index performance</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/config.py</a></li>\n<li><a>services/knowledge_service.py</a></li>\n<li><a>services/memory_service.py</a></li>\n</ul>\n<h2>Dependency Installation</h2>\n<h3>Python Environment Setup</h3>\n<pre><code class=\"language-bash\"># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# venv\\Scripts\\activate  # Windows\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>\n<h3>Core Dependencies (via pyproject.toml)</h3>\n<pre><code class=\"language-bash\"># Install all dependencies\npip install -e .\n\n# Or install manually from requirements\npip install \\\n  sqlmodel \\\n  \"sentence-transformers\" \\\n  faiss-cpu \\\n  psutil \\\n  requests \\\n  python-dotenv \\\n  numpy \\\n  \"openai\" \\\n  \"google-generativeai\" \\\n  \"pydantic>=2.0\"\n</code></pre>\n<h3>Optional GPU-Accelerated Dependencies</h3>\n<pre><code class=\"language-bash\"># For GPU support (CUDA 11.8)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip install faiss-gpu\n\n# Verify GPU availability\npython -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>\n<h3>External Service Dependencies</h3>\n<ul>\n<li><strong>Google Gemini API</strong>: Required for multi-modal processing\n<ul>\n<li>Set <code>GEMINI_API_KEY</code> in environment</li>\n</ul>\n</li>\n<li><strong>OpenAI API</strong>: Optional fallback for LLM calls\n<ul>\n<li>Set <code>OPENAI_API_KEY</code> in environment</li>\n</ul>\n</li>\n<li><strong>PostgreSQL (optional)</strong>: For production database\n<ul>\n<li>Default: SQLite (file-based)</li>\n</ul>\n</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>pyproject.toml</a></li>\n<li><a>services/multi_modal_service.py</a></li>\n<li><a>core/llm.py</a></li>\n</ul>\n<h2>Configuration Setup</h2>\n<h3>Environment Variables</h3>\n<p>Create <code>.env</code> file in project root:</p>\n<pre><code class=\"language-env\"># API Keys\nGEMINI_API_KEY=your_gemini_key_here\nOPENAI_API_KEY=your_openai_key_here\n\n# Database\nDATABASE_URL=sqlite:///./ravana.db\n# For PostgreSQL: DATABASE_URL=postgresql://user:pass@localhost/ravana\n\n# Memory Service\nMEMORY_SERVER_HOST=localhost\nMEMORY_SERVER_PORT=8001\nMEMORY_SERVICE_SHUTDOWN_TIMEOUT=30\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=logs/ravana.log\n\n# Data Feeds\nFEED_URLS=https://example.com/feed1.xml,https://example.com/feed2.json\n</code></pre>\n<h3>Database Initialization</h3>\n<pre><code class=\"language-python\">from database.engine import create_db_and_tables\n\ncreate_db_and_tables()\n</code></pre>\n<p>This creates all required tables based on <code>database/models.py</code>.</p>\n<h3>Configuration File (config.json)</h3>\n<pre><code class=\"language-json\">{\n  \"debug_mode\": false,\n  \"auto_save_interval\": 300,\n  \"max_memory_entries\": 10000,\n  \"consolidation_threshold\": 100,\n  \"embedding_model\": \"all-MiniLM-L6-v2\",\n  \"llm_provider\": \"gemini\"\n}\n</code></pre>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>core/config.py</a></li>\n<li><a>database/models.py</a></li>\n<li><a>database/engine.py</a></li>\n</ul>\n<h2>Startup Commands</h2>\n<h3>Autonomous Mode (Main AGI Loop)</h3>\n<pre><code class=\"language-bash\">python main.py\n</code></pre>\n<p>This starts the full AGI system with:</p>\n<ul>\n<li>Continuous data ingestion</li>\n<li>Decision engine</li>\n<li>Memory management</li>\n<li>Autonomous action execution</li>\n</ul>\n<h3>Physics Testing Mode</h3>\n<pre><code class=\"language-bash\"># Run all physics experiments\npython run_physics_tests.py\n\n# Interactive physics CLI\npython physics_cli.py\n</code></pre>\n<p>These modes are used for validating physical reasoning capabilities and running controlled experiments.</p>\n<h3>Single Task Mode</h3>\n<pre><code class=\"language-bash\"># Execute specific action via CLI\npython main.py --action coding --params '{\"task\": \"implement quicksort\"}'\n\n# Process multi-modal directory\npython main.py --mode multimodal --dir ./input_files\n</code></pre>\n<h3>Service-Specific Startup</h3>\n<pre><code class=\"language-bash\"># Start only memory service\npython -m modules.episodic_memory.main\n\n# Start data fetching service\npython -c \"from services.data_service import DataService; from database.engine import engine; ds = DataService(engine, ['https://rss.example.com']); ds.fetch_and_save_articles()\"\n</code></pre>\n<h3>Conversational AI Module Startup</h3>\n<pre><code class=\"language-bash\"># Start conversational AI module in standalone mode\npython launch_conversational_ai.py\n\n# Verify bot connectivity before starting\npython launch_conversational_ai.py --verify-bots\n</code></pre>\n<p>The <code>--verify-bots</code> flag checks the connectivity of configured bot platforms (Discord and Telegram) and exits with appropriate status code. The ConversationalAI class implements a singleton pattern, ensuring only one instance can be created and initialized in the system.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>main.py</a></li>\n<li><a>run_physics_tests.py</a></li>\n<li><a>physics_cli.py</a></li>\n<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>\n<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>\n</ul>\n<h2>Monitoring &#x26; Health Checks</h2>\n<h3>Service Health Endpoints</h3>\n<table>\n<thead>\n<tr>\n<th>Service</th>\n<th>Health Endpoint</th>\n<th>Process Detection</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Memory Server</td>\n<td><code>http://localhost:8001/health</code></td>\n<td>Checks for <code>memory.py</code> process</td>\n</tr>\n<tr>\n<td>Main System</td>\n<td>None (internal)</td>\n<td>PID file or process name</td>\n</tr>\n</tbody>\n</table>\n<h3>Health Check Implementation</h3>\n<pre><code class=\"language-python\"># Example health check for memory service\nasync def check_memory_service_health():\n    import requests\n    try:\n        response = requests.get(\"http://localhost:8001/health\", timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n</code></pre>\n<h3>Monitoring Metrics</h3>\n<p>The system exposes the following operational metrics:</p>\n<ul>\n<li>\n<p><strong>MemoryService.get_memory_statistics()</strong></p>\n<ul>\n<li><code>status</code>: operational/error</li>\n<li><code>total_memories</code>: count of stored memories</li>\n<li><code>last_consolidation</code>: timestamp</li>\n<li><code>memory_server_status</code>: running/not running</li>\n</ul>\n</li>\n<li>\n<p><strong>Database health</strong></p>\n<ul>\n<li>Connection pool status</li>\n<li>Table row counts</li>\n<li>Index integrity</li>\n</ul>\n</li>\n</ul>\n<h3>Prometheus Integration (Recommended)</h3>\n<p>Add metrics exporter to expose for Prometheus:</p>\n<pre><code class=\"language-python\">from prometheus_client import start_http_server, Counter, Gauge\n\n# Example metric\nMEMORY_COUNT = Gauge('ravana_memory_count', 'Number of stored memories')\n</code></pre>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/memory_service.py</a></li>\n<li><a>services/data_service.py</a></li>\n</ul>\n<h2>Logging Configuration</h2>\n<h3>Log Levels</h3>\n<ul>\n<li><strong>DEBUG</strong>: Detailed debugging information</li>\n<li><strong>INFO</strong>: Normal operation events</li>\n<li><strong>WARNING</strong>: Potential issues</li>\n<li><strong>ERROR</strong>: Recoverable errors</li>\n<li><strong>CRITICAL</strong>: System failures</li>\n</ul>\n<h3>Log Output</h3>\n<p>Logs are written to:</p>\n<ul>\n<li><strong>Console</strong>: Real-time monitoring</li>\n<li><strong>File</strong>: <code>logs/ravana.log</code> (rotated daily)</li>\n<li><strong>Structured format</strong>: JSON when <code>LOG_JSON=true</code></li>\n</ul>\n<h3>Log Categories</h3>\n<table>\n<thead>\n<tr>\n<th>Logger</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ravana</code></td>\n<td>Main system events</td>\n</tr>\n<tr>\n<td><code>MemoryService</code></td>\n<td>Memory operations</td>\n</tr>\n<tr>\n<td><code>DataService</code></td>\n<td>Data fetching and storage</td>\n</tr>\n<tr>\n<td><code>KnowledgeService</code></td>\n<td>Knowledge compression and retrieval</td>\n</tr>\n<tr>\n<td><code>MultiModalService</code></td>\n<td>Image/audio processing</td>\n</tr>\n</tbody>\n</table>\n<h3>Example Log Entry</h3>\n<pre><code class=\"language-json\">{\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"logger\": \"MemoryService\",\n  \"event\": \"Memory server detected, attempting graceful shutdown...\",\n  \"context\": {\"pid\": 1234}\n}\n</code></pre>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/memory_service.py</a></li>\n<li><a>services/knowledge_service.py</a></li>\n</ul>\n<h2>Backup &#x26; Recovery Procedures</h2>\n<h3>Database Backup</h3>\n<pre><code class=\"language-bash\"># SQLite backup\ncp ravana.db ravana.db.backup.$(date +%Y%m%d_%H%M%S)\n\n# With compression\ntar -czf ravana_db_$(date +%Y%m%d).tar.gz ravana.db\n</code></pre>\n<p>For production PostgreSQL:</p>\n<pre><code class=\"language-bash\">pg_dump -U ravana_user -h localhost ravana_db > ravana_backup.sql\n</code></pre>\n<h3>Knowledge Index Backup</h3>\n<pre><code class=\"language-bash\"># FAISS index and ID map\ncp knowledge_index.faiss knowledge_index.faiss.backup\ncp knowledge_id_map.pkl knowledge_id_map.pkl.backup\n\n# Compressed backup\ntar -czf knowledge_backup_$(date +%Y%m%d).tar.gz knowledge_index.faiss knowledge_id_map.pkl\n</code></pre>\n<h3>Automated Backup Script</h3>\n<pre><code class=\"language-bash\">#!/bin/bash\nBACKUP_DIR=\"/backups/ravana/$(date +%Y%m%d)\"\nmkdir -p $BACKUP_DIR\n\ncp ravana.db $BACKUP_DIR/\ncp knowledge_index.faiss $BACKUP_DIR/\ncp knowledge_id_map.pkl $BACKUP_DIR/\n\n# Keep last 7 days\nfind /backups/ravana -type d -name \"202*\" | sort | head -n -7 | xargs rm -rf\n</code></pre>\n<h3>Recovery Procedure</h3>\n<ol>\n<li>Stop RAVANA services</li>\n<li>Replace database and index files</li>\n<li>Restart services</li>\n<li>Verify integrity via health checks</li>\n</ol>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>database/models.py</a></li>\n<li><a>services/knowledge_service.py</a></li>\n<li><a>services/memory_service.py</a></li>\n</ul>\n<h2>Performance Tuning</h2>\n<h3>Database Optimization</h3>\n<ul>\n<li><strong>Indexing</strong>: Ensure proper indexes on timestamp fields</li>\n<li><strong>Connection pooling</strong>: Use SQLAlchemy pool settings for PostgreSQL</li>\n<li><strong>Vacuuming</strong>: Regular <code>VACUUM</code> for SQLite databases</li>\n</ul>\n<h3>Knowledge Service Tuning</h3>\n<ul>\n<li><strong>FAISS Index Type</strong>: Use <code>IndexIVFFlat</code> for large datasets (>100K entries)</li>\n<li><strong>Embedding Model</strong>: Switch to larger models (<code>all-mpnet-base-v2</code>) for better accuracy</li>\n<li><strong>Batch Processing</strong>: Process summaries in batches during compression</li>\n</ul>\n<h3>Memory Management</h3>\n<ul>\n<li><strong>Consolidation Frequency</strong>: Adjust based on activity level</li>\n<li><strong>Memory Pruning</strong>: Implement TTL-based cleanup for old memories</li>\n<li><strong>Vector Store</strong>: Consider switching to dedicated vector database (Pinecone, Weaviate) at scale</li>\n</ul>\n<h3>Caching Strategy</h3>\n<ul>\n<li><strong>Query Results</strong>: Cache frequent knowledge queries</li>\n<li><strong>LLM Responses</strong>: Cache deterministic LLM calls</li>\n<li><strong>Embeddings</strong>: Cache frequently used text embeddings</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/knowledge_service.py</a></li>\n<li><a>database/models.py</a></li>\n</ul>\n<h2>Scaling Considerations</h2>\n<h3>Horizontal Scaling</h3>\n<p>The system can be scaled by separating services:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Scalability</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Main AGI Loop</td>\n<td>Single instance (stateful)</td>\n</tr>\n<tr>\n<td>Memory Service</td>\n<td>Can run as separate microservice</td>\n</tr>\n<tr>\n<td>Data Service</td>\n<td>Can be distributed</td>\n</tr>\n<tr>\n<td>Knowledge Service</td>\n<td>Can be containerized</td>\n</tr>\n<tr>\n<td>Multi-Modal Service</td>\n<td>Horizontally scalable</td>\n</tr>\n</tbody>\n</table>\n<h3>Containerization (Docker)</h3>\n<pre><code class=\"language-dockerfile\">FROM python:3.11-slim\n\nWORKDIR /app\nCOPY . .\nRUN pip install -e .\n\nCMD [\"python\", \"main.py\"]\n</code></pre>\n<h3>Kubernetes Deployment (Recommended for Production)</h3>\n<p>Deploy components as separate pods with:</p>\n<ul>\n<li>Resource limits/requests</li>\n<li>Liveness and readiness probes</li>\n<li>Persistent volumes for database and indexes</li>\n<li>Secret management for API keys</li>\n</ul>\n<h3>Load Distribution</h3>\n<ul>\n<li><strong>Data ingestion</strong>: Distribute feed processing</li>\n<li><strong>Multi-modal processing</strong>: Queue-based worker pool</li>\n<li><strong>Knowledge compression</strong>: Scheduled batch jobs</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/data_service.py</a></li>\n<li><a>services/multi_modal_service.py</a></li>\n</ul>\n<h2>Failure Recovery</h2>\n<h3>Process Crash Recovery</h3>\n<ul>\n<li><strong>MemoryService</strong>: Automatically detects and shuts down orphaned memory server processes</li>\n<li><strong>Main System</strong>: Use process manager (systemd, PM2) for auto-restart</li>\n<li><strong>Data Service</strong>: Idempotent operations prevent duplication</li>\n</ul>\n<h3>Data Corruption Handling</h3>\n<ul>\n<li><strong>Database</strong>: Use transactions and regular integrity checks</li>\n<li><strong>FAISS Index</strong>: Recreate from database if corrupted</li>\n<li><strong>Backups</strong>: Restore from last known good state</li>\n</ul>\n<h3>Graceful Shutdown</h3>\n<pre><code class=\"language-python\"># Uses Shutdownable interface\nawait memory_service.prepare_shutdown()\nawait memory_service.shutdown(timeout=30)\n</code></pre>\n<p>Ensures:</p>\n<ul>\n<li>Memory server shutdown</li>\n<li>Database transaction completion</li>\n<li>Log flushing</li>\n<li>State persistence</li>\n</ul>\n<h3>Disaster Recovery Plan</h3>\n<ol>\n<li><strong>Immediate</strong>: Switch to backup instance</li>\n<li><strong>Short-term</strong>: Restore from latest backup</li>\n<li><strong>Long-term</strong>: Rebuild knowledge index from logs</li>\n</ol>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/memory_service.py</a></li>\n<li><a>core/shutdown_coordinator.py</a></li>\n</ul>\n<h2>Security Hardening</h2>\n<h3>API Key Protection</h3>\n<ul>\n<li>Store keys in environment variables or secret manager</li>\n<li>Never commit to version control</li>\n<li>Use short-lived keys when possible</li>\n</ul>\n<h3>Input Validation</h3>\n<ul>\n<li>Validate all external inputs</li>\n<li>Sanitize file paths in multi-modal service</li>\n<li>Limit upload sizes</li>\n</ul>\n<h3>File System Security</h3>\n<ul>\n<li>Restrict permissions on configuration files</li>\n<li>Secure temporary directory (<code>/tmp/agi_multimodal</code>)</li>\n<li>Regular cleanup of temp files via <code>cleanup_temp_files()</code></li>\n</ul>\n<h3>Network Security</h3>\n<ul>\n<li>Bind services to localhost when possible</li>\n<li>Use firewall rules to restrict access</li>\n<li>Enable TLS for external endpoints</li>\n</ul>\n<h3>Audit Logging</h3>\n<ul>\n<li>Log all security-relevant events</li>\n<li>Monitor for suspicious activity</li>\n<li>Regular log reviews</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/multi_modal_service.py</a></li>\n<li><a>core/llm.py</a></li>\n</ul>\n<h2>Operational Runbooks</h2>\n<h3>Normal Restart Procedure</h3>\n<ol>\n<li>Send SIGTERM to main process</li>\n<li>Wait for graceful shutdown (30s timeout)</li>\n<li>Verify all services terminated</li>\n<li>Start process again</li>\n</ol>\n<pre><code class=\"language-bash\">pkill -f main.py\nsleep 5\npython main.py\n</code></pre>\n<h3>Emergency Stop</h3>\n<pre><code class=\"language-bash\">pkill -9 -f memory.py\npkill -9 -f main.py\n</code></pre>\n<p>Use only if process is unresponsive.</p>\n<h3>Update Procedure</h3>\n<ol>\n<li>Pull latest code</li>\n<li>Backup database and indexes</li>\n<li>Install new dependencies</li>\n<li>Test in staging environment</li>\n<li>Deploy with rolling restart</li>\n</ol>\n<h3>Incident Response</h3>\n<table>\n<thead>\n<tr>\n<th>Incident</th>\n<th>Response</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>High CPU usage</td>\n<td>Check for runaway processes, restart memory service</td>\n</tr>\n<tr>\n<td>Database corruption</td>\n<td>Restore from backup, rebuild indexes</td>\n</tr>\n<tr>\n<td>API key exhaustion</td>\n<td>Rotate keys, check for leaks</td>\n</tr>\n<tr>\n<td>Memory leak</td>\n<td>Restart service, analyze logs</td>\n</tr>\n<tr>\n<td>Failed health check</td>\n<td>Investigate logs, restore from backup if needed</td>\n</tr>\n</tbody>\n</table>\n<h3>Routine Maintenance</h3>\n<ul>\n<li><strong>Daily</strong>: Check logs, verify backups</li>\n<li><strong>Weekly</strong>: Database optimization, cleanup temp files</li>\n<li><strong>Monthly</strong>: Security audit, dependency updates</li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>services/memory_service.py</a></li>\n<li><a>services/data_service.py</a></li>\n<li><a>main.py</a></li>\n</ul>\n<h2>Bot Connectivity Verification</h2>\n<h3>Purpose</h3>\n<p>The bot connectivity verification feature allows administrators to test the connectivity of configured bot platforms (Discord and Telegram) before starting the full conversational AI system. This helps identify configuration issues such as invalid tokens or network connectivity problems.</p>\n<h3>Verification Command</h3>\n<pre><code class=\"language-bash\">python launch_conversational_ai.py --verify-bots\n</code></pre>\n<p>This command:</p>\n<ul>\n<li>Loads the conversational AI configuration</li>\n<li>Tests connectivity for each enabled bot platform</li>\n<li>Returns exit code 0 if all enabled bots are connected successfully</li>\n<li>Returns exit code 1 if any enabled bot fails to connect</li>\n<li>Outputs detailed results to stdout</li>\n</ul>\n<h3>Expected Output</h3>\n<pre><code>=== Bot Verification Results ===\nDiscord: CONNECTED\nTelegram: FAILED\n  Message: Telegram bot did not connect in time\n</code></pre>\n<h3>Configuration Requirements</h3>\n<p>Bot connectivity verification relies on the configuration in <code>modules/conversational_ai/config.json</code>:</p>\n<pre><code class=\"language-json\">{\n  \"discord_token\": \"your_discord_bot_token\",\n  \"telegram_token\": \"your_telegram_bot_token\",\n  \"platforms\": {\n    \"discord\": {\n      \"enabled\": true,\n      \"command_prefix\": \"!\"\n    },\n    \"telegram\": {\n      \"enabled\": true,\n      \"command_prefix\": \"/\"\n    }\n  }\n}\n</code></pre>\n<h3>Integration with Deployment Workflows</h3>\n<p>The verification command can be integrated into deployment pipelines:</p>\n<pre><code class=\"language-bash\"># In CI/CD pipeline\npython launch_conversational_ai.py --verify-bots\nif [ $? -ne 0 ]; then\n    echo \"Bot verification failed, aborting deployment\"\n    exit 1\nfi\n# Proceed with deployment\n</code></pre>\n<h3>Troubleshooting</h3>\n<p>Common issues and solutions:</p>\n<ul>\n<li><strong>Token not found</strong>: Verify that tokens are correctly specified in <code>config.json</code></li>\n<li><strong>Network connectivity</strong>: Ensure the server has outbound internet access</li>\n<li><strong>Firewall restrictions</strong>: Check if firewall rules block connections to Discord/Telegram APIs</li>\n<li><strong>Invalid tokens</strong>: Regenerate bot tokens from the respective developer portals</li>\n</ul>\n<h3>Instance Tracking Implementation</h3>\n<p>The DiscordBot and TelegramBot classes implement class-level tracking to prevent multiple instances from running concurrently. Each bot class has <code>_instance_started</code> and <code>_active_instance</code> class variables that track the global state of bot instances. When a bot is started, it checks these class variables to ensure no other instance is already running. This prevents conflicts and resource contention in production environments.</p>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>\n<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>\n<li><a>modules/conversational_ai/config.json</a> - <em>Configuration for conversational AI module</em></li>\n<li><a>modules/conversational_ai/bots/discord_bot.py</a> - <em>Added instance tracking in recent commit</em></li>\n<li><a>modules/conversational_ai/bots/telegram_bot.py</a> - <em>Added instance tracking in recent commit</em></li>\n</ul>\n<h2>User Platform Preference Tracking</h2>\n<h3>Purpose</h3>\n<p>The system tracks user platform preferences to enable intelligent message routing and personalized interactions. When a user interacts with the conversational AI through a specific platform (Discord or Telegram), this preference is recorded and used for future communications.</p>\n<h3>Implementation Details</h3>\n<p>The user platform preference tracking is implemented through the following components:</p>\n<ol>\n<li>\n<p><strong>UserPlatformProfile</strong>: A dataclass that stores user platform information including:</p>\n<ul>\n<li><code>user_id</code>: Unique identifier of the user</li>\n<li><code>last_platform</code>: The last platform used (discord/telegram)</li>\n<li><code>platform_user_id</code>: Platform-specific user identifier</li>\n<li><code>preferences</code>: User preferences dictionary</li>\n<li><code>last_interaction</code>: Timestamp of last interaction</li>\n</ul>\n</li>\n<li>\n<p><strong>UserProfileManager</strong>: Manages the storage and retrieval of user profiles, persisting them across sessions.</p>\n</li>\n<li>\n<p><strong>_track_user_platform method</strong>: Automatically called whenever a user sends a message, updating their platform preference.</p>\n</li>\n</ol>\n<h3>Automatic Tracking</h3>\n<p>The system automatically tracks platform usage through the <code>process_user_message</code> method in the ConversationalAI class:</p>\n<pre><code class=\"language-python\">def _track_user_platform(self, user_id: str, platform: str):\n    \"\"\"Track the user's platform preference.\"\"\"\n    try:\n        profile = UserPlatformProfile(\n            user_id=user_id,\n            last_platform=platform,\n            platform_user_id=user_id,\n            preferences={},\n            last_interaction=datetime.now()\n        )\n        self.user_profile_manager.set_user_platform_profile(user_id, profile)\n    except Exception as e:\n        logger.error(f\"Error tracking user platform for user {user_id}: {e}\")\n</code></pre>\n<h3>Intelligent Message Routing</h3>\n<p>When sending messages to users, the system uses their stored platform preference:</p>\n<pre><code class=\"language-python\">async def send_message_to_user(self, user_id: str, message: str, platform: str = None):\n    \"\"\"Send a message to a user through the appropriate platform.\"\"\"\n    try:\n        if not platform:\n            # Try to get the user's last used platform from their profile\n            profile = self.user_profile_manager.get_user_platform_profile(user_id)\n            if profile:\n                platform = profile.last_platform\n        # Send message through the determined platform\n        # ...\n    except Exception as e:\n        logger.error(f\"Error sending message to user {user_id}: {e}\")\n</code></pre>\n<h3>Configuration</h3>\n<p>No additional configuration is required for user platform preference tracking. The feature is enabled by default and works automatically with the conversational AI module.</p>\n<h3>Use Cases</h3>\n<ol>\n<li><strong>Follow-up messages</strong>: The system can send follow-up messages through the user's preferred platform</li>\n<li><strong>Task notifications</strong>: When a task is completed, notifications are sent through the user's last-used platform</li>\n<li><strong>Cross-platform continuity</strong>: Users can switch between platforms while maintaining their conversation context</li>\n</ol>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>modules/conversational_ai/main.py</a> - <em>User platform tracking implementation</em></li>\n<li><a>modules/conversational_ai/communication/data_models.py</a> - <em>UserPlatformProfile data model</em></li>\n<li><a>modules/conversational_ai/main.py</a> - <em>Intelligent message routing</em></li>\n</ul>\n<p><strong>Referenced Files in This Document</strong></p>\n<ul>\n<li><a>main.py</a> - <em>Updated in recent commit</em></li>\n<li><a>launch_conversational_ai.py</a> - <em>Updated in recent commit</em></li>\n<li><a>core/config.py</a></li>\n<li><a>core/system.py</a></li>\n<li><a>database/models.py</a></li>\n<li><a>services/data_service.py</a></li>\n<li><a>services/knowledge_service.py</a></li>\n<li><a>services/memory_service.py</a></li>\n<li><a>services/multi_modal_service.py</a></li>\n<li><a>physics_cli.py</a></li>\n<li><a>run_physics_tests.py</a></li>\n<li><a>pyproject.toml</a></li>\n<li><a>modules/conversational_ai/main.py</a> - <em>Modified in recent commit</em></li>\n<li><a>modules/conversational_ai/config.json</a> - <em>Configuration for conversational AI module</em></li>\n<li><a>modules/conversational_ai/bots/discord_bot.py</a> - <em>Added instance tracking in recent commit</em></li>\n<li><a>modules/conversational_ai/bots/telegram_bot.py</a> - <em>Added instance tracking in recent commit</em></li>\n<li><a>modules/conversational_ai/communication/data_models.py</a> - <em>User platform tracking models</em></li>\n</ul>\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture & Design","title":"Architecture & Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment & Operations","title":"Deployment & Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true}