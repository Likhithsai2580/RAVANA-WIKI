<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta charSet="utf-8"/><title>Memory Systems<!-- --> - RAVANA AGI Documentation</title><meta name="description" content="Documentation for Memory Systems"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/aa7d986e9c238cc1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/aa7d986e9c238cc1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.0/dist/mermaid.min.js" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-eb143115b8bf2786.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a41459f5c0b49356.js" defer=""></script><script src="/_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="/_next/static/chunks/pages/docs/%5Bslug%5D-37d587d3c8e56222.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_buildManifest.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen flex flex-col"><div class="min-h-screen flex flex-col"><header class="bg-wiki-blue text-white p-4 shadow-md"><div class="container mx-auto flex justify-between items-center"><h1 class="text-2xl font-bold">RAVANA AGI Documentation</h1><nav><ul class="flex space-x-4"><li><a class="hover:underline" href="/">Home</a></li></ul></nav></div></header><div class="flex-grow container mx-auto p-4 flex flex-col md:flex-row gap-6"><div class="w-full md:w-64 flex-shrink-0"><nav class="w-full md:w-64 flex-shrink-0"><div class="bg-white rounded-lg shadow p-4 sticky top-4"><h3 class="font-bold text-lg mb-3">Documentation</h3><ul class="space-y-1"><li class="mb-3"><div class="font-semibold text-gray-700">A</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Action%20System">Action System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/API%20Reference">API Reference</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Architecture%20&amp;%20Design">Architecture &amp; Design</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">C</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Configuration">Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Conversational%20AI%20Communication%20Framework">Conversational AI Communication Framework</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Core%20System">Core System</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">D</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Database%20Schema">Database Schema</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Decision-Making%20System">Decision-Making System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Deployment%20&amp;%20Operations">Deployment &amp; Operations</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Development%20Guide">Development Guide</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">E</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Emotional%20Intelligence">Emotional Intelligence</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent">Enhanced Snake Agent</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent%20Architecture">Enhanced Snake Agent Architecture</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">G</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Graceful%20Shutdown">Graceful Shutdown</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">L</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/LLM%20Integration">LLM Integration</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">M</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 bg-wiki-blue text-white" href="/docs/Memory%20Systems">Memory Systems</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Multi-Modal%20Memory">Multi-Modal Memory</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">P</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Project%20Overview">Project Overview</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">S</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Self-Improvement">Self-Improvement</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Services">Services</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Snake%20Agent%20Configuration">Snake Agent Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules-57f9b30b-b165-48d3-8e89-196940d26190">Specialized Modules</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules">Specialized Modules</a></li></ul></li></ul></div></nav></div><main class="flex-grow"><nav class="mb-4 text-sm"><ol class="list-none p-0 inline-flex"><li class="flex items-center"><a class="text-wiki-blue hover:underline" href="/">Home</a><svg class="fill-current w-3 h-3 mx-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path></svg></li><li class="flex items-center"><span class="text-gray-500">Memory Systems</span></li></ol></nav><div class="flex flex-col md:flex-row gap-6"><article class="prose max-w-none bg-white p-6 rounded-lg shadow flex-grow"><h1>Memory Systems</h1><div><h1>Memory Systems</h1>
<h2>Update Summary</h2>
<p><strong>Changes Made</strong></p>
<ul>
<li>Updated documentation to reflect corrected many-to-many relationship declarations in VLTM data models</li>
<li>Added new section on Very Long-Term Memory (VLTM) system and its integration with existing memory systems</li>
<li>Enhanced architectural diagrams to include VLTM components and integration patterns</li>
<li>Added detailed information about memory bridges, flow direction, and synchronization mechanisms</li>
<li>Updated code examples to reflect the use of junction tables for many-to-many relationships</li>
<li>Added information about memory classification, conversion, and importance threshold evaluation</li>
<li>Integrated LLM reliability improvements from core/llm.py into memory system documentation</li>
<li>Added details about enhanced LLM error handling, JSON parsing, and response validation in memory operations</li>
</ul>
<h2>Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#memory-architecture-overview">Memory Architecture Overview</a></li>
<li><a href="#episodic-memory-system">Episodic Memory System</a></li>
<li><a href="#multi-modal-embedding-service">Multi-Modal Embedding Service</a></li>
<li><a href="#advanced-search-engine">Advanced Search Engine</a></li>
<li><a href="#multi-modal-memory-orchestration">Multi-Modal Memory Orchestration</a></li>
<li><a href="#semantic-memory-and-knowledge-compression">Semantic Memory and Knowledge Compression</a></li>
<li><a href="#memoryservice-interface-and-crud-operations">MemoryService Interface and CRUD Operations</a></li>
<li><a href="#memory-retrieval-patterns">Memory Retrieval Patterns</a></li>
<li><a href="#consolidation-triggers-and-retention-policies">Consolidation Triggers and Retention Policies</a></li>
<li><a href="#performance-considerations">Performance Considerations</a></li>
<li><a href="#debugging-memory-issues">Debugging Memory Issues</a></li>
<li><a href="#very-long-term-memory-system">Very Long-Term Memory System</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h2>Introduction</h2>
<p>The RAVANA system implements a dual-memory architecture combining episodic and semantic memory systems to enable long-term learning and contextual awareness. This document details the design, implementation, and operational characteristics of these memory systems, focusing on their storage mechanisms, retrieval patterns, and integration points. The system leverages PostgreSQL with pgvector for similarity-based retrieval and employs LLM-driven knowledge compression to transform raw experiences into structured semantic summaries. Recent enhancements have introduced multi-modal memory processing with support for text, audio, and image content, enabling cross-modal search and unified embedding generation. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that integrates with existing memory systems through configurable memory bridges, enabling strategic knowledge consolidation and cross-system synchronization. The memory system has been enhanced with improved LLM reliability features including detailed logging, enhanced error handling, and robust JSON parsing to ensure consistent memory operations.</p>
<h2>Memory Architecture Overview</h2>
<p>``mermaid
graph TB
subgraph "Memory Systems"
EM[Episodic Memory]
SM[Semantic Memory]
VLTM[Very Long-Term Memory]
end
subgraph "Processing"
EX[Memory Extraction]
CO[Consolidation]
KC[Knowledge Compression]
ES[Embedding Service]
SE[Search Engine]
MI[Memory Integration]
end
subgraph "Storage"
PG[PostgreSQL with pgvector]
SQ[SQLModel Database]
FS[File System]
end
subgraph "Access"
MS[MemoryService]
MMS[MultiModalService]
KS[KnowledgeService]
VIM[VLTM Integration Manager]
end
UserInput --> EX
EX --> EM
EM --> CO
CO --> PG
KC --> SM
SM --> SQ
MS --> EM
MMS --> EM
KS --> SM
EM --> |Vector Search| SE
SM --> |Semantic Search| KS
PG --> EM
SQ --> SM
FS --> SM
ES --> PG
SE --> PG
MMS --> ES
MMS --> SE
EM --> MI
SM --> MI
MI --> VLTM
VLTM --> KS
VIM --> MI
style EM fill:#f9f,stroke:#333
style SM fill:#bbf,stroke:#333
style VLTM fill:#9f9,stroke:#333</p>
<pre><code>
**Diagram sources**
- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)
- [knowledge_service.py](file://services/knowledge_service.py#L0-L255)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)
- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)

**Section sources**
- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)
- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)

## Episodic Memory System

The episodic memory system captures and stores specific events and interactions as discrete memory records. Each memory is stored with rich metadata and indexed using vector embeddings for similarity-based retrieval. Recent updates have replaced ChromaDB with PostgreSQL enhanced with pgvector extension, enabling robust multi-modal storage and advanced querying capabilities.

### Storage Mechanism with PostgreSQL and SentenceTransformers

Episodic memories are stored in PostgreSQL with pgvector extension, providing a production-grade database solution for vector similarity search. The system uses SentenceTransformers to generate embeddings for memory texts, enabling semantic search capabilities.

``mermaid
classDiagram
class MemoryRecord {
+uuid id
+ContentType content_type
+string content_text
+string file_path
+List[float] text_embedding
+List[float] image_embedding
+List[float] audio_embedding
+List[float] unified_embedding
+datetime created_at
+datetime last_accessed
+int access_count
+MemoryType memory_type
+float emotional_valence
+List[str] tags
}
class PostgreSQLStore {
+save_memory_record(record)
+get_memory_record(id)
+vector_search(embedding)
+get_memory_statistics()
}
class MultiModalMemoryService {
+process_text_memory(text)
+process_audio_memory(path)
+process_image_memory(path)
+search_memories(request)
}
PostgreSQLStore --> MemoryRecord : "stores"
MultiModalMemoryService --> PostgreSQLStore : "uses"
MultiModalMemoryService --> MemoryRecord : "manages"
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>models.py</a></li>
<li><a>postgresql_store.py</a></li>
<li><a>multi_modal_service.py</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>memory.py</a></li>
<li><a>postgresql_store.py</a></li>
<li><a>models.py</a></li>
</ul>
<h4>Embedding Generation and Storage</h4>
<p>The system uses the <code>all-MiniLM-L6-v2</code> SentenceTransformer model to generate 384-dimensional text embeddings. For multi-modal content, specialized embedding generation is implemented:</p>
<pre><code class="language-python">class EmbeddingService:
    def __init__(self, text_model_name: str = "all-MiniLM-L6-v2"):
        self.text_model_name = text_model_name
        self.text_embedding_dim = 384
        self.image_embedding_dim = 512
        self.audio_embedding_dim = 512
        self.unified_embedding_dim = 1024
</code></pre>
<p>Memories are stored with comprehensive metadata including creation timestamp, access statistics, content type, and confidence scores:</p>
<pre><code class="language-python">memory_record = MemoryRecord(
    content_type=content_type,
    content_text=content_text,
    file_path=file_path,
    memory_type=memory_type,
    tags=tags or [],
    emotional_valence=emotional_valence,
    confidence_score=confidence_score,
    created_at=datetime.utcnow()
)
</code></pre>
<h4>Memory Extraction Process</h4>
<p>The system extracts memories from conversations using an LLM-powered extraction process. The <code>extract_memories_from_conversation</code> method analyzes user-AI interactions and identifies key information to store:</p>
<pre><code class="language-python">async def extract_memories_from_conversation(self, request: ConversationRequest) -> MemoriesList:
    prompt = f"""
    You are a memory extraction module for an AGI. Your task is to analyze a conversation 
    and identify key pieces of information to be stored in the AGI's long-term memory.

    Focus on extracting:
    - Key facts and information
    - User preferences and characteristics
    - Important goals, plans, or intentions
    - Notable events or experiences
    - Emotional context if relevant

    Guidelines:
    - Each memory should be a single, self-contained statement
    - Keep memories concise (under 30 words)
    - Prefer information that is likely to be relevant long-term
    - Do not store transitory conversational details
    - Output as a JSON object with a "memories" array

    Conversation:
    User: {request.user_input}
    AI: {request.ai_output}
    """
</code></pre>
<p>The extraction focuses on key facts, user preferences, major goals, and core beliefs while filtering out transitory conversational details.</p>
<h2>Multi-Modal Embedding Service</h2>
<p>The EmbeddingService provides multi-modal embedding generation for text, audio, and image content, enabling cross-modal retrieval and unified embedding creation.</p>
<h3>Multi-Modal Embedding Generation</h3>
<p>The embedding service supports multiple content types with specialized processing:</p>
<p>``mermaid
flowchart TD
A[Input Content] --> B{Content Type}
B --> C[Text]
B --> D[Audio]
B --> E[Image]
C --> F[Generate Text Embeddingusing SentenceTransformer]
D --> G[Process Audio with WhisperExtract Features]
E --> H[Extract Image FeaturesColor, Dimensions, Histogram]
F --> I[Store text_embedding]
G --> J[Store audio_embedding]
H --> K[Store image_embedding]
I --> L[Generate Unified Embedding]
J --> L
K --> L
L --> M[Store unified_embedding]</p>
<pre><code>
**Diagram sources**
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)
- [models.py](file://modules/episodic_memory/models.py#L0-L250)

**Section sources**
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)

#### Text Embedding Implementation
Text embeddings are generated using SentenceTransformers with caching for performance:

```python
async def generate_text_embedding(self, text: str) -> List[float]:
    # Check cache first
    cached = self.cache.get(text, self.text_model_name)
    if cached is not None:
        return cached
    
    self._load_text_model()
    
    try:
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            None,
            lambda: self.text_model.encode(text, convert_to_tensor=False, normalize_embeddings=True)
        )
        
        embedding_list = embedding.tolist()
        self.cache.put(text, self.text_model_name, embedding_list)
        
        return embedding_list
        
    except Exception as e:
        logger.error(f"Text embedding generation failed: {e}")
        return [0.0] * self.text_embedding_dim
</code></pre>
<h4>Audio Embedding Implementation</h4>
<p>Audio embeddings are generated from Whisper transcription and audio features:</p>
<pre><code class="language-python">async def generate_audio_embedding(self, audio_features: Dict[str, Any]) -> List[float]:
    features = []
    
    # Extract numerical features from audio analysis
    if "mfcc" in audio_features:
        mfcc_data = audio_features["mfcc"]
        if "mean" in mfcc_data:
            features.extend(mfcc_data["mean"])
        if "std" in mfcc_data:
            features.extend(mfcc_data["std"])
    
    if "spectral_centroid" in audio_features:
        sc = audio_features["spectral_centroid"]
        features.extend([sc.get("mean", 0.0), sc.get("std", 0.0)])
    
    # Pad or truncate to desired dimension
    if len(features) &#x3C; self.audio_embedding_dim:
        features.extend([0.0] * (self.audio_embedding_dim - len(features)))
    else:
        features = features[:self.audio_embedding_dim]
    
    return features
</code></pre>
<h4>Image Embedding Implementation</h4>
<p>Image embeddings are generated from visual features (placeholder for CLIP in production):</p>
<pre><code class="language-python">async def generate_image_embedding(self, image_path: str) -> List[float]:
    try:
        image = Image.open(image_path).convert('RGB')
        
        # Extract basic statistics
        img_array = np.array(image)
        features = []
        
        # Color statistics
        for channel in range(3):  # RGB
            channel_data = img_array[:, :, channel].flatten()
            features.extend([
                float(np.mean(channel_data)),
                float(np.std(channel_data)),
                float(np.median(channel_data)),
                float(np.percentile(channel_data, 25)),
                float(np.percentile(channel_data, 75))
            ])
        
        # Image dimensions
        features.extend([
            float(image.width),
            float(image.height),
            float(image.width * image.height)  # Area
        ])
        
        # Histogram features
        hist, _ = np.histogram(img_array.flatten(), bins=32, range=(0, 256))
        hist_normalized = hist / np.sum(hist)
        features.extend(hist_normalized.tolist())
        
        # Pad or truncate to desired dimension
        if len(features) &#x3C; self.image_embedding_dim:
            features.extend([0.0] * (self.image_embedding_dim - len(features)))
        else:
            features = features[:self.image_embedding_dim]
        
        return features
        
    except Exception as e:
        logger.error(f"Image embedding generation failed for {image_path}: {e}")
        return [0.0] * self.image_embedding_dim
</code></pre>
<h4>Unified Embedding Generation</h4>
<p>The system generates unified embeddings by combining modalities with weighted fusion:</p>
<pre><code class="language-python">async def generate_unified_embedding(self, memory_record: MemoryRecord) -> List[float]:
    unified = []
    
    # Combine embeddings with weights
    text_weight = 0.4
    image_weight = 0.3
    audio_weight = 0.3
    
    # Text embedding (weighted)
    if memory_record.text_embedding:
        text_emb = np.array(memory_record.text_embedding) * text_weight
        unified.extend(text_emb.tolist())
    else:
        unified.extend([0.0] * int(self.unified_embedding_dim * text_weight))
    
    # Image embedding (weighted)
    if memory_record.image_embedding:
        image_emb = np.array(memory_record.image_embedding) * image_weight
        unified.extend(image_emb[:int(self.unified_embedding_dim * image_weight)].tolist())
    else:
        unified.extend([0.0] * int(self.unified_embedding_dim * image_weight))
    
    # Audio embedding (weighted)
    if memory_record.audio_embedding:
        audio_emb = np.array(memory_record.audio_embedding) * audio_weight
        unified.extend(audio_emb[:int(self.unified_embedding_dim * audio_weight)].tolist())
    else:
        unified.extend([0.0] * int(self.unified_embedding_dim * audio_weight))
    
    # Normalize the unified embedding
    unified_array = np.array(unified)
    norm = np.linalg.norm(unified_array)
    if norm > 0:
        unified_array = unified_array / norm
    
    return unified_array.tolist()
</code></pre>
<h2>Advanced Search Engine</h2>
<p>The AdvancedSearchEngine provides sophisticated search capabilities including cross-modal search, similarity search, and hybrid search modes.</p>
<h3>Cross-Modal Search Implementation</h3>
<p>The search engine supports cross-modal queries where different content types can be used to search across modalities:</p>
<pre><code class="language-python">async def cross_modal_search(self, request: CrossModalSearchRequest) -> List[SearchResult]:
    # Generate query embedding based on type
    if request.query_type == ContentType.TEXT:
        query_embedding = await self.embeddings.generate_text_embedding(request.query_content)
    elif request.query_type == ContentType.AUDIO and self.whisper:
        audio_result = await self.whisper.process_audio(request.query_content)
        query_embedding = await self.embeddings.generate_text_embedding(
            audio_result.get("transcript", "")
        )
    elif request.query_type == ContentType.IMAGE:
        query_embedding = await self.embeddings.generate_image_embedding(request.query_content)
    else:
        raise ValueError(f"Unsupported query type: {request.query_type}")
    
    # Search using unified embeddings
    results = await self.postgres.vector_search(
        embedding=query_embedding,
        embedding_type="unified",
        limit=request.limit,
        similarity_threshold=request.similarity_threshold,
        content_types=request.target_types
    )
    
    # Convert to SearchResult objects
    search_results = []
    for i, (memory_record, similarity) in enumerate(results):
        search_results.append(SearchResult(
            memory_record=memory_record,
            similarity_score=similarity,
            rank=i + 1,
            search_metadata={
                "search_type": "cross_modal_specialized",
                "query_type": request.query_type.value,
                "target_types": [ct.value for ct in request.target_types]
            }
        ))
    
    return search_results
</code></pre>
<h3>Similarity Search Implementation</h3>
<p>Find memories similar to a given memory record:</p>
<pre><code class="language-python">async def find_similar_memories(self, 
                              memory_record: MemoryRecord, 
                              limit: int = 10,
                              similarity_threshold: float = 0.7) -> List[SearchResult]:
    # Use the best available embedding
    if memory_record.unified_embedding:
        embedding = memory_record.unified_embedding
        embedding_type = "unified"
    elif memory_record.text_embedding:
        embedding = memory_record.text_embedding
        embedding_type = "text"
    elif memory_record.image_embedding:
        embedding = memory_record.image_embedding
        embedding_type = "image"
    elif memory_record.audio_embedding:
        embedding = memory_record.audio_embedding
        embedding_type = "audio"
    else:
        logger.warning("No embeddings available for similarity search")
        return []
    
    # Search for similar memories
    results = await self.postgres.vector_search(
        embedding=embedding,
        embedding_type=embedding_type,
        limit=limit + 1,  # +1 to exclude the original
        similarity_threshold=similarity_threshold
    )
    
    # Convert to SearchResult objects and exclude the original
    search_results = []
    for i, (similar_record, similarity) in enumerate(results):
        if similar_record.id != memory_record.id:  # Exclude the original
            search_results.append(SearchResult(
                memory_record=similar_record,
                similarity_score=similarity,
                rank=len(search_results) + 1,
                search_metadata={
                    "search_type": "similarity",
                    "reference_id": str(memory_record.id),
                    "embedding_type": embedding_type
                }
            ))
    
    return search_results[:limit]
</code></pre>
<h3>Hybrid Search Configuration</h3>
<p>Configure weights for hybrid search combining vector and text search:</p>
<pre><code class="language-python">def configure_search_weights(self, vector_weight: float, text_weight: float):
    """
    Configure the weights for hybrid search.
    
    Args:
        vector_weight: Weight for vector similarity (0-1)
        text_weight: Weight for text search (0-1)
    """
    total_weight = vector_weight + text_weight
    if total_weight > 0:
        self.vector_weight = vector_weight / total_weight
        self.text_weight = text_weight / total_weight
        logger.info(f"Updated search weights: vector={self.vector_weight:.2f}, text={self.text_weight:.2f}")
    else:
        logger.warning("Invalid weights provided, keeping current configuration")
</code></pre>
<h2>Multi-Modal Memory Orchestration</h2>
<p>The MultiModalMemoryService orchestrates all components of the memory system, providing a unified interface for multi-modal operations.</p>
<h3>Service Architecture</h3>
<p>``mermaid
classDiagram
class MultiModalMemoryService {
+initialize()
+close()
+process_text_memory(text)
+process_audio_memory(path)
+process_image_memory(path)
+extract_memories_from_conversation(request)
+search_memories(request)
+find_similar_memories(id)
+batch_process_files(request)
+health_check()
}
class PostgreSQLStore {
+save_memory_record()
+get_memory_record()
+vector_search()
}
class EmbeddingService {
+generate_text_embedding()
+generate_audio_embedding()
+generate_image_embedding()
+generate_unified_embedding()
}
class WhisperAudioProcessor {
+process_audio()
}
class AdvancedSearchEngine {
+search()
+cross_modal_search()
+find_similar_memories()
}
MultiModalMemoryService --> PostgreSQLStore : "uses"
MultiModalMemoryService --> EmbeddingService : "uses"
MultiModalMemoryService --> WhisperAudioProcessor : "uses"
MultiModalMemoryService --> AdvancedSearchEngine : "uses"</p>
<pre><code>
**Diagram sources**
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)
- [search_engine.py](file://modules/episodic_memory/search_engine.py#L0-L508)

**Section sources**
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)

#### Text Memory Processing
Process and store text-based memories:

```python
async def process_text_memory(self, 
                            text: str,
                            memory_type: MemoryType = MemoryType.EPISODIC,
                            tags: Optional[List[str]] = None,
                            emotional_valence: Optional[float] = None) -> MemoryRecord:
    # Create memory record
    memory_record = MemoryRecord(
        content_type=ContentType.TEXT,
        content_text=text,
        memory_type=memory_type,
        tags=tags or [],
        emotional_valence=emotional_valence,
        created_at=datetime.utcnow()
    )
    
    # Generate embeddings
    memory_record = await self.embedding_service.generate_embeddings(memory_record)
    
    # Save to database
    saved_record = await self.postgres_store.save_memory_record(memory_record)
    
    logger.info(f"Processed text memory: {saved_record.id}")
    return saved_record
</code></pre>
<h4>Audio Memory Processing</h4>
<p>Process and store audio memories with Whisper transcription:</p>
<pre><code class="language-python">async def process_audio_memory(self, 
                             audio_path: str,
                             context: Optional[str] = None,
                             memory_type: MemoryType = MemoryType.EPISODIC,
                             tags: Optional[List[str]] = None) -> MemoryRecord:
    # Process audio with Whisper
    audio_result = await self.whisper_processor.process_audio(audio_path, context)
    
    # Create audio metadata
    audio_metadata = self.whisper_processor.create_audio_metadata(audio_result)
    
    # Create memory record
    memory_record = MemoryRecord(
        content_type=ContentType.AUDIO,
        content_text=audio_result.get("transcript"),
        file_path=audio_path,
        memory_type=memory_type,
        tags=tags or [],
        confidence_score=audio_result.get("confidence", 0.8),
        audio_metadata=audio_metadata,
        created_at=datetime.utcnow()
    )
    
    # Generate embeddings
    memory_record = await self.embedding_service.generate_embeddings(memory_record)
    
    # Save to database
    saved_record = await self.postgres_store.save_memory_record(memory_record)
    
    logger.info(f"Processed audio memory: {saved_record.id}")
    return saved_record
</code></pre>
<h4>Image Memory Processing</h4>
<p>Process and store image memories:</p>
<pre><code class="language-python">async def process_image_memory(self, 
                             image_path: str,
                             description: Optional[str] = None,
                             memory_type: MemoryType = MemoryType.EPISODIC,
                             tags: Optional[List[str]] = None) -> MemoryRecord:
    # Create basic image metadata
    from PIL import Image
    with Image.open(image_path) as img:
        width, height = img.size
    
    image_metadata = ImageMetadata(
        width=width,
        height=height,
        scene_description=description
    )
    
    # Create memory record
    memory_record = MemoryRecord(
        content_type=ContentType.IMAGE,
        content_text=description,
        file_path=image_path,
        memory_type=memory_type,
        tags=tags or [],
        image_metadata=image_metadata,
        created_at=datetime.utcnow()
    )
    
    # Generate embeddings
    memory_record = await self.embedding_service.generate_embeddings(memory_record)
    
    # Save to database
    saved_record = await self.postgres_store.save_memory_record(memory_record)
    
    logger.info(f"Processed image memory: {saved_record.id}")
    return saved_record
</code></pre>
<h4>Batch Processing</h4>
<p>Process multiple files in batch with parallel processing support:</p>
<pre><code class="language-python">async def batch_process_files(self, request: BatchProcessRequest) -> BatchProcessResult:
    # Process files
    if request.parallel_processing:
        # Process in parallel with limited concurrency
        semaphore = asyncio.Semaphore(request.max_workers)
        
        async def process_with_semaphore(task):
            async with semaphore:
                return await task
        
        parallel_tasks = [process_with_semaphore(task) for task in tasks]
        results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
    else:
        # Process sequentially
        for task in tasks:
            result = await task
            results.append(result)
    
    # Process results
    processing_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processing_results.append(ProcessingResult(
                memory_record=None,
                processing_time_ms=0,
                success=False,
                error_message=str(result)
            ))
            failed_count += 1
        else:
            processing_results.append(result)
            if result.success:
                successful_count += 1
            else:
                failed_count += 1
    
    return BatchProcessResult(
        results=processing_results,
        total_processed=len(request.file_paths),
        successful_count=successful_count,
        failed_count=failed_count,
        total_time_ms=int(total_time)
    )
</code></pre>
<h2>Semantic Memory and Knowledge Compression</h2>
<p>The semantic memory system transforms episodic experiences into structured knowledge through a compression pipeline that identifies patterns, generalizes information, and creates concise summaries.</p>
<h3>Knowledge Compression Pipeline</h3>
<p>The knowledge compression pipeline converts raw experiences into semantic summaries using LLM-driven analysis:</p>
<p>``mermaid
flowchart TD
A[Raw Experiences] --> B{KnowledgeCompression}
B --> C[Pattern Recognition]
C --> D[Merge Related Facts]
D --> E[Deduplicate Redundant Info]
E --> F[Generalize Specifics]
F --> G[Create Summary]
G --> H[Store in Semantic Memory]</p>
<pre><code>
**Diagram sources**
- [main.py](file://modules/knowledge_compression/main.py#L0-L42)
- [compression_prompts.py](file://modules/knowledge_compression/compression_prompts.py#L0-L5)

**Section sources**
- [main.py](file://modules/knowledge_compression/main.py#L0-L42)
- [compressed_memory.py](file://modules/knowledge_compression/compressed_memory.py#L0-L17)

#### Compression Implementation
The compression process is implemented in the `compress_knowledge` function, which uses an LLM to summarize accumulated logs:

```python
def compress_knowledge(logs):
    prompt = COMPRESSION_PROMPT.format(logs=json.dumps(logs, indent=2))
    summary = call_llm(prompt)
    entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "summary": summary
    }
    save_summary(entry)
    return entry
</code></pre>
<p>The compression prompt instructs the LLM to produce structured summaries of new facts learned, key outcomes, and next goals:</p>
<pre><code class="language-python">COMPRESSION_PROMPT = (
    "You are an AI tasked with summarizing accumulated knowledge and logs. "
    "Given the following logs, produce a concise summary report of new facts learned, key outcomes, and next goals.\n"
    "Logs: {logs}\n"
    "Respond in a clear, structured format."
)
</code></pre>
<h4>Storage Mechanism</h4>
<p>Compressed knowledge is stored as JSON files on the filesystem, with each summary entry containing a timestamp and the LLM-generated summary:</p>
<pre><code class="language-python">def save_summary(entry):
    data = load_summaries()
    data.append(entry)
    with open(COMPRESSED_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)
</code></pre>
<p>The system also integrates with SQLModel for database-backed semantic memory storage, where summaries are stored in a relational database with metadata:</p>
<pre><code class="language-python">class Summary(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    timestamp: str
    summary_text: str
    source: str
    category: str
    content_hash: str = Field(unique=True)
</code></pre>
<h2>MemoryService Interface and CRUD Operations</h2>
<p>The <code>MemoryService</code> provides a unified interface for memory management operations, abstracting the underlying storage mechanisms.</p>
<p>``mermaid
classDiagram
class MemoryService {
+get_relevant_memories(query_text)
+save_memories(memories)
+extract_memories(user_input, ai_output)
+consolidate_memories()
}
class MemoryAPI {
+get_relevant_memories_api(request)
+save_memories_api(request)
+extract_memories_api(request)
+consolidate_memories_api(request)
}
MemoryService --> MemoryAPI : "delegates"</p>
<pre><code>
**Diagram sources**
- [memory_service.py](file://services/memory_service.py#L0-L20)
- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)

**Section sources**
- [memory_service.py](file://services/memory_service.py#L0-L20)

### CRUD Operations

The MemoryService implements the following CRUD operations:

#### Create: save_memories
Stores new memories in the episodic memory system:

```python
async def save_memories(self, memories):
    await asyncio.to_thread(save_memories, memories)
</code></pre>
<p>The operation runs in a separate thread to avoid blocking the event loop.</p>
<h4>Read: get_relevant_memories</h4>
<p>Retrieves memories relevant to a query using vector similarity search:</p>
<pre><code class="language-python">async def get_relevant_memories(self, query_text: str):
    return await get_relevant_memories_api({"query_text": query_text})
</code></pre>
<h4>Update: extract_memories</h4>
<p>Extracts and updates memories from new interactions:</p>
<pre><code class="language-python">async def extract_memories(self, user_input: str, ai_output: str):
    return await extract_memories_api({"user_input": user_input, "ai_output": ai_output})
</code></pre>
<h4>Delete: consolidate_memories</h4>
<p>Removes redundant memories during consolidation:</p>
<pre><code class="language-python">async def consolidate_memories(self):
    from modules.episodic_memory.memory import ConsolidateRequest
    return await consolidate_memories_api(ConsolidateRequest())
</code></pre>
<h2>Memory Retrieval Patterns</h2>
<p>The system implements similarity-based retrieval patterns for efficient memory access.</p>
<h3>Vector Search Implementation</h3>
<p>Memory retrieval uses PostgreSQL's pgvector extension for vector search capabilities:</p>
<pre><code class="language-python">async def vector_search(self, 
                       embedding: List[float],
                       embedding_type: str = "text",
                       limit: int = 10,
                       similarity_threshold: float = 0.7,
                       content_types: Optional[List[ContentType]] = None) -> List[Tuple[MemoryRecord, float]]:
    # Build query based on embedding type
    embedding_column = f"{embedding_type}_embedding"
    
    where_conditions = [f"{embedding_column} IS NOT NULL"]
    params = [embedding]
    param_count = 1
    
    if content_types:
        param_count += 1
        where_conditions.append(f"content_type = ANY(${param_count})")
        params.append([ct.value for ct in content_types])
    
    param_count += 1
    where_conditions.append(f"1 - ({embedding_column} &#x3C;=> ${param_count}) >= ${param_count + 1}")
    params.extend([embedding, similarity_threshold])
    
    query = f"""
        SELECT *, 1 - ({embedding_column} &#x3C;=> $1) as similarity
        FROM memory_records 
        WHERE {' AND '.join(where_conditions)}
        ORDER BY {embedding_column} &#x3C;=> $1
        LIMIT ${param_count + 2}
    """
    params.append(limit)
    
    rows = await conn.fetch(query, *params)
</code></pre>
<h3>Retrieval Parameters</h3>
<p>The retrieval process is configurable through the following parameters:</p>
<ul>
<li><strong>top_n</strong>: Maximum number of memories to return (default: 5)</li>
<li><strong>similarity_threshold</strong>: Minimum similarity score for inclusion (default: 0.7)</li>
</ul>
<p>The similarity threshold acts as a filter to ensure only highly relevant memories are retrieved, preventing information overload.</p>
<h3>Access Pattern Tracking</h3>
<p>The system tracks memory access patterns by updating metadata on retrieval:</p>
<pre><code class="language-python"># Update access metadata for retrieved memories
if ids_to_update:
    chroma_collection.update(ids=ids_to_update, metadatas=metadatas_to_update)
</code></pre>
<p>Each retrieved memory has its <code>last_accessed</code> timestamp and <code>access_count</code> updated, enabling usage-based retention policies.</p>
<h2>Consolidation Triggers and Retention Policies</h2>
<p>The system implements automated memory consolidation to prevent memory bloat and improve efficiency.</p>
<h3>Consolidation Process</h3>
<p>The consolidation process uses an LLM to merge, deduplicate, and generalize memories:</p>
<pre><code class="language-python">@app.post("/consolidate_memories/", response_model=StatusResponse, tags=["Memories"])
async def consolidate_memories_api(request: ConsolidateRequest):
    memories_data = chroma_collection.get(
        limit=request.max_memories_to_process,
        include=["metadatas"]
    )
    
    prompt = PROMPT_FOR_CONSOLIDATION + "\n" + json.dumps(memories_to_process, indent=2)
    llm_response_str = await asyncio.to_thread(call_llm, prompt)
    consolidation_plan = parse_llm_json_response(llm_response_str)
    
    # Save new consolidated memories
    if consolidation_plan["consolidated"]:
        save_memories(consolidation_plan["consolidated"], memory_type='long-term-consolidated')
    
    # Delete old memories
    if consolidation_plan["to_delete"]:
        chroma_collection.delete(ids=unique_to_delete_ids)
</code></pre>
<p>The consolidation prompt provides specific instructions for merging related memories, removing duplicates, and generalizing specific facts.</p>
<h3>Trigger Mechanism</h3>
<p>Consolidation is triggered programmatically by the system:</p>
<pre><code class="language-python">async def consolidate_memories(self):
    return await consolidate_memories_api(ConsolidateRequest())
</code></pre>
<p>In the core system, consolidation is called at strategic points in the execution flow:</p>
<pre><code class="language-python">consolidation_result = await self.memory_service.consolidate_memories()
</code></pre>
<h3>Retention Policies</h3>
<p>The system implements retention through:</p>
<ul>
<li><strong>Usage-based prioritization</strong>: Frequently accessed memories are retained</li>
<li><strong>Redundancy elimination</strong>: Duplicate or overlapping memories are removed</li>
<li><strong>Temporal relevance</strong>: Older, less accessed memories are prioritized for consolidation</li>
</ul>
<p>The system currently fetches a random batch of memories for consolidation due to ChromaDB's lack of metadata-based sorting, but this could be enhanced with custom indexing.</p>
<h2>Performance Considerations</h2>
<p>The memory system incorporates several performance optimizations and scalability considerations.</p>
<h3>Vector Search Optimization</h3>
<p>``mermaid
flowchart TD
A[Query Text] --> B[Generate Embedding]
B --> C[Vector Search in PostgreSQL]
C --> D[Filter by Similarity]
D --> E[Update Access Metadata]
E --> F[Return Results]</p>
<pre><code>
**Diagram sources**
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)

**Section sources**
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)

#### Indexing Strategies
- PostgreSQL with pgvector automatically indexes embeddings for fast similarity search
- GIN indexes are used for tag-based filtering
- The system could benefit from implementing HNSW or other approximate nearest neighbor algorithms for larger datasets

#### Performance Metrics
- Query latency: Optimized through in-memory vector indexing
- Memory footprint: Controlled through periodic consolidation
- Throughput: Async operations prevent blocking the main event loop

### Semantic Search with FAISS

For semantic memory, the system uses FAISS for efficient vector search:

```python
# Initialize FAISS index for semantic search
self.faiss_index = faiss.IndexFlatL2(self.embedding_dim)
</code></pre>
<p>The FAISS index is persisted to disk and automatically loaded on startup:</p>
<pre><code class="language-python">if os.path.exists(self.index_file) and os.path.exists(self.id_map_file):
    self.faiss_index = faiss.read_index(self.index_file)
    with open(self.id_map_file, "rb") as f:
        self.id_map = pickle.load(f)
</code></pre>
<h3>Memory Bloat Prevention</h3>
<p>The system prevents memory bloat through:</p>
<ul>
<li><strong>Consolidation</strong>: Regular merging of related memories</li>
<li><strong>Deduplication</strong>: Removal of redundant information</li>
<li><strong>Access tracking</strong>: Usage-based retention prioritization</li>
<li><strong>Batch processing</strong>: Limiting the number of memories processed at once</li>
</ul>
<h2>Debugging Memory Issues</h2>
<p>The system provides several mechanisms for debugging memory-related issues.</p>
<h3>Health Monitoring</h3>
<p>The memory service includes a health check endpoint:</p>
<pre><code class="language-python">async def health_check(self) -> Dict[str, Any]:
    """Perform comprehensive health check."""
    try:
        # Check database connection
        db_stats = await self.postgres_store.get_memory_statistics()
        db_connected = bool(db_stats)
        
        # Check embedding service
        test_embedding = await self.embedding_service.generate_text_embedding("test")
        embedding_ready = len(test_embedding) > 0
        
        uptime = (datetime.utcnow() - self.start_time).total_seconds()
        
        return {
            "status": "healthy" if db_connected and embedding_ready else "degraded",
            "database_connected": db_connected,
            "embedding_service_ready": embedding_ready,
            "memory_count": db_stats.get("total_memories", 0),
            "uptime_seconds": int(uptime),
            "initialized": self.initialized
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "initialized": self.initialized
        }
</code></pre>
<p>This endpoint verifies database connectivity and reports the current memory count.</p>
<h3>Diagnostic Endpoints</h3>
<p>Additional diagnostic capabilities include:</p>
<ul>
<li><strong>list_memories_api</strong>: Retrieves all stored memories for inspection</li>
<li><strong>Logging</strong>: Comprehensive logging of memory operations</li>
<li><strong>Status responses</strong>: Detailed operation results with metadata</li>
</ul>
<h3>Common Issues and Solutions</h3>
<h4>Retrieval Inaccuracies</h4>
<ul>
<li><strong>Cause</strong>: Low similarity threshold or poor embedding quality</li>
<li><strong>Solution</strong>: Adjust similarity_threshold parameter or retrain embeddings</li>
</ul>
<h4>Memory Leaks</h4>
<ul>
<li><strong>Cause</strong>: Failed consolidation or improper memory deletion</li>
<li><strong>Solution</strong>: Verify consolidation process and check deletion logs</li>
</ul>
<h4>Performance Degradation</h4>
<ul>
<li><strong>Cause</strong>: Large memory database without proper indexing</li>
<li><strong>Solution</strong>: Implement approximate nearest neighbor search or database partitioning</li>
</ul>
<h2>Very Long-Term Memory System</h2>
<p>The Very Long-Term Memory (VLTM) system provides strategic knowledge management and cross-system memory integration. It uses SQLModel with junction tables to properly implement many-to-many relationships between memory patterns, consolidations, and strategic knowledge.</p>
<h3>Data Model Relationships</h3>
<p>The VLTM data models use junction tables to correctly implement many-to-many relationships:</p>
<p>``mermaid
classDiagram
class VeryLongTermMemory {
+memory_id: str
+memory_type: MemoryType
+created_at: datetime
+last_accessed: datetime
+access_count: int
+importance_score: float
+compressed_content: str
+metadata_info: str
}
class MemoryPattern {
+pattern_id: str
+pattern_type: PatternType
+pattern_description: str
+confidence_score: float
+pattern_data: str
+discovered_at: datetime
}
class MemoryConsolidation {
+consolidation_id: str
+consolidation_date: datetime
+consolidation_type: ConsolidationType
+memories_processed: int
+patterns_extracted: int
+compression_ratio: float
+success: bool
}
class StrategicKnowledge {
+knowledge_id: str
+knoledge_domain: str
+knoledge_summary: str
+confidence_level: float
+last_updated: datetime
}
class ConsolidationPattern {
+consolidation_id: str
+pattern_id: str
+extraction_confidence: float
}
class PatternStrategicKnowledge {
+pattern_id: str
+knoledge_id: str
+contribution_weight: float
}</p>
<p>VeryLongTermMemory "1" -- "0..<em>" MemoryPattern : contains
MemoryPattern "0..</em>" -- "0..<em>" MemoryConsolidation : extracted in
MemoryPattern "0..</em>" -- "0..<em>" StrategicKnowledge : contributes to
MemoryConsolidation "1" -- "0..</em>" ConsolidationPattern : has
ConsolidationPattern "0..<em>" -- "0..</em>" MemoryPattern : links
StrategicKnowledge "1" -- "0..<em>" PatternStrategicKnowledge : has
PatternStrategicKnowledge "0..</em>" -- "0..*" MemoryPattern : links</p>
<pre><code>
**Diagram sources**
- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*

**Section sources**
- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*

#### Junction Table Implementation
The system uses junction tables to properly implement many-to-many relationships:

```python
class ConsolidationPattern(SQLModel, table=True):
    """Junction table linking memory consolidations and patterns"""
    __tablename__ = "consolidation_patterns"
    
    consolidation_id: str = Field(foreign_key="memory_consolidations.consolidation_id", primary_key=True)
    pattern_id: str = Field(foreign_key="memory_patterns.pattern_id", primary_key=True)
    extraction_confidence: float = Field(default=1.0)


class PatternStrategicKnowledge(SQLModel, table=True):
    """Junction table linking memory patterns and strategic knowledge"""
    __tablename__ = "pattern_strategic_knowledge"
    
    pattern_id: str = Field(foreign_key="memory_patterns.pattern_id", primary_key=True)
    knowledge_id: str = Field(foreign_key="strategic_knowledge.knowledge_id", primary_key=True)
    contribution_weight: float = Field(default=1.0)
</code></pre>
<p>The relationships are properly defined using the <code>link_model</code> parameter:</p>
<pre><code class="language-python">class MemoryPattern(SQLModel, table=True):
    # Fixed the relationship to use the junction table
    consolidations: List["MemoryConsolidation"] = Relationship(
        back_populates="extracted_patterns",
        link_model=ConsolidationPattern  # Using the junction table
    )
    strategic_knowledge: List["StrategicKnowledge"] = Relationship(
        back_populates="patterns",
        link_model=PatternStrategicKnowledge  # Using the junction table
    )
</code></pre>
<h3>Memory Integration Manager</h3>
<p>The MemoryIntegrationManager coordinates memory flow between existing memory systems and the VLTM system.</p>
<h4>Integration Architecture</h4>
<p>``mermaid
classDiagram
class MemoryIntegrationManager {
+initialize()
+shutdown()
+trigger_manual_sync()
+get_integration_status()
}
class MemoryBridge {
+source_system: str
+target_system: str
+flow_direction: MemoryFlowDirection
+memory_types: List[MemoryType]
+sync_interval_minutes: int
+batch_size: int
+enabled: bool
}
class IntegrationStats {
+memories_synchronized: int
+patterns_extracted: int
+knoledge_consolidated: int
+failed_operations: int
+processing_time_seconds: float
+last_sync_timestamp: datetime
}
class MemoryFlowDirection {
+TO_VLTM: "to_vltm"
+FROM_VLTM: "from_vltm"
+BIDIRECTIONAL: "bidirectional"
}
class IntegrationMode {
+REAL_TIME: "real_time"
+BATCH: "batch"
+HYBRID: "hybrid"
+SELECTIVE: "selective"
}</p>
<p>MemoryIntegrationManager --> MemoryBridge : "manages"
MemoryIntegrationManager --> IntegrationStats : "tracks"
MemoryIntegrationManager --> MemoryFlowDirection : "uses"
MemoryIntegrationManager --> IntegrationMode : "uses"</p>
<pre><code>
**Diagram sources**
- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*

**Section sources**
- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*

#### Memory Bridge Configuration
The system uses configurable memory bridges to control memory flow:

```python
@dataclass
class MemoryBridge:
    """Configuration for memory system bridge"""
    source_system: str
    target_system: str
    flow_direction: MemoryFlowDirection
    memory_types: List[MemoryType]
    sync_interval_minutes: int = 60
    batch_size: int = 100
    enabled: bool = True
</code></pre>
<p>Default bridges are set up during initialization:</p>
<pre><code class="language-python">async def _setup_default_bridges(self):
    """Setup default memory bridges between systems"""
    
    # Bridge: Episodic Memory  VLTM
    episodic_to_vltm = MemoryBridge(
        source_system="episodic_memory",
        target_system="vltm",
        flow_direction=MemoryFlowDirection.TO_VLTM,
        memory_types=[
            MemoryType.SUCCESSFUL_IMPROVEMENT,
            MemoryType.FAILED_EXPERIMENT,
            MemoryType.CRITICAL_FAILURE,
            MemoryType.ARCHITECTURAL_INSIGHT
        ],
        sync_interval_minutes=30,
        batch_size=50
    )
    
    # Bridge: Knowledge System  VLTM
    knowledge_to_vltm = MemoryBridge(
        source_system="knowledge_service",
        target_system="vltm",
        flow_direction=MemoryFlowDirection.TO_VLTM,
        memory_types=[
            MemoryType.STRATEGIC_KNOWLEDGE,
            MemoryType.META_LEARNING_RULE,
            MemoryType.CODE_PATTERN
        ],
        sync_interval_minutes=60,
        batch_size=25
    )
    
    # Bridge: VLTM  Knowledge System (strategic insights)
    vltm_to_knowledge = MemoryBridge(
        source_system="vltm",
        target_system="knowledge_service",
        flow_direction=MemoryFlowDirection.FROM_VLTM,
        memory_types=[MemoryType.STRATEGIC_KNOWLEDGE],
        sync_interval_minutes=120,
        batch_size=10
    )
    
    self.memory_bridges = [episodic_to_vltm, knowledge_to_vltm, vltm_to_knowledge]
</code></pre>
<h4>Memory Synchronization</h4>
<p>The integration manager continuously synchronizes memories across bridges:</p>
<pre><code class="language-python">async def _sync_bridge_continuously(self, bridge: MemoryBridge):
    """Continuously sync a memory bridge"""
    
    while self.is_running:
        try:
            await self._sync_memory_bridge(bridge)
            
            # Wait for the next sync interval
            await asyncio.sleep(bridge.sync_interval_minutes * 60)
            
        except asyncio.CancelledError:
            logger.info(f"Sync task cancelled for bridge: {bridge.source_system}  {bridge.target_system}")
            break
        except Exception as e:
            logger.error(f"Error in bridge sync: {e}")
            # Wait before retrying
            await asyncio.sleep(60)
</code></pre>
<h4>Memory Classification and Conversion</h4>
<p>The system classifies episodic memories into VLTM memory types:</p>
<pre><code class="language-python">def _classify_episodic_memory(self, memory_data: Dict[str, Any]) -> MemoryType:
    """Classify episodic memory into VLTM memory type"""
    
    content = memory_data.get("content", "").lower()
    tags = memory_data.get("tags", [])
    
    # Classification logic
    if any(word in content for word in ["optimized", "improved", "enhanced"]):
        return MemoryType.SUCCESSFUL_IMPROVEMENT
    elif any(word in content for word in ["error", "failed", "crash"]):
        if any(word in content for word in ["critical", "severe"]):
            return MemoryType.CRITICAL_FAILURE
        else:
            return MemoryType.FAILED_EXPERIMENT
    elif any(word in content for word in ["architecture", "design", "pattern"]):
        return MemoryType.ARCHITECTURAL_INSIGHT
    elif "optimization" in tags:
        return MemoryType.SUCCESSFUL_IMPROVEMENT
    else:
        return MemoryType.CODE_PATTERN
</code></pre>
<p>Memories are converted between systems with appropriate metadata:</p>
<pre><code class="language-python">async def _convert_episodic_to_vltm(self, memory_data: Dict[str, Any], memory_type: MemoryType) -> Optional[Dict[str, Any]]:
    """Convert episodic memory to VLTM format"""
    
    try:
        content = {
            "original_content": memory_data.get("content"),
            "source_system": "episodic_memory",
            "integration_info": {
                "synced_at": datetime.utcnow().isoformat(),
                "original_id": memory_data.get("id"),
                "confidence": memory_data.get("confidence", 0.5)
            }
        }
        
        metadata = {
            "episodic_sync": True,
            "original_timestamp": memory_data.get("timestamp").isoformat() if memory_data.get("timestamp") else None,
            "tags": memory_data.get("tags", [])
        }
        
        return {
            "content": content,
            "memory_type": memory_type,
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Error converting episodic memory: {e}")
        return None
</code></pre>
<h2>Conclusion</h2>
<p>The RAVANA memory system implements a sophisticated dual-architecture approach combining episodic and semantic memory systems. The episodic memory captures specific experiences using PostgreSQL with pgvector and SentenceTransformers for vector-based storage and retrieval, while the semantic memory system uses LLM-driven knowledge compression to create structured summaries. The system has been enhanced with multi-modal capabilities, supporting text, audio, and image content with cross-modal search and unified embedding generation. The MultiModalMemoryService provides a comprehensive interface for memory operations, and the system incorporates automated consolidation to prevent memory bloat. Performance is optimized through vector indexing and async operations, with comprehensive logging and diagnostic capabilities for debugging. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that properly implements many-to-many relationships using junction tables and provides strategic knowledge management through the MemoryIntegrationManager. This architecture enables the system to maintain long-term context, learn from experiences, and provide increasingly personalized responses over time.</p>
<p><strong>Referenced Files in This Document</strong></p>
<ul>
<li><a>memory.py</a> - <em>Updated in recent commit</em></li>
<li><a>client.py</a> - <em>Updated in recent commit</em></li>
<li><a>embedding_service.py</a> - <em>Added in recent commit</em></li>
<li><a>search_engine.py</a> - <em>Added in recent commit</em></li>
<li><a>multi_modal_service.py</a> - <em>Added in recent commit</em></li>
<li><a>models.py</a> - <em>Added in recent commit</em></li>
<li><a>postgresql_store.py</a> - <em>Added in recent commit</em></li>
<li><a>memory_service.py</a></li>
<li><a>compressed_memory.py</a></li>
<li><a>main.py</a></li>
<li><a>compression_prompts.py</a></li>
<li><a>test_memory.py</a></li>
<li><a>knowledge_service.py</a></li>
<li><a>vltm_data_models.py</a> - <em>Updated in recent commit</em></li>
<li><a>vltm_memory_integration_manager.py</a> - <em>Updated in recent commit</em></li>
<li><a>llm.py</a> - <em>Updated in recent commit</em></li>
</ul>
</div></article><div class="w-full md:w-64 flex-shrink-0"></div></div></main></div><footer class="bg-wiki-dark text-white p-4"><div class="container mx-auto text-center"><p> <!-- -->2025<!-- --> RAVANA AGI System Documentation</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"doc":{"slug":"Memory Systems","title":"Memory Systems","content":"\u003ch1\u003eMemory Systems\u003c/h1\u003e\n\u003ch2\u003eUpdate Summary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eChanges Made\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUpdated documentation to reflect corrected many-to-many relationship declarations in VLTM data models\u003c/li\u003e\n\u003cli\u003eAdded new section on Very Long-Term Memory (VLTM) system and its integration with existing memory systems\u003c/li\u003e\n\u003cli\u003eEnhanced architectural diagrams to include VLTM components and integration patterns\u003c/li\u003e\n\u003cli\u003eAdded detailed information about memory bridges, flow direction, and synchronization mechanisms\u003c/li\u003e\n\u003cli\u003eUpdated code examples to reflect the use of junction tables for many-to-many relationships\u003c/li\u003e\n\u003cli\u003eAdded information about memory classification, conversion, and importance threshold evaluation\u003c/li\u003e\n\u003cli\u003eIntegrated LLM reliability improvements from core/llm.py into memory system documentation\u003c/li\u003e\n\u003cli\u003eAdded details about enhanced LLM error handling, JSON parsing, and response validation in memory operations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#memory-architecture-overview\"\u003eMemory Architecture Overview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#episodic-memory-system\"\u003eEpisodic Memory System\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#multi-modal-embedding-service\"\u003eMulti-Modal Embedding Service\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#advanced-search-engine\"\u003eAdvanced Search Engine\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#multi-modal-memory-orchestration\"\u003eMulti-Modal Memory Orchestration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#semantic-memory-and-knowledge-compression\"\u003eSemantic Memory and Knowledge Compression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#memoryservice-interface-and-crud-operations\"\u003eMemoryService Interface and CRUD Operations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#memory-retrieval-patterns\"\u003eMemory Retrieval Patterns\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#consolidation-triggers-and-retention-policies\"\u003eConsolidation Triggers and Retention Policies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#performance-considerations\"\u003ePerformance Considerations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#debugging-memory-issues\"\u003eDebugging Memory Issues\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#very-long-term-memory-system\"\u003eVery Long-Term Memory System\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe RAVANA system implements a dual-memory architecture combining episodic and semantic memory systems to enable long-term learning and contextual awareness. This document details the design, implementation, and operational characteristics of these memory systems, focusing on their storage mechanisms, retrieval patterns, and integration points. The system leverages PostgreSQL with pgvector for similarity-based retrieval and employs LLM-driven knowledge compression to transform raw experiences into structured semantic summaries. Recent enhancements have introduced multi-modal memory processing with support for text, audio, and image content, enabling cross-modal search and unified embedding generation. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that integrates with existing memory systems through configurable memory bridges, enabling strategic knowledge consolidation and cross-system synchronization. The memory system has been enhanced with improved LLM reliability features including detailed logging, enhanced error handling, and robust JSON parsing to ensure consistent memory operations.\u003c/p\u003e\n\u003ch2\u003eMemory Architecture Overview\u003c/h2\u003e\n\u003cp\u003e``mermaid\ngraph TB\nsubgraph \"Memory Systems\"\nEM[Episodic Memory]\nSM[Semantic Memory]\nVLTM[Very Long-Term Memory]\nend\nsubgraph \"Processing\"\nEX[Memory Extraction]\nCO[Consolidation]\nKC[Knowledge Compression]\nES[Embedding Service]\nSE[Search Engine]\nMI[Memory Integration]\nend\nsubgraph \"Storage\"\nPG[PostgreSQL with pgvector]\nSQ[SQLModel Database]\nFS[File System]\nend\nsubgraph \"Access\"\nMS[MemoryService]\nMMS[MultiModalService]\nKS[KnowledgeService]\nVIM[VLTM Integration Manager]\nend\nUserInput --\u003e EX\nEX --\u003e EM\nEM --\u003e CO\nCO --\u003e PG\nKC --\u003e SM\nSM --\u003e SQ\nMS --\u003e EM\nMMS --\u003e EM\nKS --\u003e SM\nEM --\u003e |Vector Search| SE\nSM --\u003e |Semantic Search| KS\nPG --\u003e EM\nSQ --\u003e SM\nFS --\u003e SM\nES --\u003e PG\nSE --\u003e PG\nMMS --\u003e ES\nMMS --\u003e SE\nEM --\u003e MI\nSM --\u003e MI\nMI --\u003e VLTM\nVLTM --\u003e KS\nVIM --\u003e MI\nstyle EM fill:#f9f,stroke:#333\nstyle SM fill:#bbf,stroke:#333\nstyle VLTM fill:#9f9,stroke:#333\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [knowledge_service.py](file://services/knowledge_service.py#L0-L255)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779)\n\n## Episodic Memory System\n\nThe episodic memory system captures and stores specific events and interactions as discrete memory records. Each memory is stored with rich metadata and indexed using vector embeddings for similarity-based retrieval. Recent updates have replaced ChromaDB with PostgreSQL enhanced with pgvector extension, enabling robust multi-modal storage and advanced querying capabilities.\n\n### Storage Mechanism with PostgreSQL and SentenceTransformers\n\nEpisodic memories are stored in PostgreSQL with pgvector extension, providing a production-grade database solution for vector similarity search. The system uses SentenceTransformers to generate embeddings for memory texts, enabling semantic search capabilities.\n\n``mermaid\nclassDiagram\nclass MemoryRecord {\n+uuid id\n+ContentType content_type\n+string content_text\n+string file_path\n+List[float] text_embedding\n+List[float] image_embedding\n+List[float] audio_embedding\n+List[float] unified_embedding\n+datetime created_at\n+datetime last_accessed\n+int access_count\n+MemoryType memory_type\n+float emotional_valence\n+List[str] tags\n}\nclass PostgreSQLStore {\n+save_memory_record(record)\n+get_memory_record(id)\n+vector_search(embedding)\n+get_memory_statistics()\n}\nclass MultiModalMemoryService {\n+process_text_memory(text)\n+process_audio_memory(path)\n+process_image_memory(path)\n+search_memories(request)\n}\nPostgreSQLStore --\u003e MemoryRecord : \"stores\"\nMultiModalMemoryService --\u003e PostgreSQLStore : \"uses\"\nMultiModalMemoryService --\u003e MemoryRecord : \"manages\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emodels.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epostgresql_store.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emulti_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ememory.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epostgresql_store.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodels.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eEmbedding Generation and Storage\u003c/h4\u003e\n\u003cp\u003eThe system uses the \u003ccode\u003eall-MiniLM-L6-v2\u003c/code\u003e SentenceTransformer model to generate 384-dimensional text embeddings. For multi-modal content, specialized embedding generation is implemented:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass EmbeddingService:\n    def __init__(self, text_model_name: str = \"all-MiniLM-L6-v2\"):\n        self.text_model_name = text_model_name\n        self.text_embedding_dim = 384\n        self.image_embedding_dim = 512\n        self.audio_embedding_dim = 512\n        self.unified_embedding_dim = 1024\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMemories are stored with comprehensive metadata including creation timestamp, access statistics, content type, and confidence scores:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ememory_record = MemoryRecord(\n    content_type=content_type,\n    content_text=content_text,\n    file_path=file_path,\n    memory_type=memory_type,\n    tags=tags or [],\n    emotional_valence=emotional_valence,\n    confidence_score=confidence_score,\n    created_at=datetime.utcnow()\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eMemory Extraction Process\u003c/h4\u003e\n\u003cp\u003eThe system extracts memories from conversations using an LLM-powered extraction process. The \u003ccode\u003eextract_memories_from_conversation\u003c/code\u003e method analyzes user-AI interactions and identifies key information to store:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def extract_memories_from_conversation(self, request: ConversationRequest) -\u003e MemoriesList:\n    prompt = f\"\"\"\n    You are a memory extraction module for an AGI. Your task is to analyze a conversation \n    and identify key pieces of information to be stored in the AGI's long-term memory.\n\n    Focus on extracting:\n    - Key facts and information\n    - User preferences and characteristics\n    - Important goals, plans, or intentions\n    - Notable events or experiences\n    - Emotional context if relevant\n\n    Guidelines:\n    - Each memory should be a single, self-contained statement\n    - Keep memories concise (under 30 words)\n    - Prefer information that is likely to be relevant long-term\n    - Do not store transitory conversational details\n    - Output as a JSON object with a \"memories\" array\n\n    Conversation:\n    User: {request.user_input}\n    AI: {request.ai_output}\n    \"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe extraction focuses on key facts, user preferences, major goals, and core beliefs while filtering out transitory conversational details.\u003c/p\u003e\n\u003ch2\u003eMulti-Modal Embedding Service\u003c/h2\u003e\n\u003cp\u003eThe EmbeddingService provides multi-modal embedding generation for text, audio, and image content, enabling cross-modal retrieval and unified embedding creation.\u003c/p\u003e\n\u003ch3\u003eMulti-Modal Embedding Generation\u003c/h3\u003e\n\u003cp\u003eThe embedding service supports multiple content types with specialized processing:\u003c/p\u003e\n\u003cp\u003e``mermaid\nflowchart TD\nA[Input Content] --\u003e B{Content Type}\nB --\u003e C[Text]\nB --\u003e D[Audio]\nB --\u003e E[Image]\nC --\u003e F[Generate Text Embeddingusing SentenceTransformer]\nD --\u003e G[Process Audio with WhisperExtract Features]\nE --\u003e H[Extract Image FeaturesColor, Dimensions, Histogram]\nF --\u003e I[Store text_embedding]\nG --\u003e J[Store audio_embedding]\nH --\u003e K[Store image_embedding]\nI --\u003e L[Generate Unified Embedding]\nJ --\u003e L\nK --\u003e L\nL --\u003e M[Store unified_embedding]\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n- [models.py](file://modules/episodic_memory/models.py#L0-L250)\n\n**Section sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n\n#### Text Embedding Implementation\nText embeddings are generated using SentenceTransformers with caching for performance:\n\n```python\nasync def generate_text_embedding(self, text: str) -\u003e List[float]:\n    # Check cache first\n    cached = self.cache.get(text, self.text_model_name)\n    if cached is not None:\n        return cached\n    \n    self._load_text_model()\n    \n    try:\n        loop = asyncio.get_event_loop()\n        embedding = await loop.run_in_executor(\n            None,\n            lambda: self.text_model.encode(text, convert_to_tensor=False, normalize_embeddings=True)\n        )\n        \n        embedding_list = embedding.tolist()\n        self.cache.put(text, self.text_model_name, embedding_list)\n        \n        return embedding_list\n        \n    except Exception as e:\n        logger.error(f\"Text embedding generation failed: {e}\")\n        return [0.0] * self.text_embedding_dim\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eAudio Embedding Implementation\u003c/h4\u003e\n\u003cp\u003eAudio embeddings are generated from Whisper transcription and audio features:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def generate_audio_embedding(self, audio_features: Dict[str, Any]) -\u003e List[float]:\n    features = []\n    \n    # Extract numerical features from audio analysis\n    if \"mfcc\" in audio_features:\n        mfcc_data = audio_features[\"mfcc\"]\n        if \"mean\" in mfcc_data:\n            features.extend(mfcc_data[\"mean\"])\n        if \"std\" in mfcc_data:\n            features.extend(mfcc_data[\"std\"])\n    \n    if \"spectral_centroid\" in audio_features:\n        sc = audio_features[\"spectral_centroid\"]\n        features.extend([sc.get(\"mean\", 0.0), sc.get(\"std\", 0.0)])\n    \n    # Pad or truncate to desired dimension\n    if len(features) \u0026#x3C; self.audio_embedding_dim:\n        features.extend([0.0] * (self.audio_embedding_dim - len(features)))\n    else:\n        features = features[:self.audio_embedding_dim]\n    \n    return features\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eImage Embedding Implementation\u003c/h4\u003e\n\u003cp\u003eImage embeddings are generated from visual features (placeholder for CLIP in production):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def generate_image_embedding(self, image_path: str) -\u003e List[float]:\n    try:\n        image = Image.open(image_path).convert('RGB')\n        \n        # Extract basic statistics\n        img_array = np.array(image)\n        features = []\n        \n        # Color statistics\n        for channel in range(3):  # RGB\n            channel_data = img_array[:, :, channel].flatten()\n            features.extend([\n                float(np.mean(channel_data)),\n                float(np.std(channel_data)),\n                float(np.median(channel_data)),\n                float(np.percentile(channel_data, 25)),\n                float(np.percentile(channel_data, 75))\n            ])\n        \n        # Image dimensions\n        features.extend([\n            float(image.width),\n            float(image.height),\n            float(image.width * image.height)  # Area\n        ])\n        \n        # Histogram features\n        hist, _ = np.histogram(img_array.flatten(), bins=32, range=(0, 256))\n        hist_normalized = hist / np.sum(hist)\n        features.extend(hist_normalized.tolist())\n        \n        # Pad or truncate to desired dimension\n        if len(features) \u0026#x3C; self.image_embedding_dim:\n            features.extend([0.0] * (self.image_embedding_dim - len(features)))\n        else:\n            features = features[:self.image_embedding_dim]\n        \n        return features\n        \n    except Exception as e:\n        logger.error(f\"Image embedding generation failed for {image_path}: {e}\")\n        return [0.0] * self.image_embedding_dim\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eUnified Embedding Generation\u003c/h4\u003e\n\u003cp\u003eThe system generates unified embeddings by combining modalities with weighted fusion:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def generate_unified_embedding(self, memory_record: MemoryRecord) -\u003e List[float]:\n    unified = []\n    \n    # Combine embeddings with weights\n    text_weight = 0.4\n    image_weight = 0.3\n    audio_weight = 0.3\n    \n    # Text embedding (weighted)\n    if memory_record.text_embedding:\n        text_emb = np.array(memory_record.text_embedding) * text_weight\n        unified.extend(text_emb.tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * text_weight))\n    \n    # Image embedding (weighted)\n    if memory_record.image_embedding:\n        image_emb = np.array(memory_record.image_embedding) * image_weight\n        unified.extend(image_emb[:int(self.unified_embedding_dim * image_weight)].tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * image_weight))\n    \n    # Audio embedding (weighted)\n    if memory_record.audio_embedding:\n        audio_emb = np.array(memory_record.audio_embedding) * audio_weight\n        unified.extend(audio_emb[:int(self.unified_embedding_dim * audio_weight)].tolist())\n    else:\n        unified.extend([0.0] * int(self.unified_embedding_dim * audio_weight))\n    \n    # Normalize the unified embedding\n    unified_array = np.array(unified)\n    norm = np.linalg.norm(unified_array)\n    if norm \u003e 0:\n        unified_array = unified_array / norm\n    \n    return unified_array.tolist()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAdvanced Search Engine\u003c/h2\u003e\n\u003cp\u003eThe AdvancedSearchEngine provides sophisticated search capabilities including cross-modal search, similarity search, and hybrid search modes.\u003c/p\u003e\n\u003ch3\u003eCross-Modal Search Implementation\u003c/h3\u003e\n\u003cp\u003eThe search engine supports cross-modal queries where different content types can be used to search across modalities:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def cross_modal_search(self, request: CrossModalSearchRequest) -\u003e List[SearchResult]:\n    # Generate query embedding based on type\n    if request.query_type == ContentType.TEXT:\n        query_embedding = await self.embeddings.generate_text_embedding(request.query_content)\n    elif request.query_type == ContentType.AUDIO and self.whisper:\n        audio_result = await self.whisper.process_audio(request.query_content)\n        query_embedding = await self.embeddings.generate_text_embedding(\n            audio_result.get(\"transcript\", \"\")\n        )\n    elif request.query_type == ContentType.IMAGE:\n        query_embedding = await self.embeddings.generate_image_embedding(request.query_content)\n    else:\n        raise ValueError(f\"Unsupported query type: {request.query_type}\")\n    \n    # Search using unified embeddings\n    results = await self.postgres.vector_search(\n        embedding=query_embedding,\n        embedding_type=\"unified\",\n        limit=request.limit,\n        similarity_threshold=request.similarity_threshold,\n        content_types=request.target_types\n    )\n    \n    # Convert to SearchResult objects\n    search_results = []\n    for i, (memory_record, similarity) in enumerate(results):\n        search_results.append(SearchResult(\n            memory_record=memory_record,\n            similarity_score=similarity,\n            rank=i + 1,\n            search_metadata={\n                \"search_type\": \"cross_modal_specialized\",\n                \"query_type\": request.query_type.value,\n                \"target_types\": [ct.value for ct in request.target_types]\n            }\n        ))\n    \n    return search_results\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSimilarity Search Implementation\u003c/h3\u003e\n\u003cp\u003eFind memories similar to a given memory record:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def find_similar_memories(self, \n                              memory_record: MemoryRecord, \n                              limit: int = 10,\n                              similarity_threshold: float = 0.7) -\u003e List[SearchResult]:\n    # Use the best available embedding\n    if memory_record.unified_embedding:\n        embedding = memory_record.unified_embedding\n        embedding_type = \"unified\"\n    elif memory_record.text_embedding:\n        embedding = memory_record.text_embedding\n        embedding_type = \"text\"\n    elif memory_record.image_embedding:\n        embedding = memory_record.image_embedding\n        embedding_type = \"image\"\n    elif memory_record.audio_embedding:\n        embedding = memory_record.audio_embedding\n        embedding_type = \"audio\"\n    else:\n        logger.warning(\"No embeddings available for similarity search\")\n        return []\n    \n    # Search for similar memories\n    results = await self.postgres.vector_search(\n        embedding=embedding,\n        embedding_type=embedding_type,\n        limit=limit + 1,  # +1 to exclude the original\n        similarity_threshold=similarity_threshold\n    )\n    \n    # Convert to SearchResult objects and exclude the original\n    search_results = []\n    for i, (similar_record, similarity) in enumerate(results):\n        if similar_record.id != memory_record.id:  # Exclude the original\n            search_results.append(SearchResult(\n                memory_record=similar_record,\n                similarity_score=similarity,\n                rank=len(search_results) + 1,\n                search_metadata={\n                    \"search_type\": \"similarity\",\n                    \"reference_id\": str(memory_record.id),\n                    \"embedding_type\": embedding_type\n                }\n            ))\n    \n    return search_results[:limit]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eHybrid Search Configuration\u003c/h3\u003e\n\u003cp\u003eConfigure weights for hybrid search combining vector and text search:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef configure_search_weights(self, vector_weight: float, text_weight: float):\n    \"\"\"\n    Configure the weights for hybrid search.\n    \n    Args:\n        vector_weight: Weight for vector similarity (0-1)\n        text_weight: Weight for text search (0-1)\n    \"\"\"\n    total_weight = vector_weight + text_weight\n    if total_weight \u003e 0:\n        self.vector_weight = vector_weight / total_weight\n        self.text_weight = text_weight / total_weight\n        logger.info(f\"Updated search weights: vector={self.vector_weight:.2f}, text={self.text_weight:.2f}\")\n    else:\n        logger.warning(\"Invalid weights provided, keeping current configuration\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eMulti-Modal Memory Orchestration\u003c/h2\u003e\n\u003cp\u003eThe MultiModalMemoryService orchestrates all components of the memory system, providing a unified interface for multi-modal operations.\u003c/p\u003e\n\u003ch3\u003eService Architecture\u003c/h3\u003e\n\u003cp\u003e``mermaid\nclassDiagram\nclass MultiModalMemoryService {\n+initialize()\n+close()\n+process_text_memory(text)\n+process_audio_memory(path)\n+process_image_memory(path)\n+extract_memories_from_conversation(request)\n+search_memories(request)\n+find_similar_memories(id)\n+batch_process_files(request)\n+health_check()\n}\nclass PostgreSQLStore {\n+save_memory_record()\n+get_memory_record()\n+vector_search()\n}\nclass EmbeddingService {\n+generate_text_embedding()\n+generate_audio_embedding()\n+generate_image_embedding()\n+generate_unified_embedding()\n}\nclass WhisperAudioProcessor {\n+process_audio()\n}\nclass AdvancedSearchEngine {\n+search()\n+cross_modal_search()\n+find_similar_memories()\n}\nMultiModalMemoryService --\u003e PostgreSQLStore : \"uses\"\nMultiModalMemoryService --\u003e EmbeddingService : \"uses\"\nMultiModalMemoryService --\u003e WhisperAudioProcessor : \"uses\"\nMultiModalMemoryService --\u003e AdvancedSearchEngine : \"uses\"\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L0-L498)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L0-L508)\n\n**Section sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L0-L656)\n\n#### Text Memory Processing\nProcess and store text-based memories:\n\n```python\nasync def process_text_memory(self, \n                            text: str,\n                            memory_type: MemoryType = MemoryType.EPISODIC,\n                            tags: Optional[List[str]] = None,\n                            emotional_valence: Optional[float] = None) -\u003e MemoryRecord:\n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.TEXT,\n        content_text=text,\n        memory_type=memory_type,\n        tags=tags or [],\n        emotional_valence=emotional_valence,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed text memory: {saved_record.id}\")\n    return saved_record\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eAudio Memory Processing\u003c/h4\u003e\n\u003cp\u003eProcess and store audio memories with Whisper transcription:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def process_audio_memory(self, \n                             audio_path: str,\n                             context: Optional[str] = None,\n                             memory_type: MemoryType = MemoryType.EPISODIC,\n                             tags: Optional[List[str]] = None) -\u003e MemoryRecord:\n    # Process audio with Whisper\n    audio_result = await self.whisper_processor.process_audio(audio_path, context)\n    \n    # Create audio metadata\n    audio_metadata = self.whisper_processor.create_audio_metadata(audio_result)\n    \n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.AUDIO,\n        content_text=audio_result.get(\"transcript\"),\n        file_path=audio_path,\n        memory_type=memory_type,\n        tags=tags or [],\n        confidence_score=audio_result.get(\"confidence\", 0.8),\n        audio_metadata=audio_metadata,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed audio memory: {saved_record.id}\")\n    return saved_record\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eImage Memory Processing\u003c/h4\u003e\n\u003cp\u003eProcess and store image memories:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def process_image_memory(self, \n                             image_path: str,\n                             description: Optional[str] = None,\n                             memory_type: MemoryType = MemoryType.EPISODIC,\n                             tags: Optional[List[str]] = None) -\u003e MemoryRecord:\n    # Create basic image metadata\n    from PIL import Image\n    with Image.open(image_path) as img:\n        width, height = img.size\n    \n    image_metadata = ImageMetadata(\n        width=width,\n        height=height,\n        scene_description=description\n    )\n    \n    # Create memory record\n    memory_record = MemoryRecord(\n        content_type=ContentType.IMAGE,\n        content_text=description,\n        file_path=image_path,\n        memory_type=memory_type,\n        tags=tags or [],\n        image_metadata=image_metadata,\n        created_at=datetime.utcnow()\n    )\n    \n    # Generate embeddings\n    memory_record = await self.embedding_service.generate_embeddings(memory_record)\n    \n    # Save to database\n    saved_record = await self.postgres_store.save_memory_record(memory_record)\n    \n    logger.info(f\"Processed image memory: {saved_record.id}\")\n    return saved_record\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eBatch Processing\u003c/h4\u003e\n\u003cp\u003eProcess multiple files in batch with parallel processing support:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def batch_process_files(self, request: BatchProcessRequest) -\u003e BatchProcessResult:\n    # Process files\n    if request.parallel_processing:\n        # Process in parallel with limited concurrency\n        semaphore = asyncio.Semaphore(request.max_workers)\n        \n        async def process_with_semaphore(task):\n            async with semaphore:\n                return await task\n        \n        parallel_tasks = [process_with_semaphore(task) for task in tasks]\n        results = await asyncio.gather(*parallel_tasks, return_exceptions=True)\n    else:\n        # Process sequentially\n        for task in tasks:\n            result = await task\n            results.append(result)\n    \n    # Process results\n    processing_results = []\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            processing_results.append(ProcessingResult(\n                memory_record=None,\n                processing_time_ms=0,\n                success=False,\n                error_message=str(result)\n            ))\n            failed_count += 1\n        else:\n            processing_results.append(result)\n            if result.success:\n                successful_count += 1\n            else:\n                failed_count += 1\n    \n    return BatchProcessResult(\n        results=processing_results,\n        total_processed=len(request.file_paths),\n        successful_count=successful_count,\n        failed_count=failed_count,\n        total_time_ms=int(total_time)\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSemantic Memory and Knowledge Compression\u003c/h2\u003e\n\u003cp\u003eThe semantic memory system transforms episodic experiences into structured knowledge through a compression pipeline that identifies patterns, generalizes information, and creates concise summaries.\u003c/p\u003e\n\u003ch3\u003eKnowledge Compression Pipeline\u003c/h3\u003e\n\u003cp\u003eThe knowledge compression pipeline converts raw experiences into semantic summaries using LLM-driven analysis:\u003c/p\u003e\n\u003cp\u003e``mermaid\nflowchart TD\nA[Raw Experiences] --\u003e B{KnowledgeCompression}\nB --\u003e C[Pattern Recognition]\nC --\u003e D[Merge Related Facts]\nD --\u003e E[Deduplicate Redundant Info]\nE --\u003e F[Generalize Specifics]\nF --\u003e G[Create Summary]\nG --\u003e H[Store in Semantic Memory]\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [main.py](file://modules/knowledge_compression/main.py#L0-L42)\n- [compression_prompts.py](file://modules/knowledge_compression/compression_prompts.py#L0-L5)\n\n**Section sources**\n- [main.py](file://modules/knowledge_compression/main.py#L0-L42)\n- [compressed_memory.py](file://modules/knowledge_compression/compressed_memory.py#L0-L17)\n\n#### Compression Implementation\nThe compression process is implemented in the `compress_knowledge` function, which uses an LLM to summarize accumulated logs:\n\n```python\ndef compress_knowledge(logs):\n    prompt = COMPRESSION_PROMPT.format(logs=json.dumps(logs, indent=2))\n    summary = call_llm(prompt)\n    entry = {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"summary\": summary\n    }\n    save_summary(entry)\n    return entry\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe compression prompt instructs the LLM to produce structured summaries of new facts learned, key outcomes, and next goals:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eCOMPRESSION_PROMPT = (\n    \"You are an AI tasked with summarizing accumulated knowledge and logs. \"\n    \"Given the following logs, produce a concise summary report of new facts learned, key outcomes, and next goals.\\n\"\n    \"Logs: {logs}\\n\"\n    \"Respond in a clear, structured format.\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eStorage Mechanism\u003c/h4\u003e\n\u003cp\u003eCompressed knowledge is stored as JSON files on the filesystem, with each summary entry containing a timestamp and the LLM-generated summary:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef save_summary(entry):\n    data = load_summaries()\n    data.append(entry)\n    with open(COMPRESSED_FILE, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe system also integrates with SQLModel for database-backed semantic memory storage, where summaries are stored in a relational database with metadata:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass Summary(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    timestamp: str\n    summary_text: str\n    source: str\n    category: str\n    content_hash: str = Field(unique=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eMemoryService Interface and CRUD Operations\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003eMemoryService\u003c/code\u003e provides a unified interface for memory management operations, abstracting the underlying storage mechanisms.\u003c/p\u003e\n\u003cp\u003e``mermaid\nclassDiagram\nclass MemoryService {\n+get_relevant_memories(query_text)\n+save_memories(memories)\n+extract_memories(user_input, ai_output)\n+consolidate_memories()\n}\nclass MemoryAPI {\n+get_relevant_memories_api(request)\n+save_memories_api(request)\n+extract_memories_api(request)\n+consolidate_memories_api(request)\n}\nMemoryService --\u003e MemoryAPI : \"delegates\"\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [memory_service.py](file://services/memory_service.py#L0-L20)\n- [memory.py](file://modules/episodic_memory/memory.py#L0-L401)\n\n**Section sources**\n- [memory_service.py](file://services/memory_service.py#L0-L20)\n\n### CRUD Operations\n\nThe MemoryService implements the following CRUD operations:\n\n#### Create: save_memories\nStores new memories in the episodic memory system:\n\n```python\nasync def save_memories(self, memories):\n    await asyncio.to_thread(save_memories, memories)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe operation runs in a separate thread to avoid blocking the event loop.\u003c/p\u003e\n\u003ch4\u003eRead: get_relevant_memories\u003c/h4\u003e\n\u003cp\u003eRetrieves memories relevant to a query using vector similarity search:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def get_relevant_memories(self, query_text: str):\n    return await get_relevant_memories_api({\"query_text\": query_text})\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eUpdate: extract_memories\u003c/h4\u003e\n\u003cp\u003eExtracts and updates memories from new interactions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def extract_memories(self, user_input: str, ai_output: str):\n    return await extract_memories_api({\"user_input\": user_input, \"ai_output\": ai_output})\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eDelete: consolidate_memories\u003c/h4\u003e\n\u003cp\u003eRemoves redundant memories during consolidation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def consolidate_memories(self):\n    from modules.episodic_memory.memory import ConsolidateRequest\n    return await consolidate_memories_api(ConsolidateRequest())\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eMemory Retrieval Patterns\u003c/h2\u003e\n\u003cp\u003eThe system implements similarity-based retrieval patterns for efficient memory access.\u003c/p\u003e\n\u003ch3\u003eVector Search Implementation\u003c/h3\u003e\n\u003cp\u003eMemory retrieval uses PostgreSQL's pgvector extension for vector search capabilities:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def vector_search(self, \n                       embedding: List[float],\n                       embedding_type: str = \"text\",\n                       limit: int = 10,\n                       similarity_threshold: float = 0.7,\n                       content_types: Optional[List[ContentType]] = None) -\u003e List[Tuple[MemoryRecord, float]]:\n    # Build query based on embedding type\n    embedding_column = f\"{embedding_type}_embedding\"\n    \n    where_conditions = [f\"{embedding_column} IS NOT NULL\"]\n    params = [embedding]\n    param_count = 1\n    \n    if content_types:\n        param_count += 1\n        where_conditions.append(f\"content_type = ANY(${param_count})\")\n        params.append([ct.value for ct in content_types])\n    \n    param_count += 1\n    where_conditions.append(f\"1 - ({embedding_column} \u0026#x3C;=\u003e ${param_count}) \u003e= ${param_count + 1}\")\n    params.extend([embedding, similarity_threshold])\n    \n    query = f\"\"\"\n        SELECT *, 1 - ({embedding_column} \u0026#x3C;=\u003e $1) as similarity\n        FROM memory_records \n        WHERE {' AND '.join(where_conditions)}\n        ORDER BY {embedding_column} \u0026#x3C;=\u003e $1\n        LIMIT ${param_count + 2}\n    \"\"\"\n    params.append(limit)\n    \n    rows = await conn.fetch(query, *params)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eRetrieval Parameters\u003c/h3\u003e\n\u003cp\u003eThe retrieval process is configurable through the following parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003etop_n\u003c/strong\u003e: Maximum number of memories to return (default: 5)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esimilarity_threshold\u003c/strong\u003e: Minimum similarity score for inclusion (default: 0.7)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe similarity threshold acts as a filter to ensure only highly relevant memories are retrieved, preventing information overload.\u003c/p\u003e\n\u003ch3\u003eAccess Pattern Tracking\u003c/h3\u003e\n\u003cp\u003eThe system tracks memory access patterns by updating metadata on retrieval:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Update access metadata for retrieved memories\nif ids_to_update:\n    chroma_collection.update(ids=ids_to_update, metadatas=metadatas_to_update)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEach retrieved memory has its \u003ccode\u003elast_accessed\u003c/code\u003e timestamp and \u003ccode\u003eaccess_count\u003c/code\u003e updated, enabling usage-based retention policies.\u003c/p\u003e\n\u003ch2\u003eConsolidation Triggers and Retention Policies\u003c/h2\u003e\n\u003cp\u003eThe system implements automated memory consolidation to prevent memory bloat and improve efficiency.\u003c/p\u003e\n\u003ch3\u003eConsolidation Process\u003c/h3\u003e\n\u003cp\u003eThe consolidation process uses an LLM to merge, deduplicate, and generalize memories:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e@app.post(\"/consolidate_memories/\", response_model=StatusResponse, tags=[\"Memories\"])\nasync def consolidate_memories_api(request: ConsolidateRequest):\n    memories_data = chroma_collection.get(\n        limit=request.max_memories_to_process,\n        include=[\"metadatas\"]\n    )\n    \n    prompt = PROMPT_FOR_CONSOLIDATION + \"\\n\" + json.dumps(memories_to_process, indent=2)\n    llm_response_str = await asyncio.to_thread(call_llm, prompt)\n    consolidation_plan = parse_llm_json_response(llm_response_str)\n    \n    # Save new consolidated memories\n    if consolidation_plan[\"consolidated\"]:\n        save_memories(consolidation_plan[\"consolidated\"], memory_type='long-term-consolidated')\n    \n    # Delete old memories\n    if consolidation_plan[\"to_delete\"]:\n        chroma_collection.delete(ids=unique_to_delete_ids)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe consolidation prompt provides specific instructions for merging related memories, removing duplicates, and generalizing specific facts.\u003c/p\u003e\n\u003ch3\u003eTrigger Mechanism\u003c/h3\u003e\n\u003cp\u003eConsolidation is triggered programmatically by the system:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def consolidate_memories(self):\n    return await consolidate_memories_api(ConsolidateRequest())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the core system, consolidation is called at strategic points in the execution flow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003econsolidation_result = await self.memory_service.consolidate_memories()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eRetention Policies\u003c/h3\u003e\n\u003cp\u003eThe system implements retention through:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUsage-based prioritization\u003c/strong\u003e: Frequently accessed memories are retained\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRedundancy elimination\u003c/strong\u003e: Duplicate or overlapping memories are removed\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTemporal relevance\u003c/strong\u003e: Older, less accessed memories are prioritized for consolidation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe system currently fetches a random batch of memories for consolidation due to ChromaDB's lack of metadata-based sorting, but this could be enhanced with custom indexing.\u003c/p\u003e\n\u003ch2\u003ePerformance Considerations\u003c/h2\u003e\n\u003cp\u003eThe memory system incorporates several performance optimizations and scalability considerations.\u003c/p\u003e\n\u003ch3\u003eVector Search Optimization\u003c/h3\u003e\n\u003cp\u003e``mermaid\nflowchart TD\nA[Query Text] --\u003e B[Generate Embedding]\nB --\u003e C[Vector Search in PostgreSQL]\nC --\u003e D[Filter by Similarity]\nD --\u003e E[Update Access Metadata]\nE --\u003e F[Return Results]\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n\n**Section sources**\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L0-L590)\n\n#### Indexing Strategies\n- PostgreSQL with pgvector automatically indexes embeddings for fast similarity search\n- GIN indexes are used for tag-based filtering\n- The system could benefit from implementing HNSW or other approximate nearest neighbor algorithms for larger datasets\n\n#### Performance Metrics\n- Query latency: Optimized through in-memory vector indexing\n- Memory footprint: Controlled through periodic consolidation\n- Throughput: Async operations prevent blocking the main event loop\n\n### Semantic Search with FAISS\n\nFor semantic memory, the system uses FAISS for efficient vector search:\n\n```python\n# Initialize FAISS index for semantic search\nself.faiss_index = faiss.IndexFlatL2(self.embedding_dim)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe FAISS index is persisted to disk and automatically loaded on startup:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eif os.path.exists(self.index_file) and os.path.exists(self.id_map_file):\n    self.faiss_index = faiss.read_index(self.index_file)\n    with open(self.id_map_file, \"rb\") as f:\n        self.id_map = pickle.load(f)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eMemory Bloat Prevention\u003c/h3\u003e\n\u003cp\u003eThe system prevents memory bloat through:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConsolidation\u003c/strong\u003e: Regular merging of related memories\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeduplication\u003c/strong\u003e: Removal of redundant information\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccess tracking\u003c/strong\u003e: Usage-based retention prioritization\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBatch processing\u003c/strong\u003e: Limiting the number of memories processed at once\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDebugging Memory Issues\u003c/h2\u003e\n\u003cp\u003eThe system provides several mechanisms for debugging memory-related issues.\u003c/p\u003e\n\u003ch3\u003eHealth Monitoring\u003c/h3\u003e\n\u003cp\u003eThe memory service includes a health check endpoint:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def health_check(self) -\u003e Dict[str, Any]:\n    \"\"\"Perform comprehensive health check.\"\"\"\n    try:\n        # Check database connection\n        db_stats = await self.postgres_store.get_memory_statistics()\n        db_connected = bool(db_stats)\n        \n        # Check embedding service\n        test_embedding = await self.embedding_service.generate_text_embedding(\"test\")\n        embedding_ready = len(test_embedding) \u003e 0\n        \n        uptime = (datetime.utcnow() - self.start_time).total_seconds()\n        \n        return {\n            \"status\": \"healthy\" if db_connected and embedding_ready else \"degraded\",\n            \"database_connected\": db_connected,\n            \"embedding_service_ready\": embedding_ready,\n            \"memory_count\": db_stats.get(\"total_memories\", 0),\n            \"uptime_seconds\": int(uptime),\n            \"initialized\": self.initialized\n        }\n        \n    except Exception as e:\n        logger.error(f\"Health check failed: {e}\")\n        return {\n            \"status\": \"unhealthy\",\n            \"error\": str(e),\n            \"initialized\": self.initialized\n        }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis endpoint verifies database connectivity and reports the current memory count.\u003c/p\u003e\n\u003ch3\u003eDiagnostic Endpoints\u003c/h3\u003e\n\u003cp\u003eAdditional diagnostic capabilities include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003elist_memories_api\u003c/strong\u003e: Retrieves all stored memories for inspection\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLogging\u003c/strong\u003e: Comprehensive logging of memory operations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStatus responses\u003c/strong\u003e: Detailed operation results with metadata\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCommon Issues and Solutions\u003c/h3\u003e\n\u003ch4\u003eRetrieval Inaccuracies\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCause\u003c/strong\u003e: Low similarity threshold or poor embedding quality\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSolution\u003c/strong\u003e: Adjust similarity_threshold parameter or retrain embeddings\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eMemory Leaks\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCause\u003c/strong\u003e: Failed consolidation or improper memory deletion\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSolution\u003c/strong\u003e: Verify consolidation process and check deletion logs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ePerformance Degradation\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCause\u003c/strong\u003e: Large memory database without proper indexing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSolution\u003c/strong\u003e: Implement approximate nearest neighbor search or database partitioning\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eVery Long-Term Memory System\u003c/h2\u003e\n\u003cp\u003eThe Very Long-Term Memory (VLTM) system provides strategic knowledge management and cross-system memory integration. It uses SQLModel with junction tables to properly implement many-to-many relationships between memory patterns, consolidations, and strategic knowledge.\u003c/p\u003e\n\u003ch3\u003eData Model Relationships\u003c/h3\u003e\n\u003cp\u003eThe VLTM data models use junction tables to correctly implement many-to-many relationships:\u003c/p\u003e\n\u003cp\u003e``mermaid\nclassDiagram\nclass VeryLongTermMemory {\n+memory_id: str\n+memory_type: MemoryType\n+created_at: datetime\n+last_accessed: datetime\n+access_count: int\n+importance_score: float\n+compressed_content: str\n+metadata_info: str\n}\nclass MemoryPattern {\n+pattern_id: str\n+pattern_type: PatternType\n+pattern_description: str\n+confidence_score: float\n+pattern_data: str\n+discovered_at: datetime\n}\nclass MemoryConsolidation {\n+consolidation_id: str\n+consolidation_date: datetime\n+consolidation_type: ConsolidationType\n+memories_processed: int\n+patterns_extracted: int\n+compression_ratio: float\n+success: bool\n}\nclass StrategicKnowledge {\n+knowledge_id: str\n+knoledge_domain: str\n+knoledge_summary: str\n+confidence_level: float\n+last_updated: datetime\n}\nclass ConsolidationPattern {\n+consolidation_id: str\n+pattern_id: str\n+extraction_confidence: float\n}\nclass PatternStrategicKnowledge {\n+pattern_id: str\n+knoledge_id: str\n+contribution_weight: float\n}\u003c/p\u003e\n\u003cp\u003eVeryLongTermMemory \"1\" -- \"0..\u003cem\u003e\" MemoryPattern : contains\nMemoryPattern \"0..\u003c/em\u003e\" -- \"0..\u003cem\u003e\" MemoryConsolidation : extracted in\nMemoryPattern \"0..\u003c/em\u003e\" -- \"0..\u003cem\u003e\" StrategicKnowledge : contributes to\nMemoryConsolidation \"1\" -- \"0..\u003c/em\u003e\" ConsolidationPattern : has\nConsolidationPattern \"0..\u003cem\u003e\" -- \"0..\u003c/em\u003e\" MemoryPattern : links\nStrategicKnowledge \"1\" -- \"0..\u003cem\u003e\" PatternStrategicKnowledge : has\nPatternStrategicKnowledge \"0..\u003c/em\u003e\" -- \"0..*\" MemoryPattern : links\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*\n\n**Section sources**\n- [vltm_data_models.py](file://core/vltm_data_models.py#L0-L325) - *Updated in recent commit*\n\n#### Junction Table Implementation\nThe system uses junction tables to properly implement many-to-many relationships:\n\n```python\nclass ConsolidationPattern(SQLModel, table=True):\n    \"\"\"Junction table linking memory consolidations and patterns\"\"\"\n    __tablename__ = \"consolidation_patterns\"\n    \n    consolidation_id: str = Field(foreign_key=\"memory_consolidations.consolidation_id\", primary_key=True)\n    pattern_id: str = Field(foreign_key=\"memory_patterns.pattern_id\", primary_key=True)\n    extraction_confidence: float = Field(default=1.0)\n\n\nclass PatternStrategicKnowledge(SQLModel, table=True):\n    \"\"\"Junction table linking memory patterns and strategic knowledge\"\"\"\n    __tablename__ = \"pattern_strategic_knowledge\"\n    \n    pattern_id: str = Field(foreign_key=\"memory_patterns.pattern_id\", primary_key=True)\n    knowledge_id: str = Field(foreign_key=\"strategic_knowledge.knowledge_id\", primary_key=True)\n    contribution_weight: float = Field(default=1.0)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe relationships are properly defined using the \u003ccode\u003elink_model\u003c/code\u003e parameter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass MemoryPattern(SQLModel, table=True):\n    # Fixed the relationship to use the junction table\n    consolidations: List[\"MemoryConsolidation\"] = Relationship(\n        back_populates=\"extracted_patterns\",\n        link_model=ConsolidationPattern  # Using the junction table\n    )\n    strategic_knowledge: List[\"StrategicKnowledge\"] = Relationship(\n        back_populates=\"patterns\",\n        link_model=PatternStrategicKnowledge  # Using the junction table\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eMemory Integration Manager\u003c/h3\u003e\n\u003cp\u003eThe MemoryIntegrationManager coordinates memory flow between existing memory systems and the VLTM system.\u003c/p\u003e\n\u003ch4\u003eIntegration Architecture\u003c/h4\u003e\n\u003cp\u003e``mermaid\nclassDiagram\nclass MemoryIntegrationManager {\n+initialize()\n+shutdown()\n+trigger_manual_sync()\n+get_integration_status()\n}\nclass MemoryBridge {\n+source_system: str\n+target_system: str\n+flow_direction: MemoryFlowDirection\n+memory_types: List[MemoryType]\n+sync_interval_minutes: int\n+batch_size: int\n+enabled: bool\n}\nclass IntegrationStats {\n+memories_synchronized: int\n+patterns_extracted: int\n+knoledge_consolidated: int\n+failed_operations: int\n+processing_time_seconds: float\n+last_sync_timestamp: datetime\n}\nclass MemoryFlowDirection {\n+TO_VLTM: \"to_vltm\"\n+FROM_VLTM: \"from_vltm\"\n+BIDIRECTIONAL: \"bidirectional\"\n}\nclass IntegrationMode {\n+REAL_TIME: \"real_time\"\n+BATCH: \"batch\"\n+HYBRID: \"hybrid\"\n+SELECTIVE: \"selective\"\n}\u003c/p\u003e\n\u003cp\u003eMemoryIntegrationManager --\u003e MemoryBridge : \"manages\"\nMemoryIntegrationManager --\u003e IntegrationStats : \"tracks\"\nMemoryIntegrationManager --\u003e MemoryFlowDirection : \"uses\"\nMemoryIntegrationManager --\u003e IntegrationMode : \"uses\"\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*\n\n**Section sources**\n- [vltm_memory_integration_manager.py](file://core/vltm_memory_integration_manager.py#L0-L779) - *Updated in recent commit*\n\n#### Memory Bridge Configuration\nThe system uses configurable memory bridges to control memory flow:\n\n```python\n@dataclass\nclass MemoryBridge:\n    \"\"\"Configuration for memory system bridge\"\"\"\n    source_system: str\n    target_system: str\n    flow_direction: MemoryFlowDirection\n    memory_types: List[MemoryType]\n    sync_interval_minutes: int = 60\n    batch_size: int = 100\n    enabled: bool = True\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDefault bridges are set up during initialization:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def _setup_default_bridges(self):\n    \"\"\"Setup default memory bridges between systems\"\"\"\n    \n    # Bridge: Episodic Memory  VLTM\n    episodic_to_vltm = MemoryBridge(\n        source_system=\"episodic_memory\",\n        target_system=\"vltm\",\n        flow_direction=MemoryFlowDirection.TO_VLTM,\n        memory_types=[\n            MemoryType.SUCCESSFUL_IMPROVEMENT,\n            MemoryType.FAILED_EXPERIMENT,\n            MemoryType.CRITICAL_FAILURE,\n            MemoryType.ARCHITECTURAL_INSIGHT\n        ],\n        sync_interval_minutes=30,\n        batch_size=50\n    )\n    \n    # Bridge: Knowledge System  VLTM\n    knowledge_to_vltm = MemoryBridge(\n        source_system=\"knowledge_service\",\n        target_system=\"vltm\",\n        flow_direction=MemoryFlowDirection.TO_VLTM,\n        memory_types=[\n            MemoryType.STRATEGIC_KNOWLEDGE,\n            MemoryType.META_LEARNING_RULE,\n            MemoryType.CODE_PATTERN\n        ],\n        sync_interval_minutes=60,\n        batch_size=25\n    )\n    \n    # Bridge: VLTM  Knowledge System (strategic insights)\n    vltm_to_knowledge = MemoryBridge(\n        source_system=\"vltm\",\n        target_system=\"knowledge_service\",\n        flow_direction=MemoryFlowDirection.FROM_VLTM,\n        memory_types=[MemoryType.STRATEGIC_KNOWLEDGE],\n        sync_interval_minutes=120,\n        batch_size=10\n    )\n    \n    self.memory_bridges = [episodic_to_vltm, knowledge_to_vltm, vltm_to_knowledge]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eMemory Synchronization\u003c/h4\u003e\n\u003cp\u003eThe integration manager continuously synchronizes memories across bridges:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def _sync_bridge_continuously(self, bridge: MemoryBridge):\n    \"\"\"Continuously sync a memory bridge\"\"\"\n    \n    while self.is_running:\n        try:\n            await self._sync_memory_bridge(bridge)\n            \n            # Wait for the next sync interval\n            await asyncio.sleep(bridge.sync_interval_minutes * 60)\n            \n        except asyncio.CancelledError:\n            logger.info(f\"Sync task cancelled for bridge: {bridge.source_system}  {bridge.target_system}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in bridge sync: {e}\")\n            # Wait before retrying\n            await asyncio.sleep(60)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eMemory Classification and Conversion\u003c/h4\u003e\n\u003cp\u003eThe system classifies episodic memories into VLTM memory types:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef _classify_episodic_memory(self, memory_data: Dict[str, Any]) -\u003e MemoryType:\n    \"\"\"Classify episodic memory into VLTM memory type\"\"\"\n    \n    content = memory_data.get(\"content\", \"\").lower()\n    tags = memory_data.get(\"tags\", [])\n    \n    # Classification logic\n    if any(word in content for word in [\"optimized\", \"improved\", \"enhanced\"]):\n        return MemoryType.SUCCESSFUL_IMPROVEMENT\n    elif any(word in content for word in [\"error\", \"failed\", \"crash\"]):\n        if any(word in content for word in [\"critical\", \"severe\"]):\n            return MemoryType.CRITICAL_FAILURE\n        else:\n            return MemoryType.FAILED_EXPERIMENT\n    elif any(word in content for word in [\"architecture\", \"design\", \"pattern\"]):\n        return MemoryType.ARCHITECTURAL_INSIGHT\n    elif \"optimization\" in tags:\n        return MemoryType.SUCCESSFUL_IMPROVEMENT\n    else:\n        return MemoryType.CODE_PATTERN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMemories are converted between systems with appropriate metadata:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003easync def _convert_episodic_to_vltm(self, memory_data: Dict[str, Any], memory_type: MemoryType) -\u003e Optional[Dict[str, Any]]:\n    \"\"\"Convert episodic memory to VLTM format\"\"\"\n    \n    try:\n        content = {\n            \"original_content\": memory_data.get(\"content\"),\n            \"source_system\": \"episodic_memory\",\n            \"integration_info\": {\n                \"synced_at\": datetime.utcnow().isoformat(),\n                \"original_id\": memory_data.get(\"id\"),\n                \"confidence\": memory_data.get(\"confidence\", 0.5)\n            }\n        }\n        \n        metadata = {\n            \"episodic_sync\": True,\n            \"original_timestamp\": memory_data.get(\"timestamp\").isoformat() if memory_data.get(\"timestamp\") else None,\n            \"tags\": memory_data.get(\"tags\", [])\n        }\n        \n        return {\n            \"content\": content,\n            \"memory_type\": memory_type,\n            \"metadata\": metadata\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error converting episodic memory: {e}\")\n        return None\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eThe RAVANA memory system implements a sophisticated dual-architecture approach combining episodic and semantic memory systems. The episodic memory captures specific experiences using PostgreSQL with pgvector and SentenceTransformers for vector-based storage and retrieval, while the semantic memory system uses LLM-driven knowledge compression to create structured summaries. The system has been enhanced with multi-modal capabilities, supporting text, audio, and image content with cross-modal search and unified embedding generation. The MultiModalMemoryService provides a comprehensive interface for memory operations, and the system incorporates automated consolidation to prevent memory bloat. Performance is optimized through vector indexing and async operations, with comprehensive logging and diagnostic capabilities for debugging. Additionally, the system now includes a Very Long-Term Memory (VLTM) system that properly implements many-to-many relationships using junction tables and provides strategic knowledge management through the MemoryIntegrationManager. This architecture enables the system to maintain long-term context, learn from experiences, and provide increasingly personalized responses over time.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReferenced Files in This Document\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ememory.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eclient.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eembedding_service.py\u003c/a\u003e - \u003cem\u003eAdded in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003esearch_engine.py\u003c/a\u003e - \u003cem\u003eAdded in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emulti_modal_service.py\u003c/a\u003e - \u003cem\u003eAdded in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emodels.py\u003c/a\u003e - \u003cem\u003eAdded in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epostgresql_store.py\u003c/a\u003e - \u003cem\u003eAdded in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ememory_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecompressed_memory.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emain.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ecompression_prompts.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003etest_memory.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eknowledge_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003evltm_data_models.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003evltm_memory_integration_manager.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ellm.py\u003c/a\u003e - \u003cem\u003eUpdated in recent commit\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture \u0026 Design","title":"Architecture \u0026 Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment \u0026 Operations","title":"Deployment \u0026 Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true},"page":"/docs/[slug]","query":{"slug":"Memory Systems"},"buildId":"QHWQNiRZOuW15nbk5-ngt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>