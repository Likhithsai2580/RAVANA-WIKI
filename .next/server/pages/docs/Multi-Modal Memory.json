{"pageProps":{"doc":{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory","content":"<h1>Multi-Modal Memory</h1>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#project-structure\">Project Structure</a></li>\n<li><a href=\"#core-components\">Core Components</a></li>\n<li><a href=\"#architecture-overview\">Architecture Overview</a></li>\n<li><a href=\"#detailed-component-analysis\">Detailed Component Analysis</a></li>\n<li><a href=\"#dependency-analysis\">Dependency Analysis</a></li>\n<li><a href=\"#performance-considerations\">Performance Considerations</a></li>\n<li><a href=\"#troubleshooting-guide\">Troubleshooting Guide</a></li>\n<li><a href=\"#conclusion\">Conclusion</a></li>\n</ol>\n<h2>Introduction</h2>\n<p>The Multi-Modal Memory system is an advanced episodic memory module within the RAVANA project, designed to store, retrieve, and process information across multiple modalities including text, audio, image, and video. This system enhances the AGI's long-term memory capabilities by enabling semantic understanding and cross-modal retrieval. It integrates Whisper for audio transcription, PostgreSQL with pgvector for high-performance vector storage, and supports hybrid search modes combining vector similarity and full-text search. The system maintains backward compatibility with legacy ChromaDB-based storage while offering a scalable, robust foundation for multi-modal data persistence and retrieval.</p>\n<h2>Project Structure</h2>\n<p>The multi-modal memory system is organized into a modular structure within the <code>modules/episodic_memory</code> directory. The core components include API endpoints, data models, database operations, embedding generation, and specialized processors for different media types. The system is designed for extensibility and integration with the broader RAVANA framework.</p>\n<p>``mermaid\ngraph TB\nsubgraph \"API Layer\"\nA[memory.py] --> B[FastAPI Endpoints]\nend\nsubgraph \"Service Layer\"\nC[multi_modal_service.py] --> D[Orchestration]\nD --> E[PostgreSQLStore]\nD --> F[EmbeddingService]\nD --> G[WhisperAudioProcessor]\nD --> H[AdvancedSearchEngine]\nend\nsubgraph \"Data Layer\"\nI[postgresql_store.py] --> J[PostgreSQL]\nK[models.py] --> L[Pydantic Models]\nend\nsubgraph \"Processing Layer\"\nM[embedding_service.py] --> N[Text Embeddings]\nM --> O[Image Embeddings]\nM --> P[Audio Embeddings]\nQ[whisper_processor.py] --> R[Audio Transcription]\nend\nsubgraph \"Utilities\"\nS[setup_database.py] --> T[Schema &#x26; Migration]\nU[search_engine.py] --> V[Hybrid Search]\nend\nB --> C\nC --> I\nC --> M\nC --> Q\nC --> U\nI --> K\nM --> K</p>\n<pre><code>\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py)\n- [models.py](file://modules/episodic_memory/models.py)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)\n\n## Core Components\nThe Multi-Modal Memory system comprises several core components that work together to enable robust, scalable memory operations. The system is built on FastAPI for its RESTful interface, leveraging PostgreSQL with the pgvector extension for efficient vector similarity search. At its heart is the `MultiModalMemoryService` class, which orchestrates interactions between the database, embedding generation, audio processing, and search functionalities. The data model is defined using Pydantic, ensuring type safety and validation for all memory records and API requests. Audio processing is powered by Whisper, enabling transcription and feature extraction from spoken content. The system supports both legacy ChromaDB operations and the new PostgreSQL-based storage, ensuring backward compatibility during migration. Key components include the `PostgreSQLStore` for database operations, `EmbeddingService` for generating text, image, and audio embeddings, and `AdvancedSearchEngine` for executing hybrid and cross-modal searches.\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [models.py](file://modules/episodic_memory/models.py#L1-L251)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n\n## Architecture Overview\nThe Multi-Modal Memory system follows a layered architecture with clear separation of concerns. The API layer, implemented with FastAPI, exposes endpoints for memory operations and search. The service layer, centered around `MultiModalMemoryService`, coordinates all business logic and integrates various components. The data layer uses PostgreSQL with pgvector for persistent storage of memory records and their embeddings. The processing layer handles the generation of embeddings for different modalities and the extraction of features from audio and image content.\n\n``mermaid\ngraph TD\nA[Client] --> B[API Layer]\nB --> C[Service Layer]\nC --> D[Data Layer]\nC --> E[Processing Layer]\nsubgraph \"API Layer\"\nB[FastAPI]\nB --> B1[/extract_memories/]\nB --> B2[/save_memories/]\nB --> B3[/memories/audio/]\nB --> B4[/search/advanced/]\nend\nsubgraph \"Service Layer\"\nC[MultiModalMemoryService]\nC --> C1[PostgreSQLStore]\nC --> C2[EmbeddingService]\nC --> C3[WhisperAudioProcessor]\nC --> C4[AdvancedSearchEngine]\nend\nsubgraph \"Data Layer\"\nD[PostgreSQL]\nD --> D1[memory_records]\nD --> D2[audio_memories]\nD --> D3[image_memories]\nend\nsubgraph \"Processing Layer\"\nE[Embedding Generation]\nE --> E1[SentenceTransformer]\nE --> E2[Whisper]\nE --> E3[CLIP - Placeholder]\nend\nstyle B fill:#f9f,stroke:#333\nstyle C fill:#bbf,stroke:#333\nstyle D fill:#f96,stroke:#333\nstyle E fill:#6f9,stroke:#333\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>memory.py</a></li>\n<li><a>multi_modal_service.py</a></li>\n<li><a>postgresql_store.py</a></li>\n<li><a>embedding_service.py</a></li>\n<li><a>whisper_processor.py</a></li>\n</ul>\n<h2>Detailed Component Analysis</h2>\n<h3>MultiModalMemoryService Analysis</h3>\n<p>The <code>MultiModalMemoryService</code> is the central orchestrator of the multi-modal memory system. It integrates the PostgreSQL store, embedding service, Whisper audio processor, and search engine to provide a unified interface for memory operations. The service is initialized with a database URL and model configurations, and it manages the lifecycle of its components through <code>initialize()</code> and <code>close()</code> methods.</p>\n<p>``mermaid\nclassDiagram\nclass MultiModalMemoryService {\n-database_url : str\n-device : str\n-initialized : bool\n-start_time : datetime\n+initialize() : Coroutine\n+close() : Coroutine\n+process_text_memory(text : str, memory_type : MemoryType, tags : List[str], emotional_valence : float) : Coroutine<del>MemoryRecord</del>\n+process_audio_memory(audio_path : str, context : str, memory_type : MemoryType, tags : List[str]) : Coroutine<del>MemoryRecord</del>\n+process_image_memory(image_path : str, description : str, memory_type : MemoryType, tags : List[str]) : Coroutine<del>MemoryRecord</del>\n+extract_memories_from_conversation(request : ConversationRequest) : Coroutine<del>MemoriesList</del>\n+save_extracted_memories(memories_list : MemoriesList) : Coroutine<del>List[MemoryRecord]</del>\n+search_memories(request : SearchRequest) : Coroutine<del>SearchResponse</del>\n+find_similar_memories(memory_id : UUID, limit : int, similarity_threshold : float) : Coroutine<del>List[MemoryRecord]</del>\n+batch_process_files(request : BatchProcessRequest) : Coroutine<del>BatchProcessResult</del>\n+get_memory_statistics() : Coroutine<del>MemoryStatistics</del>\n+consolidate_memories(memory_ids : List[UUID], max_memories : int) : Coroutine<del>Dict[str, Any]</del>\n+health_check() : Coroutine<del>Dict[str, Any]</del>\n}\nclass PostgreSQLStore {\n-database_url : str\n-pool_size : int\n-max_connections : int\n-pool : asyncpg.Pool\n+initialize() : Coroutine\n+close() : Coroutine\n+save_memory_record(memory_record : MemoryRecord) : Coroutine<del>MemoryRecord</del>\n+get_memory_record(memory_id : UUID) : Coroutine<del>Optional[MemoryRecord]</del>\n+vector_search(embedding : List[float], embedding_type : str, limit : int, similarity_threshold : float, content_types : List[ContentType]) : Coroutine<del>List[Tuple[MemoryRecord, float]]</del>\n+text_search(query_text : str, limit : int, content_types : List[ContentType]) : Coroutine<del>List[Tuple[MemoryRecord, float]]</del>\n+delete_memory_record(memory_id : UUID) : Coroutine<del>bool</del>\n+get_memory_statistics() : Coroutine<del>Dict[str, Any]</del>\n+cleanup_old_memories(days_old : int, keep_minimum : int) : Coroutine<del>int</del>\n}\nclass EmbeddingService {\n-text_model_name : str\n-device : str\n-cache_size : int\n-text_model : SentenceTransformer\n-whisper_processor : WhisperAudioProcessor\n-cache : EmbeddingCache\n+generate_text_embedding(text : str) : Coroutine<del>List[float]</del>\n+generate_image_embedding(image_path : str) : Coroutine<del>List[float]</del>\n+generate_audio_embedding(audio_features : Dict[str, Any]) : Coroutine<del>List[float]</del>\n+generate_unified_embedding(memory_record : MemoryRecord) : Coroutine<del>List[float]</del>\n+generate_embeddings(memory_record : MemoryRecord) : Coroutine<del>MemoryRecord</del>\n+compute_similarity(embedding1 : List[float], embedding2 : List[float]) : Coroutine<del>float</del>\n+batch_generate_embeddings(texts : List[str]) : Coroutine<del>List[List[float]]</del>\n+clear_cache() : void\n+cleanup() : void\n}\nclass WhisperAudioProcessor {\n-model_size : str\n-device : str\n-model : WhisperModel\n+process_audio(audio_path : str, context : str) : Coroutine<del>Dict[str, Any]</del>\n+create_audio_metadata(audio_result : Dict[str, Any]) : AudioMetadata\n+cleanup() : void\n}\nclass AdvancedSearchEngine {\n-postgres_store : PostgreSQLStore\n-embedding_service : EmbeddingService\n-whisper_processor : WhisperAudioProcessor\n+search(request : SearchRequest) : Coroutine<del>SearchResponse</del>\n+cross_modal_search(request : CrossModalSearchRequest) : Coroutine<del>List[SearchResult]</del>\n+find_similar_memories(reference_memory : MemoryRecord, limit : int, similarity_threshold : float) : Coroutine<del>List[SearchResult]</del>\n}\nMultiModalMemoryService --> PostgreSQLStore : \"uses\"\nMultiModalMemoryService --> EmbeddingService : \"uses\"\nMultiModalMemoryService --> WhisperAudioProcessor : \"uses\"\nMultiModalMemoryService --> AdvancedSearchEngine : \"uses\"\nAdvancedSearchEngine --> PostgreSQLStore : \"uses\"\nAdvancedSearchEngine --> EmbeddingService : \"uses\"\nAdvancedSearchEngine --> WhisperAudioProcessor : \"uses\"\nEmbeddingService --> WhisperAudioProcessor : \"uses\"</p>\n<pre><code>\n**Diagram sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)\n\n**Section sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n\n### Memory Record and Data Models\nThe data model for the multi-modal memory system is defined in `models.py` using Pydantic. The `MemoryRecord` class is the core data structure, capable of storing information from various modalities. It includes fields for text, audio, image, and video content, along with their respective metadata and embeddings. The model supports validation through Pydantic validators, ensuring data integrity.\n\n``mermaid\nclassDiagram\nclass MemoryRecord {\nid : Optional[UUID]\ncontent_type : ContentType\ncontent_text : Optional[str]\ncontent_metadata : Dict[str, Any]\nfile_path : Optional[str]\ntext_embedding : Optional[List[float]]\nimage_embedding : Optional[List[float]]\naudio_embedding : Optional[List[float]]\nunified_embedding : Optional[List[float]]\ncreated_at : Optional[datetime]\nlast_accessed : Optional[datetime]\naccess_count : int\nmemory_type : MemoryType\nemotional_valence : Optional[float]\nconfidence_score : float\ntags : List[str]\naudio_metadata : Optional[AudioMetadata]\nimage_metadata : Optional[ImageMetadata]\nvideo_metadata : Optional[VideoMetadata]\n}\nclass AudioMetadata {\ntranscript : Optional[str]\nlanguage_code : Optional[str]\nconfidence_scores : Dict[str, float]\nduration_seconds : Optional[float]\naudio_features : Dict[str, Any]\nsample_rate : Optional[int]\nchannels : Optional[int]\n}\nclass ImageMetadata {\nwidth : Optional[int]\nheight : Optional[int]\nobject_detections : Dict[str, Any]\nscene_description : Optional[str]\nimage_hash : Optional[str]\ncolor_palette : Dict[str, Any]\nimage_features : Dict[str, Any]\n}\nclass VideoMetadata {\nduration_seconds : Optional[float]\nframe_rate : Optional[float]\nwidth : Optional[int]\nheight : Optional[int]\nvideo_features : Dict[str, Any]\nthumbnail_path : Optional[str]\n}\nclass SearchRequest {\nquery : str\ncontent_types : Optional[List[ContentType]]\nmemory_types : Optional[List[MemoryType]]\nsearch_mode : SearchMode\nlimit : int\nsimilarity_threshold : float\ninclude_metadata : bool\ntags : Optional[List[str]]\nquery_content_type : Optional[ContentType]\ntarget_content_types : Optional[List[ContentType]]\ncreated_after : Optional[datetime]\ncreated_before : Optional[datetime]\n}\nclass SearchResult {\nmemory_record : MemoryRecord\nsimilarity_score : float\nrank : int\nsearch_metadata : Dict[str, Any]\n}\nclass SearchResponse {\nresults : List[SearchResult]\ntotal_found : int\nsearch_time_ms : int\nsearch_mode : SearchMode\nquery_metadata : Dict[str, Any]\n}\nenum ContentType {\nTEXT\nAUDIO\nIMAGE\nVIDEO\n}\nenum MemoryType {\nEPISODIC\nSEMANTIC\nCONSOLIDATED\nWORKING\n}\nenum SearchMode {\nVECTOR\nTEXT\nHYBRID\nCROSS_MODAL\n}\nMemoryRecord --> AudioMetadata : \"has\"\nMemoryRecord --> ImageMetadata : \"has\"\nMemoryRecord --> VideoMetadata : \"has\"\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>models.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>models.py</a></li>\n</ul>\n<h3>API Endpoints and Request Flow</h3>\n<p>The API endpoints are implemented in <code>memory.py</code> using FastAPI. The system supports both legacy endpoints for backward compatibility and new endpoints for multi-modal operations. The request flow for processing an audio file involves uploading the file, transcribing it with Whisper, generating embeddings, and storing the memory record in PostgreSQL.</p>\n<p>``mermaid\nsequenceDiagram\nparticipant Client\nparticipant API as memory.py\nparticipant Service as MultiModalMemoryService\nparticipant Whisper as WhisperAudioProcessor\nparticipant Embedding as EmbeddingService\nparticipant DB as PostgreSQLStore\nClient->>API : POST /memories/audio/ (UploadFile)\nAPI->>Service : process_audio_memory(audio_path, context, tags)\nService->>Whisper : process_audio(audio_path, context)\nWhisper-->>Service : audio_result (transcript, features)\nService->>Service : create AudioMetadata\nService->>Embedding : generate_text_embedding(transcript)\nEmbedding-->>Service : text_embedding\nService->>Embedding : generate_audio_embedding(features)\nEmbedding-->>Service : audio_embedding\nService->>Embedding : generate_unified_embedding()\nEmbedding-->>Service : unified_embedding\nService->>DB : save_memory_record(memory_record)\nDB-->>Service : saved_record\nService-->>API : ProcessingResult\nAPI-->>Client : 200 OK (ProcessingResult)\nNote over Client,DB : Audio memory processing flow</p>\n<pre><code>\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n\n### Database Schema and Migration\nThe database schema is defined in `schema.sql` and managed through `setup_database.py`. The system uses PostgreSQL with the pgvector extension to store memory records and their embeddings. The migration process allows for seamless transition from the legacy ChromaDB storage to the new PostgreSQL-based system, including data migration and schema creation.\n\n``mermaid\nerDiagram\nmemory_records ||--o{ audio_memories : \"1:1\"\nmemory_records ||--o{ image_memories : \"1:1\"\nmemory_records ||--o{ video_memories : \"1:1\"\nmemory_records {\nuuid id PK\nstring content_type\ntext content_text\njsonb content_metadata\nstring file_path\nvector text_embedding\nvector image_embedding\nvector audio_embedding\nvector unified_embedding\ntimestamp created_at\ntimestamp last_accessed\nint access_count\nstring memory_type\nfloat emotional_valence\nfloat confidence_score\nstring[] tags\n}\naudio_memories {\nuuid memory_id PK FK\ntext transcript\nstring language_code\njsonb confidence_scores\nfloat duration_seconds\njsonb audio_features\nint sample_rate\nint channels\n}\nimage_memories {\nuuid memory_id PK FK\nint width\nint height\njsonb object_detections\ntext scene_description\nstring image_hash\njsonb color_palette\njsonb image_features\n}\nvideo_memories {\nuuid memory_id PK FK\nfloat duration_seconds\nfloat frame_rate\nint width\nint height\njsonb video_features\nstring thumbnail_path\n}\n</code></pre>\n<p><strong>Diagram sources</strong></p>\n<ul>\n<li><a>schema.sql</a></li>\n<li><a>setup_database.py</a></li>\n<li><a>postgresql_store.py</a></li>\n</ul>\n<p><strong>Section sources</strong></p>\n<ul>\n<li><a>setup_database.py</a></li>\n</ul>\n<h2>Dependency Analysis</h2>\n<p>The Multi-Modal Memory system has a well-defined dependency structure. The core dependencies include FastAPI for the web framework, asyncpg for PostgreSQL connectivity, sentence-transformers for text embeddings, and faster-whisper for audio processing. The system also depends on pgvector for vector similarity search in PostgreSQL. The component dependencies are managed through Python's import system, with clear interfaces between modules.</p>\n<p>``mermaid\ngraph TD\nA[FastAPI] --> B[memory.py]\nB --> C[multi_modal_service.py]\nC --> D[postgresql_store.py]\nC --> E[embedding_service.py]\nC --> F[whisper_processor.py]\nC --> G[search_engine.py]\nD --> H[asyncpg]\nE --> I[sentence-transformers]\nE --> J[torch]\nF --> K[faster-whisper]\nD --> L[pgvector]\nE --> M[Pillow]\nG --> N[numpy]\nstyle A fill:#f96,stroke:#333\nstyle H fill:#f96,stroke:#333\nstyle I fill:#f96,stroke:#333\nstyle J fill:#f96,stroke:#333\nstyle K fill:#f96,stroke:#333\nstyle L fill:#f96,stroke:#333\nstyle M fill:#f96,stroke:#333\nstyle N fill:#f96,stroke:#333</p>\n<pre><code>\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [requirements.txt](file://modules/episodic_memory/requirements.txt)\n\n## Performance Considerations\nThe Multi-Modal Memory system incorporates several performance optimizations. The `EmbeddingService` includes an in-memory cache to avoid recomputation of text embeddings, significantly improving response times for repeated queries. The PostgreSQL database uses IVFFlat indexes for efficient vector similarity search. Connection pooling is implemented to handle concurrent requests efficiently. Audio processing is optimized by resampling to 16kHz and limiting maximum file duration. The system also supports batch processing of files with configurable parallelism to maximize throughput.\n\n**Section sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)\n\n## Troubleshooting Guide\nCommon issues with the Multi-Modal Memory system include database connectivity problems, missing pgvector extension, and audio processing failures. Ensure that PostgreSQL is running and the pgvector extension is installed. Verify the `POSTGRES_URL` environment variable is correctly set. For audio processing issues, check that the Whisper model is properly downloaded and that the audio file format is supported. Enable debug logging by setting `LOG_LEVEL=DEBUG` to get more detailed error messages. If migration from ChromaDB fails, ensure the ChromaDB directory exists and is accessible. Monitor the system's health using the `/health` endpoint, which reports database connectivity and service status.\n\n**Section sources**\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py#L1-L434)\n\n## Conclusion\nThe Multi-Modal Memory system represents a significant advancement in the RAVANA project's memory capabilities. By supporting multiple data modalities and leveraging state-of-the-art technologies like Whisper and pgvector, it enables rich, context-aware memory storage and retrieval. The system's modular architecture ensures maintainability and extensibility, while its backward compatibility facilitates smooth integration with existing components. The comprehensive API and client library make it easy to incorporate multi-modal memory operations into various applications. With its robust performance optimizations and detailed troubleshooting guidance, the system is well-positioned to serve as a foundational component for advanced AGI applications.\n\n**Referenced Files in This Document**   \n- [memory.py](file://modules/episodic_memory/memory.py)\n- [models.py](file://modules/episodic_memory/models.py)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)\n</code></pre>\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture & Design","title":"Architecture & Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment & Operations","title":"Deployment & Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true}