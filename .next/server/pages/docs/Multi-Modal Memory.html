<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta charSet="utf-8"/><title>Multi-Modal Memory<!-- --> - RAVANA AGI Documentation</title><meta name="description" content="Documentation for Multi-Modal Memory"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/aa7d986e9c238cc1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/aa7d986e9c238cc1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" defer="" data-nscript="beforeInteractive"></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.0/dist/mermaid.min.js" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-eb143115b8bf2786.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a41459f5c0b49356.js" defer=""></script><script src="/_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="/_next/static/chunks/pages/docs/%5Bslug%5D-37d587d3c8e56222.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_buildManifest.js" defer=""></script><script src="/_next/static/QHWQNiRZOuW15nbk5-ngt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen flex flex-col"><div class="min-h-screen flex flex-col"><header class="bg-wiki-blue text-white p-4 shadow-md"><div class="container mx-auto flex justify-between items-center"><h1 class="text-2xl font-bold">RAVANA AGI Documentation</h1><nav><ul class="flex space-x-4"><li><a class="hover:underline" href="/">Home</a></li></ul></nav></div></header><div class="flex-grow container mx-auto p-4 flex flex-col md:flex-row gap-6"><div class="w-full md:w-64 flex-shrink-0"><nav class="w-full md:w-64 flex-shrink-0"><div class="bg-white rounded-lg shadow p-4 sticky top-4"><h3 class="font-bold text-lg mb-3">Documentation</h3><ul class="space-y-1"><li class="mb-3"><div class="font-semibold text-gray-700">A</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Action%20System">Action System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/API%20Reference">API Reference</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Architecture%20&amp;%20Design">Architecture &amp; Design</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">C</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Configuration">Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Conversational%20AI%20Communication%20Framework">Conversational AI Communication Framework</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Core%20System">Core System</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">D</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Database%20Schema">Database Schema</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Decision-Making%20System">Decision-Making System</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Deployment%20&amp;%20Operations">Deployment &amp; Operations</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Development%20Guide">Development Guide</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">E</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Emotional%20Intelligence">Emotional Intelligence</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent">Enhanced Snake Agent</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Enhanced%20Snake%20Agent%20Architecture">Enhanced Snake Agent Architecture</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">G</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Graceful%20Shutdown">Graceful Shutdown</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">L</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/LLM%20Integration">LLM Integration</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">M</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Memory%20Systems">Memory Systems</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 bg-wiki-blue text-white" href="/docs/Multi-Modal%20Memory">Multi-Modal Memory</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">P</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Project%20Overview">Project Overview</a></li></ul></li><li class="mb-3"><div class="font-semibold text-gray-700">S</div><ul class="ml-2 mt-1 space-y-1"><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Self-Improvement">Self-Improvement</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Services">Services</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Snake%20Agent%20Configuration">Snake Agent Configuration</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules-57f9b30b-b165-48d3-8e89-196940d26190">Specialized Modules</a></li><li><a class="block py-1 px-2 rounded hover:bg-gray-100 text-gray-600" href="/docs/Specialized%20Modules">Specialized Modules</a></li></ul></li></ul></div></nav></div><main class="flex-grow"><nav class="mb-4 text-sm"><ol class="list-none p-0 inline-flex"><li class="flex items-center"><a class="text-wiki-blue hover:underline" href="/">Home</a><svg class="fill-current w-3 h-3 mx-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path></svg></li><li class="flex items-center"><span class="text-gray-500">Multi-Modal Memory</span></li></ol></nav><div class="flex flex-col md:flex-row gap-6"><article class="prose max-w-none bg-white p-6 rounded-lg shadow flex-grow"><h1>Multi-Modal Memory</h1><div><h1>Multi-Modal Memory</h1>
<h2>Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#project-structure">Project Structure</a></li>
<li><a href="#core-components">Core Components</a></li>
<li><a href="#architecture-overview">Architecture Overview</a></li>
<li><a href="#detailed-component-analysis">Detailed Component Analysis</a></li>
<li><a href="#dependency-analysis">Dependency Analysis</a></li>
<li><a href="#performance-considerations">Performance Considerations</a></li>
<li><a href="#troubleshooting-guide">Troubleshooting Guide</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h2>Introduction</h2>
<p>The Multi-Modal Memory system is an advanced episodic memory module within the RAVANA project, designed to store, retrieve, and process information across multiple modalities including text, audio, image, and video. This system enhances the AGI's long-term memory capabilities by enabling semantic understanding and cross-modal retrieval. It integrates Whisper for audio transcription, PostgreSQL with pgvector for high-performance vector storage, and supports hybrid search modes combining vector similarity and full-text search. The system maintains backward compatibility with legacy ChromaDB-based storage while offering a scalable, robust foundation for multi-modal data persistence and retrieval.</p>
<h2>Project Structure</h2>
<p>The multi-modal memory system is organized into a modular structure within the <code>modules/episodic_memory</code> directory. The core components include API endpoints, data models, database operations, embedding generation, and specialized processors for different media types. The system is designed for extensibility and integration with the broader RAVANA framework.</p>
<p>``mermaid
graph TB
subgraph "API Layer"
A[memory.py] --> B[FastAPI Endpoints]
end
subgraph "Service Layer"
C[multi_modal_service.py] --> D[Orchestration]
D --> E[PostgreSQLStore]
D --> F[EmbeddingService]
D --> G[WhisperAudioProcessor]
D --> H[AdvancedSearchEngine]
end
subgraph "Data Layer"
I[postgresql_store.py] --> J[PostgreSQL]
K[models.py] --> L[Pydantic Models]
end
subgraph "Processing Layer"
M[embedding_service.py] --> N[Text Embeddings]
M --> O[Image Embeddings]
M --> P[Audio Embeddings]
Q[whisper_processor.py] --> R[Audio Transcription]
end
subgraph "Utilities"
S[setup_database.py] --> T[Schema &#x26; Migration]
U[search_engine.py] --> V[Hybrid Search]
end
B --> C
C --> I
C --> M
C --> Q
C --> U
I --> K
M --> K</p>
<pre><code>
**Diagram sources**
- [memory.py](file://modules/episodic_memory/memory.py)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)
- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)
- [search_engine.py](file://modules/episodic_memory/search_engine.py)
- [models.py](file://modules/episodic_memory/models.py)
- [setup_database.py](file://modules/episodic_memory/setup_database.py)

**Section sources**
- [memory.py](file://modules/episodic_memory/memory.py)
- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)

## Core Components
The Multi-Modal Memory system comprises several core components that work together to enable robust, scalable memory operations. The system is built on FastAPI for its RESTful interface, leveraging PostgreSQL with the pgvector extension for efficient vector similarity search. At its heart is the `MultiModalMemoryService` class, which orchestrates interactions between the database, embedding generation, audio processing, and search functionalities. The data model is defined using Pydantic, ensuring type safety and validation for all memory records and API requests. Audio processing is powered by Whisper, enabling transcription and feature extraction from spoken content. The system supports both legacy ChromaDB operations and the new PostgreSQL-based storage, ensuring backward compatibility during migration. Key components include the `PostgreSQLStore` for database operations, `EmbeddingService` for generating text, image, and audio embeddings, and `AdvancedSearchEngine` for executing hybrid and cross-modal searches.

**Section sources**
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)
- [models.py](file://modules/episodic_memory/models.py#L1-L251)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)

## Architecture Overview
The Multi-Modal Memory system follows a layered architecture with clear separation of concerns. The API layer, implemented with FastAPI, exposes endpoints for memory operations and search. The service layer, centered around `MultiModalMemoryService`, coordinates all business logic and integrates various components. The data layer uses PostgreSQL with pgvector for persistent storage of memory records and their embeddings. The processing layer handles the generation of embeddings for different modalities and the extraction of features from audio and image content.

``mermaid
graph TD
A[Client] --> B[API Layer]
B --> C[Service Layer]
C --> D[Data Layer]
C --> E[Processing Layer]
subgraph "API Layer"
B[FastAPI]
B --> B1[/extract_memories/]
B --> B2[/save_memories/]
B --> B3[/memories/audio/]
B --> B4[/search/advanced/]
end
subgraph "Service Layer"
C[MultiModalMemoryService]
C --> C1[PostgreSQLStore]
C --> C2[EmbeddingService]
C --> C3[WhisperAudioProcessor]
C --> C4[AdvancedSearchEngine]
end
subgraph "Data Layer"
D[PostgreSQL]
D --> D1[memory_records]
D --> D2[audio_memories]
D --> D3[image_memories]
end
subgraph "Processing Layer"
E[Embedding Generation]
E --> E1[SentenceTransformer]
E --> E2[Whisper]
E --> E3[CLIP - Placeholder]
end
style B fill:#f9f,stroke:#333
style C fill:#bbf,stroke:#333
style D fill:#f96,stroke:#333
style E fill:#6f9,stroke:#333
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>memory.py</a></li>
<li><a>multi_modal_service.py</a></li>
<li><a>postgresql_store.py</a></li>
<li><a>embedding_service.py</a></li>
<li><a>whisper_processor.py</a></li>
</ul>
<h2>Detailed Component Analysis</h2>
<h3>MultiModalMemoryService Analysis</h3>
<p>The <code>MultiModalMemoryService</code> is the central orchestrator of the multi-modal memory system. It integrates the PostgreSQL store, embedding service, Whisper audio processor, and search engine to provide a unified interface for memory operations. The service is initialized with a database URL and model configurations, and it manages the lifecycle of its components through <code>initialize()</code> and <code>close()</code> methods.</p>
<p>``mermaid
classDiagram
class MultiModalMemoryService {
-database_url : str
-device : str
-initialized : bool
-start_time : datetime
+initialize() : Coroutine
+close() : Coroutine
+process_text_memory(text : str, memory_type : MemoryType, tags : List[str], emotional_valence : float) : Coroutine<del>MemoryRecord</del>
+process_audio_memory(audio_path : str, context : str, memory_type : MemoryType, tags : List[str]) : Coroutine<del>MemoryRecord</del>
+process_image_memory(image_path : str, description : str, memory_type : MemoryType, tags : List[str]) : Coroutine<del>MemoryRecord</del>
+extract_memories_from_conversation(request : ConversationRequest) : Coroutine<del>MemoriesList</del>
+save_extracted_memories(memories_list : MemoriesList) : Coroutine<del>List[MemoryRecord]</del>
+search_memories(request : SearchRequest) : Coroutine<del>SearchResponse</del>
+find_similar_memories(memory_id : UUID, limit : int, similarity_threshold : float) : Coroutine<del>List[MemoryRecord]</del>
+batch_process_files(request : BatchProcessRequest) : Coroutine<del>BatchProcessResult</del>
+get_memory_statistics() : Coroutine<del>MemoryStatistics</del>
+consolidate_memories(memory_ids : List[UUID], max_memories : int) : Coroutine<del>Dict[str, Any]</del>
+health_check() : Coroutine<del>Dict[str, Any]</del>
}
class PostgreSQLStore {
-database_url : str
-pool_size : int
-max_connections : int
-pool : asyncpg.Pool
+initialize() : Coroutine
+close() : Coroutine
+save_memory_record(memory_record : MemoryRecord) : Coroutine<del>MemoryRecord</del>
+get_memory_record(memory_id : UUID) : Coroutine<del>Optional[MemoryRecord]</del>
+vector_search(embedding : List[float], embedding_type : str, limit : int, similarity_threshold : float, content_types : List[ContentType]) : Coroutine<del>List[Tuple[MemoryRecord, float]]</del>
+text_search(query_text : str, limit : int, content_types : List[ContentType]) : Coroutine<del>List[Tuple[MemoryRecord, float]]</del>
+delete_memory_record(memory_id : UUID) : Coroutine<del>bool</del>
+get_memory_statistics() : Coroutine<del>Dict[str, Any]</del>
+cleanup_old_memories(days_old : int, keep_minimum : int) : Coroutine<del>int</del>
}
class EmbeddingService {
-text_model_name : str
-device : str
-cache_size : int
-text_model : SentenceTransformer
-whisper_processor : WhisperAudioProcessor
-cache : EmbeddingCache
+generate_text_embedding(text : str) : Coroutine<del>List[float]</del>
+generate_image_embedding(image_path : str) : Coroutine<del>List[float]</del>
+generate_audio_embedding(audio_features : Dict[str, Any]) : Coroutine<del>List[float]</del>
+generate_unified_embedding(memory_record : MemoryRecord) : Coroutine<del>List[float]</del>
+generate_embeddings(memory_record : MemoryRecord) : Coroutine<del>MemoryRecord</del>
+compute_similarity(embedding1 : List[float], embedding2 : List[float]) : Coroutine<del>float</del>
+batch_generate_embeddings(texts : List[str]) : Coroutine<del>List[List[float]]</del>
+clear_cache() : void
+cleanup() : void
}
class WhisperAudioProcessor {
-model_size : str
-device : str
-model : WhisperModel
+process_audio(audio_path : str, context : str) : Coroutine<del>Dict[str, Any]</del>
+create_audio_metadata(audio_result : Dict[str, Any]) : AudioMetadata
+cleanup() : void
}
class AdvancedSearchEngine {
-postgres_store : PostgreSQLStore
-embedding_service : EmbeddingService
-whisper_processor : WhisperAudioProcessor
+search(request : SearchRequest) : Coroutine<del>SearchResponse</del>
+cross_modal_search(request : CrossModalSearchRequest) : Coroutine<del>List[SearchResult]</del>
+find_similar_memories(reference_memory : MemoryRecord, limit : int, similarity_threshold : float) : Coroutine<del>List[SearchResult]</del>
}
MultiModalMemoryService --> PostgreSQLStore : "uses"
MultiModalMemoryService --> EmbeddingService : "uses"
MultiModalMemoryService --> WhisperAudioProcessor : "uses"
MultiModalMemoryService --> AdvancedSearchEngine : "uses"
AdvancedSearchEngine --> PostgreSQLStore : "uses"
AdvancedSearchEngine --> EmbeddingService : "uses"
AdvancedSearchEngine --> WhisperAudioProcessor : "uses"
EmbeddingService --> WhisperAudioProcessor : "uses"</p>
<pre><code>
**Diagram sources**
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)
- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)
- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)

**Section sources**
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)

### Memory Record and Data Models
The data model for the multi-modal memory system is defined in `models.py` using Pydantic. The `MemoryRecord` class is the core data structure, capable of storing information from various modalities. It includes fields for text, audio, image, and video content, along with their respective metadata and embeddings. The model supports validation through Pydantic validators, ensuring data integrity.

``mermaid
classDiagram
class MemoryRecord {
id : Optional[UUID]
content_type : ContentType
content_text : Optional[str]
content_metadata : Dict[str, Any]
file_path : Optional[str]
text_embedding : Optional[List[float]]
image_embedding : Optional[List[float]]
audio_embedding : Optional[List[float]]
unified_embedding : Optional[List[float]]
created_at : Optional[datetime]
last_accessed : Optional[datetime]
access_count : int
memory_type : MemoryType
emotional_valence : Optional[float]
confidence_score : float
tags : List[str]
audio_metadata : Optional[AudioMetadata]
image_metadata : Optional[ImageMetadata]
video_metadata : Optional[VideoMetadata]
}
class AudioMetadata {
transcript : Optional[str]
language_code : Optional[str]
confidence_scores : Dict[str, float]
duration_seconds : Optional[float]
audio_features : Dict[str, Any]
sample_rate : Optional[int]
channels : Optional[int]
}
class ImageMetadata {
width : Optional[int]
height : Optional[int]
object_detections : Dict[str, Any]
scene_description : Optional[str]
image_hash : Optional[str]
color_palette : Dict[str, Any]
image_features : Dict[str, Any]
}
class VideoMetadata {
duration_seconds : Optional[float]
frame_rate : Optional[float]
width : Optional[int]
height : Optional[int]
video_features : Dict[str, Any]
thumbnail_path : Optional[str]
}
class SearchRequest {
query : str
content_types : Optional[List[ContentType]]
memory_types : Optional[List[MemoryType]]
search_mode : SearchMode
limit : int
similarity_threshold : float
include_metadata : bool
tags : Optional[List[str]]
query_content_type : Optional[ContentType]
target_content_types : Optional[List[ContentType]]
created_after : Optional[datetime]
created_before : Optional[datetime]
}
class SearchResult {
memory_record : MemoryRecord
similarity_score : float
rank : int
search_metadata : Dict[str, Any]
}
class SearchResponse {
results : List[SearchResult]
total_found : int
search_time_ms : int
search_mode : SearchMode
query_metadata : Dict[str, Any]
}
enum ContentType {
TEXT
AUDIO
IMAGE
VIDEO
}
enum MemoryType {
EPISODIC
SEMANTIC
CONSOLIDATED
WORKING
}
enum SearchMode {
VECTOR
TEXT
HYBRID
CROSS_MODAL
}
MemoryRecord --> AudioMetadata : "has"
MemoryRecord --> ImageMetadata : "has"
MemoryRecord --> VideoMetadata : "has"
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>models.py</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>models.py</a></li>
</ul>
<h3>API Endpoints and Request Flow</h3>
<p>The API endpoints are implemented in <code>memory.py</code> using FastAPI. The system supports both legacy endpoints for backward compatibility and new endpoints for multi-modal operations. The request flow for processing an audio file involves uploading the file, transcribing it with Whisper, generating embeddings, and storing the memory record in PostgreSQL.</p>
<p>``mermaid
sequenceDiagram
participant Client
participant API as memory.py
participant Service as MultiModalMemoryService
participant Whisper as WhisperAudioProcessor
participant Embedding as EmbeddingService
participant DB as PostgreSQLStore
Client->>API : POST /memories/audio/ (UploadFile)
API->>Service : process_audio_memory(audio_path, context, tags)
Service->>Whisper : process_audio(audio_path, context)
Whisper-->>Service : audio_result (transcript, features)
Service->>Service : create AudioMetadata
Service->>Embedding : generate_text_embedding(transcript)
Embedding-->>Service : text_embedding
Service->>Embedding : generate_audio_embedding(features)
Embedding-->>Service : audio_embedding
Service->>Embedding : generate_unified_embedding()
Embedding-->>Service : unified_embedding
Service->>DB : save_memory_record(memory_record)
DB-->>Service : saved_record
Service-->>API : ProcessingResult
API-->>Client : 200 OK (ProcessingResult)
Note over Client,DB : Audio memory processing flow</p>
<pre><code>
**Diagram sources**
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)
- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)

**Section sources**
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)

### Database Schema and Migration
The database schema is defined in `schema.sql` and managed through `setup_database.py`. The system uses PostgreSQL with the pgvector extension to store memory records and their embeddings. The migration process allows for seamless transition from the legacy ChromaDB storage to the new PostgreSQL-based system, including data migration and schema creation.

``mermaid
erDiagram
memory_records ||--o{ audio_memories : "1:1"
memory_records ||--o{ image_memories : "1:1"
memory_records ||--o{ video_memories : "1:1"
memory_records {
uuid id PK
string content_type
text content_text
jsonb content_metadata
string file_path
vector text_embedding
vector image_embedding
vector audio_embedding
vector unified_embedding
timestamp created_at
timestamp last_accessed
int access_count
string memory_type
float emotional_valence
float confidence_score
string[] tags
}
audio_memories {
uuid memory_id PK FK
text transcript
string language_code
jsonb confidence_scores
float duration_seconds
jsonb audio_features
int sample_rate
int channels
}
image_memories {
uuid memory_id PK FK
int width
int height
jsonb object_detections
text scene_description
string image_hash
jsonb color_palette
jsonb image_features
}
video_memories {
uuid memory_id PK FK
float duration_seconds
float frame_rate
int width
int height
jsonb video_features
string thumbnail_path
}
</code></pre>
<p><strong>Diagram sources</strong></p>
<ul>
<li><a>schema.sql</a></li>
<li><a>setup_database.py</a></li>
<li><a>postgresql_store.py</a></li>
</ul>
<p><strong>Section sources</strong></p>
<ul>
<li><a>setup_database.py</a></li>
</ul>
<h2>Dependency Analysis</h2>
<p>The Multi-Modal Memory system has a well-defined dependency structure. The core dependencies include FastAPI for the web framework, asyncpg for PostgreSQL connectivity, sentence-transformers for text embeddings, and faster-whisper for audio processing. The system also depends on pgvector for vector similarity search in PostgreSQL. The component dependencies are managed through Python's import system, with clear interfaces between modules.</p>
<p>``mermaid
graph TD
A[FastAPI] --> B[memory.py]
B --> C[multi_modal_service.py]
C --> D[postgresql_store.py]
C --> E[embedding_service.py]
C --> F[whisper_processor.py]
C --> G[search_engine.py]
D --> H[asyncpg]
E --> I[sentence-transformers]
E --> J[torch]
F --> K[faster-whisper]
D --> L[pgvector]
E --> M[Pillow]
G --> N[numpy]
style A fill:#f96,stroke:#333
style H fill:#f96,stroke:#333
style I fill:#f96,stroke:#333
style J fill:#f96,stroke:#333
style K fill:#f96,stroke:#333
style L fill:#f96,stroke:#333
style M fill:#f96,stroke:#333
style N fill:#f96,stroke:#333</p>
<pre><code>
**Diagram sources**
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)
- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)
- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)

**Section sources**
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)
- [requirements.txt](file://modules/episodic_memory/requirements.txt)

## Performance Considerations
The Multi-Modal Memory system incorporates several performance optimizations. The `EmbeddingService` includes an in-memory cache to avoid recomputation of text embeddings, significantly improving response times for repeated queries. The PostgreSQL database uses IVFFlat indexes for efficient vector similarity search. Connection pooling is implemented to handle concurrent requests efficiently. Audio processing is optimized by resampling to 16kHz and limiting maximum file duration. The system also supports batch processing of files with configurable parallelism to maximize throughput.

**Section sources**
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)
- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)

## Troubleshooting Guide
Common issues with the Multi-Modal Memory system include database connectivity problems, missing pgvector extension, and audio processing failures. Ensure that PostgreSQL is running and the pgvector extension is installed. Verify the `POSTGRES_URL` environment variable is correctly set. For audio processing issues, check that the Whisper model is properly downloaded and that the audio file format is supported. Enable debug logging by setting `LOG_LEVEL=DEBUG` to get more detailed error messages. If migration from ChromaDB fails, ensure the ChromaDB directory exists and is accessible. Monitor the system's health using the `/health` endpoint, which reports database connectivity and service status.

**Section sources**
- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)
- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)
- [setup_database.py](file://modules/episodic_memory/setup_database.py#L1-L434)

## Conclusion
The Multi-Modal Memory system represents a significant advancement in the RAVANA project's memory capabilities. By supporting multiple data modalities and leveraging state-of-the-art technologies like Whisper and pgvector, it enables rich, context-aware memory storage and retrieval. The system's modular architecture ensures maintainability and extensibility, while its backward compatibility facilitates smooth integration with existing components. The comprehensive API and client library make it easy to incorporate multi-modal memory operations into various applications. With its robust performance optimizations and detailed troubleshooting guidance, the system is well-positioned to serve as a foundational component for advanced AGI applications.

**Referenced Files in This Document**   
- [memory.py](file://modules/episodic_memory/memory.py)
- [models.py](file://modules/episodic_memory/models.py)
- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)
- [setup_database.py](file://modules/episodic_memory/setup_database.py)
- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)
- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)
- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)
- [search_engine.py](file://modules/episodic_memory/search_engine.py)
- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)
</code></pre>
</div></article><div class="w-full md:w-64 flex-shrink-0"></div></div></main></div><footer class="bg-wiki-dark text-white p-4"><div class="container mx-auto text-center"><p>Â© <!-- -->2025<!-- --> RAVANA AGI System Documentation</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"doc":{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory","content":"\u003ch1\u003eMulti-Modal Memory\u003c/h1\u003e\n\u003ch2\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#project-structure\"\u003eProject Structure\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#core-components\"\u003eCore Components\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#architecture-overview\"\u003eArchitecture Overview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#detailed-component-analysis\"\u003eDetailed Component Analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dependency-analysis\"\u003eDependency Analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#performance-considerations\"\u003ePerformance Considerations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#troubleshooting-guide\"\u003eTroubleshooting Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe Multi-Modal Memory system is an advanced episodic memory module within the RAVANA project, designed to store, retrieve, and process information across multiple modalities including text, audio, image, and video. This system enhances the AGI's long-term memory capabilities by enabling semantic understanding and cross-modal retrieval. It integrates Whisper for audio transcription, PostgreSQL with pgvector for high-performance vector storage, and supports hybrid search modes combining vector similarity and full-text search. The system maintains backward compatibility with legacy ChromaDB-based storage while offering a scalable, robust foundation for multi-modal data persistence and retrieval.\u003c/p\u003e\n\u003ch2\u003eProject Structure\u003c/h2\u003e\n\u003cp\u003eThe multi-modal memory system is organized into a modular structure within the \u003ccode\u003emodules/episodic_memory\u003c/code\u003e directory. The core components include API endpoints, data models, database operations, embedding generation, and specialized processors for different media types. The system is designed for extensibility and integration with the broader RAVANA framework.\u003c/p\u003e\n\u003cp\u003e``mermaid\ngraph TB\nsubgraph \"API Layer\"\nA[memory.py] --\u003e B[FastAPI Endpoints]\nend\nsubgraph \"Service Layer\"\nC[multi_modal_service.py] --\u003e D[Orchestration]\nD --\u003e E[PostgreSQLStore]\nD --\u003e F[EmbeddingService]\nD --\u003e G[WhisperAudioProcessor]\nD --\u003e H[AdvancedSearchEngine]\nend\nsubgraph \"Data Layer\"\nI[postgresql_store.py] --\u003e J[PostgreSQL]\nK[models.py] --\u003e L[Pydantic Models]\nend\nsubgraph \"Processing Layer\"\nM[embedding_service.py] --\u003e N[Text Embeddings]\nM --\u003e O[Image Embeddings]\nM --\u003e P[Audio Embeddings]\nQ[whisper_processor.py] --\u003e R[Audio Transcription]\nend\nsubgraph \"Utilities\"\nS[setup_database.py] --\u003e T[Schema \u0026#x26; Migration]\nU[search_engine.py] --\u003e V[Hybrid Search]\nend\nB --\u003e C\nC --\u003e I\nC --\u003e M\nC --\u003e Q\nC --\u003e U\nI --\u003e K\nM --\u003e K\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py)\n- [models.py](file://modules/episodic_memory/models.py)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)\n\n## Core Components\nThe Multi-Modal Memory system comprises several core components that work together to enable robust, scalable memory operations. The system is built on FastAPI for its RESTful interface, leveraging PostgreSQL with the pgvector extension for efficient vector similarity search. At its heart is the `MultiModalMemoryService` class, which orchestrates interactions between the database, embedding generation, audio processing, and search functionalities. The data model is defined using Pydantic, ensuring type safety and validation for all memory records and API requests. Audio processing is powered by Whisper, enabling transcription and feature extraction from spoken content. The system supports both legacy ChromaDB operations and the new PostgreSQL-based storage, ensuring backward compatibility during migration. Key components include the `PostgreSQLStore` for database operations, `EmbeddingService` for generating text, image, and audio embeddings, and `AdvancedSearchEngine` for executing hybrid and cross-modal searches.\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [models.py](file://modules/episodic_memory/models.py#L1-L251)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n\n## Architecture Overview\nThe Multi-Modal Memory system follows a layered architecture with clear separation of concerns. The API layer, implemented with FastAPI, exposes endpoints for memory operations and search. The service layer, centered around `MultiModalMemoryService`, coordinates all business logic and integrates various components. The data layer uses PostgreSQL with pgvector for persistent storage of memory records and their embeddings. The processing layer handles the generation of embeddings for different modalities and the extraction of features from audio and image content.\n\n``mermaid\ngraph TD\nA[Client] --\u003e B[API Layer]\nB --\u003e C[Service Layer]\nC --\u003e D[Data Layer]\nC --\u003e E[Processing Layer]\nsubgraph \"API Layer\"\nB[FastAPI]\nB --\u003e B1[/extract_memories/]\nB --\u003e B2[/save_memories/]\nB --\u003e B3[/memories/audio/]\nB --\u003e B4[/search/advanced/]\nend\nsubgraph \"Service Layer\"\nC[MultiModalMemoryService]\nC --\u003e C1[PostgreSQLStore]\nC --\u003e C2[EmbeddingService]\nC --\u003e C3[WhisperAudioProcessor]\nC --\u003e C4[AdvancedSearchEngine]\nend\nsubgraph \"Data Layer\"\nD[PostgreSQL]\nD --\u003e D1[memory_records]\nD --\u003e D2[audio_memories]\nD --\u003e D3[image_memories]\nend\nsubgraph \"Processing Layer\"\nE[Embedding Generation]\nE --\u003e E1[SentenceTransformer]\nE --\u003e E2[Whisper]\nE --\u003e E3[CLIP - Placeholder]\nend\nstyle B fill:#f9f,stroke:#333\nstyle C fill:#bbf,stroke:#333\nstyle D fill:#f96,stroke:#333\nstyle E fill:#6f9,stroke:#333\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003ememory.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003emulti_modal_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epostgresql_store.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003eembedding_service.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003ewhisper_processor.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDetailed Component Analysis\u003c/h2\u003e\n\u003ch3\u003eMultiModalMemoryService Analysis\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eMultiModalMemoryService\u003c/code\u003e is the central orchestrator of the multi-modal memory system. It integrates the PostgreSQL store, embedding service, Whisper audio processor, and search engine to provide a unified interface for memory operations. The service is initialized with a database URL and model configurations, and it manages the lifecycle of its components through \u003ccode\u003einitialize()\u003c/code\u003e and \u003ccode\u003eclose()\u003c/code\u003e methods.\u003c/p\u003e\n\u003cp\u003e``mermaid\nclassDiagram\nclass MultiModalMemoryService {\n-database_url : str\n-device : str\n-initialized : bool\n-start_time : datetime\n+initialize() : Coroutine\n+close() : Coroutine\n+process_text_memory(text : str, memory_type : MemoryType, tags : List[str], emotional_valence : float) : Coroutine\u003cdel\u003eMemoryRecord\u003c/del\u003e\n+process_audio_memory(audio_path : str, context : str, memory_type : MemoryType, tags : List[str]) : Coroutine\u003cdel\u003eMemoryRecord\u003c/del\u003e\n+process_image_memory(image_path : str, description : str, memory_type : MemoryType, tags : List[str]) : Coroutine\u003cdel\u003eMemoryRecord\u003c/del\u003e\n+extract_memories_from_conversation(request : ConversationRequest) : Coroutine\u003cdel\u003eMemoriesList\u003c/del\u003e\n+save_extracted_memories(memories_list : MemoriesList) : Coroutine\u003cdel\u003eList[MemoryRecord]\u003c/del\u003e\n+search_memories(request : SearchRequest) : Coroutine\u003cdel\u003eSearchResponse\u003c/del\u003e\n+find_similar_memories(memory_id : UUID, limit : int, similarity_threshold : float) : Coroutine\u003cdel\u003eList[MemoryRecord]\u003c/del\u003e\n+batch_process_files(request : BatchProcessRequest) : Coroutine\u003cdel\u003eBatchProcessResult\u003c/del\u003e\n+get_memory_statistics() : Coroutine\u003cdel\u003eMemoryStatistics\u003c/del\u003e\n+consolidate_memories(memory_ids : List[UUID], max_memories : int) : Coroutine\u003cdel\u003eDict[str, Any]\u003c/del\u003e\n+health_check() : Coroutine\u003cdel\u003eDict[str, Any]\u003c/del\u003e\n}\nclass PostgreSQLStore {\n-database_url : str\n-pool_size : int\n-max_connections : int\n-pool : asyncpg.Pool\n+initialize() : Coroutine\n+close() : Coroutine\n+save_memory_record(memory_record : MemoryRecord) : Coroutine\u003cdel\u003eMemoryRecord\u003c/del\u003e\n+get_memory_record(memory_id : UUID) : Coroutine\u003cdel\u003eOptional[MemoryRecord]\u003c/del\u003e\n+vector_search(embedding : List[float], embedding_type : str, limit : int, similarity_threshold : float, content_types : List[ContentType]) : Coroutine\u003cdel\u003eList[Tuple[MemoryRecord, float]]\u003c/del\u003e\n+text_search(query_text : str, limit : int, content_types : List[ContentType]) : Coroutine\u003cdel\u003eList[Tuple[MemoryRecord, float]]\u003c/del\u003e\n+delete_memory_record(memory_id : UUID) : Coroutine\u003cdel\u003ebool\u003c/del\u003e\n+get_memory_statistics() : Coroutine\u003cdel\u003eDict[str, Any]\u003c/del\u003e\n+cleanup_old_memories(days_old : int, keep_minimum : int) : Coroutine\u003cdel\u003eint\u003c/del\u003e\n}\nclass EmbeddingService {\n-text_model_name : str\n-device : str\n-cache_size : int\n-text_model : SentenceTransformer\n-whisper_processor : WhisperAudioProcessor\n-cache : EmbeddingCache\n+generate_text_embedding(text : str) : Coroutine\u003cdel\u003eList[float]\u003c/del\u003e\n+generate_image_embedding(image_path : str) : Coroutine\u003cdel\u003eList[float]\u003c/del\u003e\n+generate_audio_embedding(audio_features : Dict[str, Any]) : Coroutine\u003cdel\u003eList[float]\u003c/del\u003e\n+generate_unified_embedding(memory_record : MemoryRecord) : Coroutine\u003cdel\u003eList[float]\u003c/del\u003e\n+generate_embeddings(memory_record : MemoryRecord) : Coroutine\u003cdel\u003eMemoryRecord\u003c/del\u003e\n+compute_similarity(embedding1 : List[float], embedding2 : List[float]) : Coroutine\u003cdel\u003efloat\u003c/del\u003e\n+batch_generate_embeddings(texts : List[str]) : Coroutine\u003cdel\u003eList[List[float]]\u003c/del\u003e\n+clear_cache() : void\n+cleanup() : void\n}\nclass WhisperAudioProcessor {\n-model_size : str\n-device : str\n-model : WhisperModel\n+process_audio(audio_path : str, context : str) : Coroutine\u003cdel\u003eDict[str, Any]\u003c/del\u003e\n+create_audio_metadata(audio_result : Dict[str, Any]) : AudioMetadata\n+cleanup() : void\n}\nclass AdvancedSearchEngine {\n-postgres_store : PostgreSQLStore\n-embedding_service : EmbeddingService\n-whisper_processor : WhisperAudioProcessor\n+search(request : SearchRequest) : Coroutine\u003cdel\u003eSearchResponse\u003c/del\u003e\n+cross_modal_search(request : CrossModalSearchRequest) : Coroutine\u003cdel\u003eList[SearchResult]\u003c/del\u003e\n+find_similar_memories(reference_memory : MemoryRecord, limit : int, similarity_threshold : float) : Coroutine\u003cdel\u003eList[SearchResult]\u003c/del\u003e\n}\nMultiModalMemoryService --\u003e PostgreSQLStore : \"uses\"\nMultiModalMemoryService --\u003e EmbeddingService : \"uses\"\nMultiModalMemoryService --\u003e WhisperAudioProcessor : \"uses\"\nMultiModalMemoryService --\u003e AdvancedSearchEngine : \"uses\"\nAdvancedSearchEngine --\u003e PostgreSQLStore : \"uses\"\nAdvancedSearchEngine --\u003e EmbeddingService : \"uses\"\nAdvancedSearchEngine --\u003e WhisperAudioProcessor : \"uses\"\nEmbeddingService --\u003e WhisperAudioProcessor : \"uses\"\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)\n\n**Section sources**\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n\n### Memory Record and Data Models\nThe data model for the multi-modal memory system is defined in `models.py` using Pydantic. The `MemoryRecord` class is the core data structure, capable of storing information from various modalities. It includes fields for text, audio, image, and video content, along with their respective metadata and embeddings. The model supports validation through Pydantic validators, ensuring data integrity.\n\n``mermaid\nclassDiagram\nclass MemoryRecord {\nid : Optional[UUID]\ncontent_type : ContentType\ncontent_text : Optional[str]\ncontent_metadata : Dict[str, Any]\nfile_path : Optional[str]\ntext_embedding : Optional[List[float]]\nimage_embedding : Optional[List[float]]\naudio_embedding : Optional[List[float]]\nunified_embedding : Optional[List[float]]\ncreated_at : Optional[datetime]\nlast_accessed : Optional[datetime]\naccess_count : int\nmemory_type : MemoryType\nemotional_valence : Optional[float]\nconfidence_score : float\ntags : List[str]\naudio_metadata : Optional[AudioMetadata]\nimage_metadata : Optional[ImageMetadata]\nvideo_metadata : Optional[VideoMetadata]\n}\nclass AudioMetadata {\ntranscript : Optional[str]\nlanguage_code : Optional[str]\nconfidence_scores : Dict[str, float]\nduration_seconds : Optional[float]\naudio_features : Dict[str, Any]\nsample_rate : Optional[int]\nchannels : Optional[int]\n}\nclass ImageMetadata {\nwidth : Optional[int]\nheight : Optional[int]\nobject_detections : Dict[str, Any]\nscene_description : Optional[str]\nimage_hash : Optional[str]\ncolor_palette : Dict[str, Any]\nimage_features : Dict[str, Any]\n}\nclass VideoMetadata {\nduration_seconds : Optional[float]\nframe_rate : Optional[float]\nwidth : Optional[int]\nheight : Optional[int]\nvideo_features : Dict[str, Any]\nthumbnail_path : Optional[str]\n}\nclass SearchRequest {\nquery : str\ncontent_types : Optional[List[ContentType]]\nmemory_types : Optional[List[MemoryType]]\nsearch_mode : SearchMode\nlimit : int\nsimilarity_threshold : float\ninclude_metadata : bool\ntags : Optional[List[str]]\nquery_content_type : Optional[ContentType]\ntarget_content_types : Optional[List[ContentType]]\ncreated_after : Optional[datetime]\ncreated_before : Optional[datetime]\n}\nclass SearchResult {\nmemory_record : MemoryRecord\nsimilarity_score : float\nrank : int\nsearch_metadata : Dict[str, Any]\n}\nclass SearchResponse {\nresults : List[SearchResult]\ntotal_found : int\nsearch_time_ms : int\nsearch_mode : SearchMode\nquery_metadata : Dict[str, Any]\n}\nenum ContentType {\nTEXT\nAUDIO\nIMAGE\nVIDEO\n}\nenum MemoryType {\nEPISODIC\nSEMANTIC\nCONSOLIDATED\nWORKING\n}\nenum SearchMode {\nVECTOR\nTEXT\nHYBRID\nCROSS_MODAL\n}\nMemoryRecord --\u003e AudioMetadata : \"has\"\nMemoryRecord --\u003e ImageMetadata : \"has\"\nMemoryRecord --\u003e VideoMetadata : \"has\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emodels.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003emodels.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAPI Endpoints and Request Flow\u003c/h3\u003e\n\u003cp\u003eThe API endpoints are implemented in \u003ccode\u003ememory.py\u003c/code\u003e using FastAPI. The system supports both legacy endpoints for backward compatibility and new endpoints for multi-modal operations. The request flow for processing an audio file involves uploading the file, transcribing it with Whisper, generating embeddings, and storing the memory record in PostgreSQL.\u003c/p\u003e\n\u003cp\u003e``mermaid\nsequenceDiagram\nparticipant Client\nparticipant API as memory.py\nparticipant Service as MultiModalMemoryService\nparticipant Whisper as WhisperAudioProcessor\nparticipant Embedding as EmbeddingService\nparticipant DB as PostgreSQLStore\nClient-\u003e\u003eAPI : POST /memories/audio/ (UploadFile)\nAPI-\u003e\u003eService : process_audio_memory(audio_path, context, tags)\nService-\u003e\u003eWhisper : process_audio(audio_path, context)\nWhisper--\u003e\u003eService : audio_result (transcript, features)\nService-\u003e\u003eService : create AudioMetadata\nService-\u003e\u003eEmbedding : generate_text_embedding(transcript)\nEmbedding--\u003e\u003eService : text_embedding\nService-\u003e\u003eEmbedding : generate_audio_embedding(features)\nEmbedding--\u003e\u003eService : audio_embedding\nService-\u003e\u003eEmbedding : generate_unified_embedding()\nEmbedding--\u003e\u003eService : unified_embedding\nService-\u003e\u003eDB : save_memory_record(memory_record)\nDB--\u003e\u003eService : saved_record\nService--\u003e\u003eAPI : ProcessingResult\nAPI--\u003e\u003eClient : 200 OK (ProcessingResult)\nNote over Client,DB : Audio memory processing flow\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n\n### Database Schema and Migration\nThe database schema is defined in `schema.sql` and managed through `setup_database.py`. The system uses PostgreSQL with the pgvector extension to store memory records and their embeddings. The migration process allows for seamless transition from the legacy ChromaDB storage to the new PostgreSQL-based system, including data migration and schema creation.\n\n``mermaid\nerDiagram\nmemory_records ||--o{ audio_memories : \"1:1\"\nmemory_records ||--o{ image_memories : \"1:1\"\nmemory_records ||--o{ video_memories : \"1:1\"\nmemory_records {\nuuid id PK\nstring content_type\ntext content_text\njsonb content_metadata\nstring file_path\nvector text_embedding\nvector image_embedding\nvector audio_embedding\nvector unified_embedding\ntimestamp created_at\ntimestamp last_accessed\nint access_count\nstring memory_type\nfloat emotional_valence\nfloat confidence_score\nstring[] tags\n}\naudio_memories {\nuuid memory_id PK FK\ntext transcript\nstring language_code\njsonb confidence_scores\nfloat duration_seconds\njsonb audio_features\nint sample_rate\nint channels\n}\nimage_memories {\nuuid memory_id PK FK\nint width\nint height\njsonb object_detections\ntext scene_description\nstring image_hash\njsonb color_palette\njsonb image_features\n}\nvideo_memories {\nuuid memory_id PK FK\nfloat duration_seconds\nfloat frame_rate\nint width\nint height\njsonb video_features\nstring thumbnail_path\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDiagram sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003eschema.sql\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003esetup_database.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca\u003epostgresql_store.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSection sources\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\u003esetup_database.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDependency Analysis\u003c/h2\u003e\n\u003cp\u003eThe Multi-Modal Memory system has a well-defined dependency structure. The core dependencies include FastAPI for the web framework, asyncpg for PostgreSQL connectivity, sentence-transformers for text embeddings, and faster-whisper for audio processing. The system also depends on pgvector for vector similarity search in PostgreSQL. The component dependencies are managed through Python's import system, with clear interfaces between modules.\u003c/p\u003e\n\u003cp\u003e``mermaid\ngraph TD\nA[FastAPI] --\u003e B[memory.py]\nB --\u003e C[multi_modal_service.py]\nC --\u003e D[postgresql_store.py]\nC --\u003e E[embedding_service.py]\nC --\u003e F[whisper_processor.py]\nC --\u003e G[search_engine.py]\nD --\u003e H[asyncpg]\nE --\u003e I[sentence-transformers]\nE --\u003e J[torch]\nF --\u003e K[faster-whisper]\nD --\u003e L[pgvector]\nE --\u003e M[Pillow]\nG --\u003e N[numpy]\nstyle A fill:#f96,stroke:#333\nstyle H fill:#f96,stroke:#333\nstyle I fill:#f96,stroke:#333\nstyle J fill:#f96,stroke:#333\nstyle K fill:#f96,stroke:#333\nstyle L fill:#f96,stroke:#333\nstyle M fill:#f96,stroke:#333\nstyle N fill:#f96,stroke:#333\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n**Diagram sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py#L1-L400)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py#L1-L350)\n\n**Section sources**\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [requirements.txt](file://modules/episodic_memory/requirements.txt)\n\n## Performance Considerations\nThe Multi-Modal Memory system incorporates several performance optimizations. The `EmbeddingService` includes an in-memory cache to avoid recomputation of text embeddings, significantly improving response times for repeated queries. The PostgreSQL database uses IVFFlat indexes for efficient vector similarity search. Connection pooling is implemented to handle concurrent requests efficiently. Audio processing is optimized by resampling to 16kHz and limiting maximum file duration. The system also supports batch processing of files with configurable parallelism to maximize throughput.\n\n**Section sources**\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py#L1-L499)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py#L1-L591)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py#L1-L657)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)\n\n## Troubleshooting Guide\nCommon issues with the Multi-Modal Memory system include database connectivity problems, missing pgvector extension, and audio processing failures. Ensure that PostgreSQL is running and the pgvector extension is installed. Verify the `POSTGRES_URL` environment variable is correctly set. For audio processing issues, check that the Whisper model is properly downloaded and that the audio file format is supported. Enable debug logging by setting `LOG_LEVEL=DEBUG` to get more detailed error messages. If migration from ChromaDB fails, ensure the ChromaDB directory exists and is accessible. Monitor the system's health using the `/health` endpoint, which reports database connectivity and service status.\n\n**Section sources**\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md#L1-L316)\n- [memory.py](file://modules/episodic_memory/memory.py#L1-L722)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py#L1-L434)\n\n## Conclusion\nThe Multi-Modal Memory system represents a significant advancement in the RAVANA project's memory capabilities. By supporting multiple data modalities and leveraging state-of-the-art technologies like Whisper and pgvector, it enables rich, context-aware memory storage and retrieval. The system's modular architecture ensures maintainability and extensibility, while its backward compatibility facilitates smooth integration with existing components. The comprehensive API and client library make it easy to incorporate multi-modal memory operations into various applications. With its robust performance optimizations and detailed troubleshooting guidance, the system is well-positioned to serve as a foundational component for advanced AGI applications.\n\n**Referenced Files in This Document**   \n- [memory.py](file://modules/episodic_memory/memory.py)\n- [models.py](file://modules/episodic_memory/models.py)\n- [README_MULTIMODAL.md](file://modules/episodic_memory/README_MULTIMODAL.md)\n- [setup_database.py](file://modules/episodic_memory/setup_database.py)\n- [multi_modal_service.py](file://modules/episodic_memory/multi_modal_service.py)\n- [postgresql_store.py](file://modules/episodic_memory/postgresql_store.py)\n- [embedding_service.py](file://modules/episodic_memory/embedding_service.py)\n- [search_engine.py](file://modules/episodic_memory/search_engine.py)\n- [whisper_processor.py](file://modules/episodic_memory/whisper_processor.py)\n\u003c/code\u003e\u003c/pre\u003e\n"},"docs":[{"slug":"Action System","title":"Action System"},{"slug":"API Reference","title":"API Reference"},{"slug":"Architecture \u0026 Design","title":"Architecture \u0026 Design"},{"slug":"Configuration","title":"Configuration"},{"slug":"Conversational AI Communication Framework","title":"Conversational AI Communication Framework"},{"slug":"Core System","title":"Core System"},{"slug":"Database Schema","title":"Database Schema"},{"slug":"Decision-Making System","title":"Decision-Making System"},{"slug":"Deployment \u0026 Operations","title":"Deployment \u0026 Operations"},{"slug":"Development Guide","title":"Development Guide"},{"slug":"Emotional Intelligence","title":"Emotional Intelligence"},{"slug":"Enhanced Snake Agent","title":"Enhanced Snake Agent"},{"slug":"Enhanced Snake Agent Architecture","title":"Enhanced Snake Agent Architecture"},{"slug":"Graceful Shutdown","title":"Graceful Shutdown"},{"slug":"LLM Integration","title":"LLM Integration"},{"slug":"Memory Systems","title":"Memory Systems"},{"slug":"Multi-Modal Memory","title":"Multi-Modal Memory"},{"slug":"Project Overview","title":"Project Overview"},{"slug":"Self-Improvement","title":"Self-Improvement"},{"slug":"Services","title":"Services"},{"slug":"Snake Agent Configuration","title":"Snake Agent Configuration"},{"slug":"Specialized Modules-57f9b30b-b165-48d3-8e89-196940d26190","title":"Specialized Modules"},{"slug":"Specialized Modules","title":"Specialized Modules"}]},"__N_SSG":true},"page":"/docs/[slug]","query":{"slug":"Multi-Modal Memory"},"buildId":"QHWQNiRZOuW15nbk5-ngt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>